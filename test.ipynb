{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from CxExtractor import CxExtractor\n",
    "import io\n",
    "def crawl_main_content(url, pathname, output_name, threshold=186):\n",
    "    cx = CxExtractor(threshold=threshold)\n",
    "    s = cx.crawl(url)\n",
    "    with io.open(pathname + output_name + '.txt', 'w') as f:                                              \n",
    "        f.write(s)                                             "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-  \n",
    "#find title and href\n",
    "import sys  \n",
    "import re   \n",
    "import urllib2\n",
    "import requests\n",
    "from bs4 import BeautifulSoup  \n",
    "def get_baidu_result(key, pathname='', pagenum=1):\n",
    "    req = requests.get('http://www.baidu.com/s?', params={'wd': key, 'pn': str(10 * (pagenum - 1)), 'rsv_srlang': 'cn'})\n",
    "    print req.url\n",
    "#     req = requests.get('http://www.baidu.com/s?wd=wind%26five&pn=10')\n",
    "    soup=BeautifulSoup(html,\"lxml\")\n",
    "    linkpattern=re.compile('href=\\\"(.+?)\\\"')\n",
    "    div=soup.find('div',id='wrapper').find('div',id='wrapper_wrapper').find('div',id='container').find('div',id='content_left')\n",
    "    print type(div.find('h3'))\n",
    "    re_dict={}\n",
    "    for link in div.find_all('div', [re.compile('result c-container '), re.compile('result-op c-container')]):\n",
    "        a = link.find('h3').find('a')\n",
    "        print a.text\n",
    "        print a['href']\n",
    "#         crawl_main_content(a['href'], pathname, a.text)\n",
    "#     for i in range(11,15):\n",
    "#         a=div.find('div',id=str(i)).find('h3').find('a')\n",
    "#         re_link=linkpattern.findall(str(a))\n",
    "#         re_title=a.text\n",
    "#         re_dict[re_title]=re_link[0]\n",
    "#     for r in re_dict:\n",
    "#         print r\n",
    "#         print re_dict[r]\n",
    "#         print '....................\\n'\n",
    "#     return re_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input key word: deep learning\n",
      "input number of pages: 5\n",
      "http://www.baidu.com/s?wd=deep+learning&pn=0&rsv_srlang=cn\n",
      "<class 'bs4.element.Tag'>\n",
      "\n",
      "  百度深度学习研究院-Relentless for Ultimate Intelligence\n",
      "\n",
      "http://www.baidu.com/link?url=PJ9IsjY67M0f0B41ZEDy2EA8OfQrZAZ60enAhCzDn5y\n",
      "深度学习_百度百科\n",
      "http://www.baidu.com/link?url=1XnGvbBkYAnHP9ft2Jfe3i5pMBNReTDDQ-PVz1j5tFG-oS76eLEsw07leK2BbuFHFzpqkA5D9kXYvUL8jYDLKcdYg55u2ydbA0frNHJ9xQR_9kIhXJ9WjHGHYSAN_XWA\n",
      "Deep Learning Tutorials — DeepLearning 0.1 documentation\n",
      "http://www.baidu.com/link?url=ZH9OX66mlKFaVihNyFQQQLBF_TOvHncbsTmZOk12PlApCKnVDwQT7xnotQhgnXTx\n",
      "Deep Learning | Coursera\n",
      "http://www.baidu.com/link?url=lOb7F9zO6G-dchVu9ow2wCCB5IJnIZPI5gt_Ey0FVNcZNCwuaUOKwyfP4bc4WmoqbDrQvnSjj4FtGmHJmiNnxa\n",
      "机器学习——深度学习(Deep Learning) - Rachel Zhang的专栏 - ...\n",
      "http://www.baidu.com/link?url=WL223aFt_D6OUxcaHkKJt2xLs-a6PDo_ta_ghGlsZC7Jt3jAyUtxBHWmWP-ZRj6uaGeaFF9ztUFV2W2RLbf1r6z6xAqRF5RDB2E3JR3hAim\n",
      "deep learning初学该怎么入门?是要先好好学习神经网络吗? - 知乎\n",
      "http://www.baidu.com/link?url=UlMoDsN-bXTc1UBTCczJhD6Z3aIYsD3oGdaKi0B7TSCtpj1Ube7RB4rrI0lv-9gVH4KlsL5_IEfkkdTk-OrWtK\n",
      "Deep Learning\n",
      "http://www.baidu.com/link?url=VWWEoiAqTf_P5XtIDpw2PvEvfIQViO3pXkh-hlwzmMheqOnFqMMxjA9WeMwyRJ52\n",
      "Deep learning - Wikipedia\n",
      "http://www.baidu.com/link?url=WqewOyRG8lVthibc1Vv854GRx9nfAYy76OG2TMEqR0smiu6ksWYLDOn6Gd9aG6XBEoTqMPN4GTDEz8CYSNgSY_\n",
      "Deep Learning(深度学习)学习笔记整理系列之(三) - zouxy09的专栏...\n",
      "http://www.baidu.com/link?url=MNp8EqBr1ext5s80yadAGydcDzyU2wrLZaMbyljuW_-zq-l41BBT3tXvJ3LC9eM0dJdngaEg5F8yBDdmh6Bl5q\n",
      "http://www.baidu.com/s?wd=deep+learning&pn=10&rsv_srlang=cn\n",
      "<class 'bs4.element.Tag'>\n",
      "Teaching for Deep Learning_百度学术\n",
      "http://www.baidu.com/link?url=2_1TMGeO9vn1Il9G2joDi1ZkxlpAjJjrUs8G30wF4a2uYp6s0Dk9PWXJXU28Fnr48C62EGi1jn2r6afA8iorvyZ8uge2EKwmJO5Ehlq6zsAmfD7yb48JN1I7NaqSC-LKiNPfNf75nRK6PA2F7-jeLTcbGbCimGP8FxBWuFZBw8zDeTnFOI43NrdfHZdrSAbRKHcnsYonRcSYPI6mtazIyqLGb1L_NKUDV2aeLCmCuKYrD1SyRRo7WyQOr8jZAnbJhRr8_ebol0F7dopbn2NJo_\n",
      "Neural networks and deep learning\n",
      "http://www.baidu.com/link?url=A_ksoVeb9UpNpKNXSU-6kgjFOTWXjzT-ymQ-VKVzDAI_xLMN37pbvq9ojYzLUge8YUdfQSLSK7Wl-r6PA9w0ha\n",
      "机器学习——深度学习(Deep Learning) - Rachel Zhang的专栏 - ...\n",
      "http://www.baidu.com/link?url=t7Ayui3qxCeAOTfefpeutBPPtSvCaf4Q-AX2zXqLrENTp9f3P20BHefZYxvflqPAnlWcpur0avwCF6VmCaBXe4dtLIFff6B-VM0KVTtLISy\n",
      "Deep Learning\n",
      "http://www.baidu.com/link?url=zX1SY7REaC2hU4ILKYrcQRdKtNBxQI9Fkl0vyNdl8nAs7do0Ht51MaPEsm2acwuJ\n",
      "Deep Learning | 我爱自然语言处理\n",
      "http://www.baidu.com/link?url=NwByuwPXYgWxki9mGxQfAVKqTZksTakHCxthpvyqeUcd0kLZDPtEGcsHfXkfnamT\n",
      "百度深度学习研究院\n",
      "http://www.baidu.com/link?url=RIBo4Rn2pyYpj_fJSdJBF63OZc83GucZBLv_1I4uK97\n",
      "Deep learning:四十一(Dropout简单理解) - tornadomeet - 博客园\n",
      "http://www.baidu.com/link?url=tvtsgZUsvT32DiIt9hJrfeHtu8obEUpoCktbx03QN-F1I5SImB7AzPBInqsjdtrEyY6bas3zLSXZIECVjo03Zq\n",
      "Deeplearning4j: Open-source, Distributed Deep Learning for ...\n",
      "http://www.baidu.com/link?url=ljn2ahH7JB17cIlGeBkXlpbRQb0jKRaGdXZzALFIQm6EhSoeUebzL_q7Wg8Gmq_B\n",
      "深度学习 deep learning 介绍 王晓刚_图文_百度文库\n",
      "http://www.baidu.com/link?url=h2Cl2postsfTTATuhOcTzlli2AJ0vWSZLmCmJyCNdf7F7MDJea3ipjQjJo6LbunRjxOqAPQli7n6nrdlecrQR6Y1WVzmJkHtZE3hp0FhBFq\n",
      "deep learning 知识总结_百度文库\n",
      "http://www.baidu.com/link?url=MLZXziUiaXT29Q1Rl0yoNE2bX7BGW5cfnqAFLt5Xp6avB5k2hbBpOn5Z8xldbNO1dlNzlDdAnY_FhK7y_WVLMt0XSrIw-w3lqgGjvwuvRYq\n",
      "http://www.baidu.com/s?wd=deep+learning&pn=20&rsv_srlang=cn\n",
      "<class 'bs4.element.Tag'>\n",
      "Deep Learning\n",
      "http://www.baidu.com/link?url=4akfEcDLsDsFwbjoQ_huiLayvVskF45S_9yvhZdcIGXvuAaP6shfu5e3R2T4Rw9A\n",
      "Deep Learning(深度学习)\n",
      "http://www.baidu.com/link?url=IfX3RJRRXbNQtYYp6sWBMrjTAAC5VKRwp_u_Ig6nEAaB-fsuQlxjTdHJG4845OherRfFmV_sWdH_0SRSWOnjAwR4RsTowDpl65bILLwmDUm\n",
      "Deep Learning 教程翻译_邓侃_新浪博客\n",
      "http://www.baidu.com/link?url=IsshjoqmU0WyNEDcwT83BJUD4JU7reYsUTV658dX1534_IRAB5lMy9RE7q7rFi7VAsYi_9C81n7OKHzNDT0hpa\n",
      "Deep Learning - Quora\n",
      "http://www.baidu.com/link?url=Kytu7Zb9IhE-TmudXZ9ExSaIP0a4gxy9jtxDFtQVwKpxLwF-MvxeNjT4NbKQJA_N\n",
      "Is “Deep Learning” a Revolution in Artificial Intelligence?...\n",
      "http://www.baidu.com/link?url=s9-06O_w1A2f88E3FSzE08dwMmCSkh7IkvVqlQOLqEz6k_m5MNJ2AOphvfBzUVWg4FJXSv-m_1lKAqN0TYYs-vov5eTfJSF_DOuApNJUHsbK8MIHVX1wUXQFVMXh1qPzuFfWT4-_I7qjFUtELO0Bva\n",
      "Deep Learning - MIT Technology Review\n",
      "http://www.baidu.com/link?url=jWy7MfPqlD2OBn9O-FrSAzYGgJywM_TXvNlzCHa_bJWpeV_R9_cJXrAh162rnWJzWeLbDZ3k8vndbAJAyLBmxwt_eDzP5MAyGV7V8y4unz_\n",
      "Deep Learning(深度学习)学习笔记整理系列之常用模型(四、五、六...\n",
      "http://www.baidu.com/link?url=xn4EdYUWxDc55Umih8udG7rgSrvN8JI0o-eh9mWTG84co0u-nF-_6w_cnXZi3xvg\n",
      "Deep Learning for NLP - NAACL 2013 Tutorial\n",
      "http://www.baidu.com/link?url=g8OyNX2AVKB5KznFv0tsT3RfZP0pCspJbavoMtDQALi4Lv3vdlznhNiEdvKfGlEv8fhJYF91cLwIw67vn3a2p_\n",
      "Deep Learning in NLP (一)词向量和语言模型 – licstar的博客\n",
      "http://www.baidu.com/link?url=737jT53wuesSMc5ZDn4ndL7uH5wB5AyybzLqIYsYSWUSrZ1MeJ5IdP7GVLXGmCPL\n",
      "Stanford University CS231n: Convolutional Neural Networks for...\n",
      "http://www.baidu.com/link?url=I0cvVBGnM2TQ-1-3Voz-0WS0YgWlE6cSmBYNgcPjJnH_kF3xl7Yd8Z8IJA6VyGuW\n",
      "http://www.baidu.com/s?wd=deep+learning&pn=30&rsv_srlang=cn\n",
      "<class 'bs4.element.Tag'>\n",
      "Deep Learning Tutorial: Perceptrons to Machine Learning ...\n",
      "http://www.baidu.com/link?url=t17_yk0e4lrMLOP-189_qLYTWNtY0ceNWJN2xprObCnRr2Hpex1o4B88EJHl91ZjZNSOlo-iiFsjHqifsNn_V3dV8wP1hDzBMbx-aE_uXuMQlwPvfHIzOca4_Kerodislm8w1n-BZpiEpEqbkSHHeQ0C5Ib3FJXTLV1JRqNaH9G\n",
      "Caffe | Deep Learning Framework\n",
      "http://www.baidu.com/link?url=yyXjzVf-CvnP5_OGkCGRX5hJl3UNGfK6RnSioLTNfFNveGp6YBScPhESGQkcQXKn\n",
      "[1606.07792] Wide & Deep Learning for Recommender Systems\n",
      "http://www.baidu.com/link?url=J_uFGzrZKoltKiWwmjIe7KDZLyC8ytE9A_H-Kac5ETww7oylarFsF7EZ5zSJ2wEA\n",
      "CS 294 Deep Reinforcement Learning, Fall 2017\n",
      "http://www.baidu.com/link?url=2sYYSYoqLMi8wCY6uGIqpXA8DWhOWealE_h1hF9HfwYRa7OiZvIPKLxVUbRCtlp5\n",
      "Skymind - Deep learning for Enterprise on Hadoop and Spark\n",
      "http://www.baidu.com/link?url=MxWDEFXLABRe7qCu3LqbYUmiLyE_qJP1Fg2-V6zTS-e\n",
      "Deep Learning Summer School, Montreal 2016 - VideoLectures - ...\n",
      "http://www.baidu.com/link?url=T9AStlZi3H-ulMPy3tCa93v-8CHK5dnjT3fN9E9yNTm6JzFR7ktcbuS2iAUaM6Tl3FIzi0mgQAp0ATbgRMWZr_\n",
      "Tsinghua University Deep Learning 2017 Summer School\n",
      "http://www.baidu.com/link?url=tecQ9s1A0YCbFs4wjeFwoGyPMcJDF-9UbvS9qKoYwjCjr071CFYuhYQOIxhxw7nx\n",
      "Richard Socher - Deep Learning Tutorial\n",
      "http://www.baidu.com/link?url=a5eH7v3yqK8itmjQWYHtX9nR4pT7rgnlmVgFDosZ0Q-FFj-W3OMYfE2eaL4sHOChPdv95Xcwu0OijWl-o-9D7ADsuHCdPKL1f_WmCIWqIsbNK7kHEgfp6v00GITbr3vp\n",
      "Deep Learning For Coders—36 hours of lessons for free\n",
      "http://www.baidu.com/link?url=djihIjpAbHQ8mqXjMOr4g7kbN8zgUXQndvY00YTc_eO\n",
      "2013年deeplearning 的最新进展和相关论文\n",
      "http://www.baidu.com/link?url=4lYMlkdLM_dvhGu8ASKo_rfnPpSaUXt9Ugp4vqcri2tlZ_8bFcvfJ0fewo262F4C2wq6yTMwHun5141ByfiA8K\n",
      "http://www.baidu.com/s?wd=deep+learning&pn=40&rsv_srlang=cn\n",
      "<class 'bs4.element.Tag'>\n",
      "Deep Learning Definition | Investopedia\n",
      "http://www.baidu.com/link?url=2hAu1bjxwbY-svCyktUBTROLdl8BN1hRZHx8n5v0z5PziFKKCHtp_Az9FI893G3LtBqEXr8jRk9Cbcc1pW_yca\n",
      "Deeplearning4j: Open-source, Distributed Deep Learning for ...\n",
      "http://www.baidu.com/link?url=mMqzAcMXlJbyAdM3vGMtBCeE06UAwZgs7F78EZok5kaOGdBMw7g5bRHcEGTsUCMK\n",
      "深度学习论文阅读路线图 Deep Learning Papers Reading Roa..._知乎\n",
      "http://www.baidu.com/link?url=k7txLIs0WGkxV7x7jOcrtge_lukX2x6ZYQ1qCXmQx5YzuMz2Y8OA_bVuWUeQcDgV\n",
      "Deep Learning (豆瓣)\n",
      "http://www.baidu.com/link?url=kRC7crT_Y1y_gPbjsGDWnRSc16IaQoHLxHaeXzILEqzaFDzq7Xogqw9GUGvOwlQ8MlKJyXlhmMzoqYKlufIoXa\n",
      "deep learning:Theano安装攻略 - 钰的日志 - 网易博客\n",
      "http://www.baidu.com/link?url=B--vJpUTiK1T94Hz-EpGWw4lhgYJ7pegd01xbkqCoOnbNT6n-ByzbDifQ-_xZY1pFaQG9ZWeqjKZLJUULtisJD4FYxE6_l8RfFqQhZzkrNW\n",
      "11-485/11-685 Deep Learning\n",
      "http://www.baidu.com/link?url=yNI6o8s_W5mTnhsGAzYmArLopwvZO_-KKbG2tpde7Qnei2m0iKka8b2MPcQEcBHe\n",
      "Deep Learning - Quora\n",
      "http://www.baidu.com/link?url=hSi3aJpCCwBBFrWI8t16xNaY7OiUwmlVOdaJzeSyOfSMyJrSnBBofbzMNgiltQYj\n",
      "Stanford University CS224d: Deep Learning for Natural ...\n",
      "http://www.baidu.com/link?url=GizgWKMs7IryxJKvXkL7PKyVjwjO27i56kuFAVY7bfHlov7aXyl_5EX4Ng9QPHSv\n",
      "深度学习(Deep Learning)发展史\n",
      "http://www.baidu.com/link?url=xf5Q01igvJywt3dIBgQLUfu7PcId-vKEAyd6FK_aIRDokxf_eZeFZF2E2P0kx64o7OzsIDrqcfsaCW5cDQBSlMWlaWDoHj0SK5eOBXxVLgm\n",
      "Deep learning(深度学习) - 道客巴巴\n",
      "http://www.baidu.com/link?url=ei0eMcHDGZ77El5XEM1kixdF94b1T7ZRuniyR2CWP1_PeEPpE5xLL2FcdSQV4a3e9qwf0KxQszy2FvQIIlK1c_\n"
     ]
    }
   ],
   "source": [
    "key = raw_input('input key word: ')\n",
    "num_page = int(raw_input('input number of pages: '))\n",
    "for i in range(1, 1 + num_page):\n",
    "    get_baidu_result(key, './en_results/', i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ISO-8859-1\n",
      "Succeed to get the url\n",
      "num_a: 167\n",
      "language: False  enratio: 0.792292870906\n",
      "32\n"
     ]
    }
   ],
   "source": [
    "crawl_main_content('https://baike.baidu.com/item/%E8%85%BE%E8%AE%AF%E5%85%AC%E7%9B%8A%E6%85%88%E5%96%84%E5%9F%BA%E9%87%91%E4%BC%9A', '', 'test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://www.baidu.com/s?wd=wind%26five&pn=10\n"
     ]
    }
   ],
   "source": [
    "r = requests.get('http://www.baidu.com/s?', params={'wd': 'wind&five', 'pn': '10'})\n",
    "print r.url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('./en_results/d.txt', 'w') as f:\n",
    "    f.write('lalala')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "url = 'https://baike.baidu.com/item/%E8%85%BE%E8%AE%AF%E5%85%AC%E7%9B%8A%E6%85%88%E5%96%84%E5%9F%BA%E9%87%91%E4%BC%9A'\n",
    "soup = BeautifulSoup(requests.get(url).content, 'html.parser')\n",
    "div = soup.find_all('div', class_=re.compile('content'), recursive=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<listiterator at 0x7fdb74e87cd0>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.children"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "utf-8\n",
      "Succeed to get the url\n",
      "num_a: 132\n",
      "http://www.citeulike.org/posturl2?username=&url=http://dx.doi.org/10.1038/nature14539&title=Deep+learning&rewrite=yes\n",
      "http://www.facebook.com/sharer.php?u=http://dx.doi.org/10.1038/nature14539&t=Deep+learning\n",
      "http://twitter.com/home/?status=Deep+learning+http://dx.doi.org/10.1038/nature14539\n",
      "http://del.icio.us/post?url=http://dx.doi.org/10.1038/nature14539&title=Deep+learning\n",
      "http://digg.com/submit?phase=2&url=http://dx.doi.org/10.1038/nature14539&title=Deep+learning\n",
      "https://plus.google.com/share?hl=en&url=http://dx.doi.org/10.1038/nature14539&title=Deep+learning\n",
      "http://www.linkedin.com/cws/share?url=http://dx.doi.org/10.1038/nature14539&title=Deep+learning\n",
      "http://www.reddit.com/s/http://dx.doi.org/10.1038/nature14539\n",
      "http://www.stumbleupon.com/submit?url=http://dx.doi.org/10.1038/nature14539&title=Deep+learning\n",
      "https://s100.copyright.com/AppDispatchServlet?publisherName=NPG&publication=Nature&title=Deep+learning&contentID=10.1038%2Fnature14539&volumeNum=521&issueNum=7553&numPages=9&pageNumbers=pp436-444&publicationDate=2015-05-27&author=Yann+LeCun%2C+Yoshua+Bengio%2C+Geoffrey+Hinton\n",
      "http://colah.github.io/\n",
      "http://links.isiglobalnet2.com/gateway/Gateway.cgi?amp;GWVersion=2&amp;SrcAuth=Nature&amp;SrcApp=Nature&amp;DestLinkType=FullRecord&amp;KeyUT=000320381400008&amp;DestApp=WOS_CPL\n",
      "http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?holding=npg&amp;cmd=Retrieve&amp;db=PubMed&amp;list_uids=23787344&amp;dopt=Abstract\n",
      "http://dx.doi.org/10.1109/TPAMI.2012.231\n",
      "http://arxiv.org/abs/1409.4842\n",
      "http://links.isiglobalnet2.com/gateway/Gateway.cgi?amp;GWVersion=2&amp;SrcAuth=Nature&amp;SrcApp=Nature&amp;DestLinkType=FullRecord&amp;KeyUT=000310345000010&amp;DestApp=WOS_CPL\n",
      "http://dx.doi.org/10.1109/MSP.2012.2205597\n",
      "http://chemport.cas.org/cgi-bin/sdcgi?APP=ftslink&action=reflink&origin=npg&version=1.0&coi=1:CAS:528:DC%2BC2MXhvFGns70%3D&pissn=0028-0836&pyear=2015&md5=138ea485d4f122823538c6a9cfe75084\n",
      "http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?holding=npg&amp;cmd=Retrieve&amp;db=PubMed&amp;list_uids=25635324&amp;dopt=Abstract\n",
      "http://dx.doi.org/10.1021/ci500747n\n",
      "http://chemport.cas.org/cgi-bin/sdcgi?APP=ftslink&action=reflink&origin=npg&version=1.0&coi=1:CAS:528:DC%2BC38Xht1SqsrjF&pissn=0028-0836&pyear=2015&md5=96e0f8e3873ea542b082189fa3386a47\n",
      "http://dx.doi.org/10.1088/1742-6596/368/1/012030\n",
      "https://www.kaggle.com/c/higgs-boson\n",
      "http://chemport.cas.org/cgi-bin/sdcgi?APP=ftslink&action=reflink&origin=npg&version=1.0&coi=1:CAS:528:DC%2BC3sXht1emtrnJ&pissn=0028-0836&pyear=2015&md5=a7ae31a55ea3671463a67c8068f3b2ab\n",
      "http://links.isiglobalnet2.com/gateway/Gateway.cgi?amp;GWVersion=2&amp;SrcAuth=Nature&amp;SrcApp=Nature&amp;DestLinkType=FullRecord&amp;KeyUT=000322825500027&amp;DestApp=WOS_CPL\n",
      "http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?holding=npg&amp;cmd=Retrieve&amp;db=PubMed&amp;list_uids=23925239&amp;dopt=Abstract\n",
      "http://chemport.cas.org/cgi-bin/sdcgi?APP=ftslink&action=reflink&origin=npg&version=1.0&coi=1:CAS:528:DC%2BC2cXpvFCqsLg%3D&pissn=0028-0836&pyear=2015&md5=3da19f675748277f327df9d90a212176\n",
      "http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?holding=npg&amp;cmd=Retrieve&amp;db=PubMed&amp;list_uids=24931975&amp;dopt=Abstract\n",
      "http://dx.doi.org/10.1093/bioinformatics/btu277\n",
      "http://chemport.cas.org/cgi-bin/sdcgi?APP=ftslink&action=reflink&origin=npg&version=1.0&coi=1:CAS:528:DC%2BC2MXitFeltg%3D%3D&pissn=0028-0836&pyear=2015&md5=c6a7f55873f2355f6a6606596ecda7ad\n",
      "http://dx.doi.org/10.1126/science.1254806\n",
      "http://links.isiglobalnet2.com/gateway/Gateway.cgi?amp;GWVersion=2&amp;SrcAuth=Nature&amp;SrcApp=Nature&amp;DestLinkType=FullRecord&amp;KeyUT=000298102200003&amp;DestApp=WOS_CPL\n",
      "http://arxiv.org/abs/1406.3676v3\n",
      "http://arxiv.org/abs/1412.2007\n",
      "http://links.isiglobalnet2.com/gateway/Gateway.cgi?amp;GWVersion=2&amp;SrcAuth=Nature&amp;SrcApp=Nature&amp;DestLinkType=FullRecord&amp;KeyUT=A1986E327500055&amp;DestApp=WOS_CPL\n",
      "http://arxiv.org/abs/1412.0233\n",
      "http://dx.doi.org/10.1162/neco.2006.18.7.1527\n",
      "http://chemport.cas.org/cgi-bin/sdcgi?APP=ftslink&action=reflink&origin=npg&version=1.0&coi=1:CAS:528:DC%2BD28Xnt1KntrY%3D&pissn=0028-0836&pyear=2015&md5=832bf43b8d636787e5dc90de46fa3735\n",
      "http://links.isiglobalnet2.com/gateway/Gateway.cgi?amp;GWVersion=2&amp;SrcAuth=Nature&amp;SrcApp=Nature&amp;DestLinkType=FullRecord&amp;KeyUT=000239308600057&amp;DestApp=WOS_CPL\n",
      "http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?holding=npg&amp;cmd=Retrieve&amp;db=PubMed&amp;list_uids=16873662&amp;dopt=Abstract\n",
      "http://dx.doi.org/10.1126/science.1127647\n",
      "http://arxiv.org/abs/1212.0142\n",
      "http://links.isiglobalnet2.com/gateway/Gateway.cgi?amp;GWVersion=2&amp;SrcAuth=Nature&amp;SrcApp=Nature&amp;DestLinkType=FullRecord&amp;KeyUT=000298325600005&amp;DestApp=WOS_CPL\n",
      "http://dx.doi.org/10.1109/TASL.2011.2109382\n",
      "http://dx.doi.org/10.1109/TASL.2011.2134090\n",
      "http://dx.doi.org/10.1109/TPAMI.2013.50\n",
      "http://links.isiglobalnet2.com/gateway/Gateway.cgi?amp;GWVersion=2&amp;SrcAuth=Nature&amp;SrcApp=Nature&amp;DestLinkType=FullRecord&amp;KeyUT=000076557300010&amp;DestApp=WOS_CPL\n",
      "http://dx.doi.org/10.1109/5.726791\n",
      "http://chemport.cas.org/cgi-bin/sdcgi?APP=ftslink&action=reflink&origin=npg&version=1.0&coi=1:STN:280:CC2D2M7gtFY%3D&pissn=0028-0836&pyear=2015&md5=9e2ca3fe844124dc62d7674d1c178a62\n",
      "http://links.isiglobalnet2.com/gateway/Gateway.cgi?amp;GWVersion=2&amp;SrcAuth=Nature&amp;SrcApp=Nature&amp;DestLinkType=FullRecord&amp;KeyUT=A19626729B00007&amp;DestApp=WOS_CPL\n",
      "http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?holding=npg&amp;cmd=Retrieve&amp;db=PubMed&amp;list_uids=14449617&amp;dopt=Abstract\n",
      "http://dx.doi.org/10.1113/jphysiol.1962.sp006837\n",
      "http://chemport.cas.org/cgi-bin/sdcgi?APP=ftslink&action=reflink&origin=npg&version=1.0&coi=1:STN:280:By2A2MzlsFQ%3D&pissn=0028-0836&pyear=2015&md5=e9bea0cca53abeca2c29b88f674550ef\n",
      "http://links.isiglobalnet2.com/gateway/Gateway.cgi?amp;GWVersion=2&amp;SrcAuth=Nature&amp;SrcApp=Nature&amp;DestLinkType=FullRecord&amp;KeyUT=000208047200002&amp;DestApp=WOS_CPL\n",
      "http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?holding=npg&amp;cmd=Retrieve&amp;db=PubMed&amp;list_uids=1822724&amp;dopt=Abstract\n",
      "http://dx.doi.org/10.1093/cercor/1.1.1-a\n",
      "http://dx.doi.org/10.1371/journal.pcbi.1003963\n",
      "http://links.isiglobalnet2.com/gateway/Gateway.cgi?amp;GWVersion=2&amp;SrcAuth=Nature&amp;SrcApp=Nature&amp;DestLinkType=FullRecord&amp;KeyUT=A1982PN35900003&amp;DestApp=WOS_CPL\n",
      "http://dx.doi.org/10.1016/0031-3203(82)90024-3\n",
      "http://links.isiglobalnet2.com/gateway/Gateway.cgi?amp;GWVersion=2&amp;SrcAuth=Nature&amp;SrcApp=Nature&amp;DestLinkType=FullRecord&amp;KeyUT=A1989T335400002&amp;DestApp=WOS_CPL\n",
      "http://dx.doi.org/10.1109/29.21701\n",
      "http://chemport.cas.org/cgi-bin/sdcgi?APP=ftslink&action=reflink&origin=npg&version=1.0&coi=1:STN:280:DC%2BD1c%2FpvVamug%3D%3D&pissn=0028-0836&pyear=2015&md5=069dc4c0110f6357d72e35776e2b7f30\n",
      "http://links.isiglobalnet2.com/gateway/Gateway.cgi?amp;GWVersion=2&amp;SrcAuth=Nature&amp;SrcApp=Nature&amp;DestLinkType=FullRecord&amp;KeyUT=A1997WA61200010&amp;DestApp=WOS_CPL\n",
      "http://dx.doi.org/10.1109/72.554195\n",
      "http://links.isiglobalnet2.com/gateway/Gateway.cgi?amp;GWVersion=2&amp;SrcAuth=Nature&amp;SrcApp=Nature&amp;DestLinkType=FullRecord&amp;KeyUT=000306162600036&amp;DestApp=WOS_CPL\n",
      "http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?holding=npg&amp;cmd=Retrieve&amp;db=PubMed&amp;list_uids=22386783&amp;dopt=Abstract\n",
      "http://dx.doi.org/10.1016/j.neunet.2012.02.023\n",
      "http://links.isiglobalnet2.com/gateway/Gateway.cgi?amp;GWVersion=2&amp;SrcAuth=Nature&amp;SrcApp=Nature&amp;DestLinkType=FullRecord&amp;KeyUT=000231319400013&amp;DestApp=WOS_CPL\n",
      "http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?holding=npg&amp;cmd=Retrieve&amp;db=PubMed&amp;list_uids=16190471&amp;dopt=Abstract\n",
      "http://dx.doi.org/10.1109/TIP.2005.852470\n",
      "http://links.isiglobalnet2.com/gateway/Gateway.cgi?amp;GWVersion=2&amp;SrcAuth=Nature&amp;SrcApp=Nature&amp;DestLinkType=FullRecord&amp;KeyUT=000273897500007&amp;DestApp=WOS_CPL\n",
      "http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?holding=npg&amp;cmd=Retrieve&amp;db=PubMed&amp;list_uids=19922289&amp;dopt=Abstract\n",
      "http://dx.doi.org/10.1162/neco.2009.10-08-881\n",
      "http://dx.doi.org/10.1109/TPAMI.2004.97\n",
      "http://links.isiglobalnet2.com/gateway/Gateway.cgi?amp;GWVersion=2&amp;SrcAuth=Nature&amp;SrcApp=Nature&amp;DestLinkType=FullRecord&amp;KeyUT=000248351700011&amp;DestApp=WOS_CPL\n",
      "http://arxiv.org/abs/1411.4280\n",
      "http://links.isiglobalnet2.com/gateway/Gateway.cgi?amp;GWVersion=2&amp;SrcAuth=Nature&amp;SrcApp=Nature&amp;DestLinkType=FullRecord&amp;KeyUT=000263004900003&amp;DestApp=WOS_CPL\n",
      "http://dx.doi.org/10.1002/rob.20276\n",
      "http://arxiv.org/abs/1202.2160\n",
      "http://links.isiglobalnet2.com/gateway/Gateway.cgi?amp;GWVersion=2&amp;SrcAuth=Nature&amp;SrcApp=Nature&amp;DestLinkType=FullRecord&amp;KeyUT=000344638300002&amp;DestApp=WOS_CPL\n",
      "http://arxiv.org/abs/1312.6229\n",
      "http://arxiv.org/abs/1409.1556\n",
      "http://links.isiglobalnet2.com/gateway/Gateway.cgi?amp;GWVersion=2&amp;SrcAuth=Nature&amp;SrcApp=Nature&amp;DestLinkType=FullRecord&amp;KeyUT=A1991GQ90500035&amp;DestApp=WOS_CPL\n",
      "http://dx.doi.org/10.1109/4.104196\n",
      "http://dx.doi.org/10.1137/140957081\n",
      "http://links.isiglobalnet2.com/gateway/Gateway.cgi?amp;GWVersion=2&amp;SrcAuth=Nature&amp;SrcApp=Nature&amp;DestLinkType=FullRecord&amp;KeyUT=000245355600005&amp;DestApp=WOS_CPL\n",
      "http://dx.doi.org/10.1016/j.csl.2006.09.003\n",
      "http://arxiv.org/abs/1409.0473\n",
      "http://chemport.cas.org/cgi-bin/sdcgi?APP=ftslink&action=reflink&origin=npg&version=1.0&coi=1:STN:280:DC%2BD1c7gvFansQ%3D%3D&pissn=0028-0836&pyear=2015&md5=8ba9715320c004163d0896ba95a069ec\n",
      "http://links.isiglobalnet2.com/gateway/Gateway.cgi?amp;GWVersion=2&amp;SrcAuth=Nature&amp;SrcApp=Nature&amp;DestLinkType=FullRecord&amp;KeyUT=A1994NH76800002&amp;DestApp=WOS_CPL\n",
      "http://dx.doi.org/10.1109/72.279181\n",
      "http://chemport.cas.org/cgi-bin/sdcgi?APP=ftslink&action=reflink&origin=npg&version=1.0&coi=1:STN:280:DyaK1c%2FhvVahsQ%3D%3D&pissn=0028-0836&pyear=2015&md5=27b8ddf640ecc244e0b7612cf3e73fc9\n",
      "http://links.isiglobalnet2.com/gateway/Gateway.cgi?amp;GWVersion=2&amp;SrcAuth=Nature&amp;SrcApp=Nature&amp;DestLinkType=FullRecord&amp;KeyUT=A1997YA04500007&amp;DestApp=WOS_CPL\n",
      "http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?holding=npg&amp;cmd=Retrieve&amp;db=PubMed&amp;list_uids=9377276&amp;dopt=Abstract\n",
      "http://dx.doi.org/10.1162/neco.1997.9.8.1735\n",
      "http://papers.nips.cc/paper/1102-hierarchical-recurrent-neural-networks-for-long-term-dependencies\n",
      "http://arxiv.org/abs/1502.03044\n",
      "http://arxiv.org/abs/1410.5401\n",
      "http://arxiv.org/abs/1410.3916\n",
      "http://arxiv.org/abs/1502.05698\n",
      "http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?holding=npg&amp;cmd=Retrieve&amp;db=PubMed&amp;list_uids=7777849&amp;dopt=Abstract\n",
      "http://dx.doi.org/10.1126/science.7761831\n",
      "http://dx.doi.org/10.1109/TPAMI.2013.29\n",
      "http://arxiv.org/abs/1412.7755\n",
      "http://chemport.cas.org/cgi-bin/sdcgi?APP=ftslink&action=reflink&origin=npg&version=1.0&coi=1:CAS:528:DC%2BC2MXjsVagur0%3D&pissn=0028-0836&pyear=2015&md5=a4af97dee067ee16090ab6a91b0a2c6a\n",
      "http://links.isiglobalnet2.com/gateway/Gateway.cgi?amp;GWVersion=2&amp;SrcAuth=Nature&amp;SrcApp=Nature&amp;DestLinkType=FullRecord&amp;KeyUT=000350097300045&amp;DestApp=WOS_CPL\n",
      "http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?holding=npg&amp;cmd=Retrieve&amp;db=PubMed&amp;list_uids=25719670&amp;dopt=Abstract\n",
      "http://links.isiglobalnet2.com/gateway/Gateway.cgi?amp;GWVersion=2&amp;SrcAuth=Nature&amp;SrcApp=Nature&amp;DestLinkType=FullRecord&amp;KeyUT=000330616900002&amp;DestApp=WOS_CPL\n",
      "http://dx.doi.org/10.1007/s10994-013-5335-x\n",
      "http://arxiv.org/abs/1502.03044\n",
      "http://www.nature.com/reprints\n",
      "http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&term=Yann+LeCun%5Bauthor%5D\n",
      "http://scholar.google.co.uk/scholar?as_q=&num=10&btnG=Search+Scholar&as_epq=&as_oq=&as_eq=&as_occt=any&as_sauthors=Yann+LeCun&as_publication=&as_ylo=&as_yhi=&as_allsubj=all&hl=en\n",
      "http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&term=Yoshua+Bengio%5Bauthor%5D\n",
      "http://scholar.google.co.uk/scholar?as_q=&num=10&btnG=Search+Scholar&as_epq=&as_oq=&as_eq=&as_occt=any&as_sauthors=Yoshua+Bengio&as_publication=&as_ylo=&as_yhi=&as_allsubj=all&hl=en\n",
      "http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&term=Geoffrey+Hinton%5Bauthor%5D\n",
      "http://scholar.google.co.uk/scholar?as_q=&num=10&btnG=Search+Scholar&as_epq=&as_oq=&as_eq=&as_occt=any&as_sauthors=Geoffrey+Hinton&as_publication=&as_ylo=&as_yhi=&as_allsubj=all&hl=en\n",
      "http://www.nature.com/doifinder/10.1038/548512a\n",
      "http://www.nature.com/doifinder/10.1038/548512a\n",
      "http://www.nature.com/naturejobs/science/jobs/601537-multiple-postdoctoral-fellowships-in-cardiac-signal-processing-and-instrumentation-boston-ma-united-states\n",
      "http://www.nature.com/naturejobs/science/jobs/599981-assistant-associate-or-full-professor\n",
      "http://www.nature.com/naturejobs/science/jobs/602685-associate-editor-senior-editor-roles-nature-research-talent-pool-2017\n",
      "http://www.nature.com/natureevents/science/events/61443-Asia_Pacific_Vascular_Biology_Organization_Conference\n",
      "http://www.nature.com/natureevents/science/events/55077-22nd_World_Congress_on_Advances_in_Oncology_and_20th_International_Symposium_on_Molecular_Medicine\n",
      "http://www.nature.com/natureevents/science/events/56019-The_2017_GCHERA_World_Agriculture_Prize_Awarding_Ceremony_World_Dialogue\n",
      "http://dx.doi.org/10.1038/srep45938\n",
      "http://dx.doi.org/10.1038/srep46479\n",
      "http://dx.doi.org/10.1038/srep46450\n",
      "http://www.nature.com/doifinder/10.1038/nature23305\n",
      "http://www.nature.com/doifinder/10.1038/nature23465\n",
      "http://www.nature.com/doifinder/10.1038/548520a\n",
      "http://www.natureasia.com\n",
      "15\n",
      "\n",
      "15\n",
      "<!doctype html>\n",
      "0\n",
      "40\n",
      "\n",
      "40\n",
      "\n",
      "40\n",
      "Deep learning : Nature : Nature Research\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "20\n",
      "\n",
      "38\n",
      "\n",
      "38\n",
      "Jump to main content\n",
      "18\n",
      "Jump to navigation\n",
      "0\n",
      "0\n",
      "56\n",
      "\n",
      "85\n",
      "\n",
      "85\n",
      "We use cookies to improve your experience with our site.\n",
      "29\n",
      "Accept and close | More info.\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "19\n",
      "\n",
      "19\n",
      "\n",
      "41\n",
      "nature.com homepage\n",
      "39\n",
      "\n",
      "39\n",
      "Publications A-Z index\n",
      "17\n",
      "Browse by subject\n",
      "0\n",
      "4\n",
      "\n",
      "4\n",
      "\n",
      "9\n",
      "Cart\n",
      "5\n",
      "\n",
      "13\n",
      "Login\n",
      "8\n",
      "\n",
      "8\n",
      "Register\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "6\n",
      "\n",
      "45\n",
      "\n",
      "45\n",
      "Nature\n",
      "39\n",
      "International weekly journal of science\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "18\n",
      "\n",
      "18\n",
      "\n",
      "18\n",
      " GoAdvanced search\n",
      "0\n",
      "0\n",
      "0\n",
      "8\n",
      "\n",
      "8\n",
      "\n",
      "12\n",
      "MenuMenu\n",
      "18\n",
      "\n",
      "26\n",
      "Home\n",
      "36\n",
      "News & Comment\n",
      "35\n",
      "Research\n",
      "34\n",
      "Careers & Jobs\n",
      "33\n",
      "Current Issue\n",
      "31\n",
      "Archive\n",
      "24\n",
      "Audio & Video\n",
      "11\n",
      "For Authors\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "7\n",
      "\n",
      "17\n",
      "\n",
      "27\n",
      "Archive\n",
      "28\n",
      "Volume 521\n",
      "25\n",
      "Issue 7553\n",
      "22\n",
      "Insights\n",
      "14\n",
      "Reviews\n",
      "7\n",
      "Article\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "8\n",
      "\n",
      "17\n",
      "\n",
      "23\n",
      "Nature |\n",
      "15\n",
      "Insight |\n",
      "6\n",
      "Review\n",
      "0\n",
      "0\n",
      "0\n",
      "5\n",
      "\n",
      "5\n",
      "\n",
      "19\n",
      "Print\n",
      "14\n",
      "\n",
      "14\n",
      "Share/bookmark\n",
      "0\n",
      "0\n",
      "0\n",
      "11\n",
      "\n",
      "19\n",
      "\n",
      "26\n",
      "Cite U Like\n",
      "24\n",
      "Facebook\n",
      "20\n",
      "Twitter\n",
      "19\n",
      "Delicious\n",
      "18\n",
      "Digg\n",
      "20\n",
      "Google\n",
      "25\n",
      "LinkedIn\n",
      "17\n",
      "Reddit\n",
      "11\n",
      "StumbleUpon\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "16\n",
      "\n",
      "16\n",
      "\n",
      "22\n",
      "Previous article\n",
      "9\n",
      "\n",
      "16\n",
      "Nature\n",
      "10\n",
      " | \n",
      "27\n",
      "Insight\n",
      "20\n",
      "\n",
      "20\n",
      "Machine intelligence\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "12\n",
      "\n",
      "12\n",
      "\n",
      "18\n",
      "Next article\n",
      "9\n",
      "\n",
      "16\n",
      "Nature\n",
      "10\n",
      " | \n",
      "73\n",
      "Insight\n",
      "66\n",
      "\n",
      "66\n",
      "Reinforcement learning improves behaviour from evaluative feedback\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "13\n",
      "\n",
      "13\n",
      "\n",
      "13\n",
      "Deep learning\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "16\n",
      "\n",
      "16\n",
      "\n",
      "16\n",
      "Yann LeCun1, 2, \n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "16\n",
      "\n",
      "16\n",
      "\n",
      "16\n",
      "Yoshua Bengio3, \n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "21\n",
      "\n",
      "21\n",
      "\n",
      "21\n",
      "Geoffrey Hinton4, 5, \n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "12\n",
      "\n",
      "32\n",
      "\n",
      "32\n",
      "Affiliations\n",
      "20\n",
      "Corresponding author\n",
      "0\n",
      "13\n",
      "\n",
      "19\n",
      "\n",
      "26\n",
      "Journal name:\n",
      "13\n",
      "Nature\n",
      "11\n",
      "Volume:\n",
      "10\n",
      "\n",
      "16\n",
      "521,\n",
      "28\n",
      "Pages:\n",
      "35\n",
      "436444\n",
      "34\n",
      " Date published:\n",
      "41\n",
      "(28 May 2015)\n",
      "28\n",
      " DOI:\n",
      "23\n",
      "doi:10.1038/nature14539\n",
      "0\n",
      "8\n",
      "\n",
      "8\n",
      "\n",
      "24\n",
      "Received\n",
      "16\n",
      "\n",
      "25\n",
      "25 February 2015\n",
      "9\n",
      "\n",
      "20\n",
      " Accepted\n",
      "11\n",
      "\n",
      "11\n",
      "01 May 2015\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "16\n",
      "\n",
      "16\n",
      "\n",
      "16\n",
      "Published online\n",
      "11\n",
      "\n",
      "11\n",
      "\n",
      "11\n",
      "27 May 2015\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "13\n",
      "\n",
      "13\n",
      "\n",
      "23\n",
      "Article tools\n",
      "10\n",
      "\n",
      "14\n",
      " Full text\n",
      "4\n",
      "\n",
      "12\n",
      " PDF\n",
      "8\n",
      "\n",
      "28\n",
      "Citation\n",
      "20\n",
      "\n",
      "20\n",
      "Rights & permissions\n",
      "0\n",
      "15\n",
      "\n",
      "15\n",
      "\n",
      "15\n",
      "Article metrics\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "8\n",
      "\n",
      "8\n",
      "\n",
      "8\n",
      "Abstract\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "38\n",
      "\n",
      "38\n",
      "\n",
      "38\n",
      "Abstract References Author information\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "807\n",
      "\n",
      "807\n",
      "\n",
      "821\n",
      "Deep learning allows computational models that are composed of multiple processing layers to learn representations of data with multiple levels of abstraction. These methods have dramatically improved the state-of-the-art in speech recognition, visual object recognition, object detection and many other domains such as drug discovery and genomics. Deep learning discovers intricate structure in large data sets by using the backpropagation algorithm to indicate how a machine should change its internal parameters that are used to compute the representation in each layer from the representation in the previous layer. Deep convolutional nets have brought about breakthroughs in processing images, video, speech and audio, whereas recurrent nets have shone light on sequential data such as text and speech.\n",
      "14\n",
      "\n",
      "14\n",
      "View full text\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "14\n",
      "\n",
      "14\n",
      "\n",
      "56\n",
      "Subject terms:\n",
      "42\n",
      "\n",
      "42\n",
      "Mathematics and computing Computer science\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "11\n",
      "\n",
      "11\n",
      "\n",
      "11\n",
      "At a glance\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "7\n",
      "\n",
      "7\n",
      "\n",
      "7\n",
      "Figures\n",
      "0\n",
      "16\n",
      "\n",
      "16\n",
      "\n",
      "16\n",
      "View all figures\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "57\n",
      "\n",
      "57\n",
      "\n",
      "57\n",
      "Figure 1: Multilayer neural networks and backpropagation.\n",
      "2711\n",
      "\n",
      "2711\n",
      "\n",
      "2711\n",
      "a, A multi-layer neural network (shown by the connected dots) can distort the input space to make the classes of data (examples of which are on the red and blue lines) linearly separable. Note how a regular grid (shown on the left) in input space is also transformed (shown in the middle panel) by hidden units. This is an illustrative example with only two input units, two hidden units and one output unit, but the networks used for object recognition or natural language processing contain tens or hundreds of thousands of units. Reproduced with permission from C. Olah (http://colah.github.io/). b, The chain rule of derivatives tells us how two small effects (that of a small change of x on y, and that of y on z) are composed. A small change x in x gets transformed first into a small change y in y by getting multiplied by y/x (that is, the definition of partial derivative). Similarly, the change y creates a change z in z. Substituting one equation into the other gives the chain rule of derivatives  how x gets turned into z through multiplication by the product of y/x and z/x. It also works when x, y and z are vectors (and the derivatives are Jacobian matrices). c, The equations used for computing the forward pass in a neural net with two hidden layers and one output layer, each constituting a module through which one can backpropagate gradients. At each layer, we first compute the total input z to each unit, which is a weighted sum of the outputs of the units in the layer below. Then a non-linear function f(.) is applied to z to get the output of the unit. For simplicity, we have omitted bias terms. The non-linear functions used in neural networks include the rectified linear unit (ReLU) f(z) = max(0,z), commonly used in recent years, as well as the more conventional sigmoids, such as the hyberbolic tangent, f(z) = (exp(z)  exp(z))/(exp(z) + exp(z)) and logistic function logistic, f(z) = 1/(1 + exp(z)). d, The equations used for computing the backward pass. At each hidden layer we compute the error derivative with respect to the output of each unit, which is a weighted sum of the error derivatives with respect to the total inputs to the units in the layer above. We then convert the error derivative with respect to the output into the error derivative with respect to the input by multiplying it by the gradient of f(z). At the output layer, the error derivative with respect to the output of a unit is computed by differentiating the cost function. This gives yl  tl if the cost function for unit l is 0.5(yl  tl)2, where tl is the target value. Once the E/zk is known, the error-derivative for the weight wjk on the connection from unit j in the layer below is just yj E/zk.\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "41\n",
      "\n",
      "41\n",
      "\n",
      "41\n",
      "Figure 2: Inside a convolutional network.\n",
      "525\n",
      "\n",
      "525\n",
      "\n",
      "525\n",
      "The outputs (not the filters) of each layer (horizontally) of a typical convolutional network architecture applied to the image of a Samoyed dog (bottom left; and RGB (red, green, blue) inputs, bottom right). Each rectangular image is a feature map corresponding to the output for one of the learned features, detected at each of the image positions. Information flows bottom up, with lower-level features acting as oriented edge detectors, and a score is computed for each image class in output. ReLU, rectified linear unit.\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "29\n",
      "\n",
      "29\n",
      "\n",
      "29\n",
      "Figure 3: From image to text.\n",
      "592\n",
      "\n",
      "592\n",
      "\n",
      "592\n",
      "Captions generated by a recurrent neural network (RNN) taking, as extra input, the representation extracted by a deep convolution neural network (CNN) from a test image, with the RNN trained to translate high-level representations of images into captions (top). Reproduced with permission from ref. 102. When the RNN is given the ability to focus its attention on a different location in the input image (middle and bottom; the lighter patches were given more attention) as it generates each word (bold), we found86 that it exploits this to achieve better translation of images into captions.\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "47\n",
      "\n",
      "47\n",
      "\n",
      "47\n",
      "Figure 4: Visualizing the learned word vectors.\n",
      "701\n",
      "\n",
      "701\n",
      "\n",
      "701\n",
      "On the left is an illustration of word representations learned for modelling language, non-linearly projected to 2D for visualization using the t-SNE algorithm103. On the right is a 2D representation of phrases learned by an English-to-French encoderdecoder recurrent neural network75. One can observe that semantically similar words or sequences of words are mapped to nearby representations. The distributed representations of words are obtained by using backpropagation to jointly learn a representation for each word and a function that predicts a target quantity such as the next word in a sequence (for language modelling) or a whole sequence of translated words (for machine translation)18, 75.\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "118\n",
      "\n",
      "118\n",
      "\n",
      "118\n",
      "Figure 5: A recurrent neural network and the unfolding in time of the computation involved in its forward computation.\n",
      "995\n",
      "\n",
      "995\n",
      "\n",
      "995\n",
      "The artificial neurons (for example, hidden units grouped under node s with values st at time t) get inputs from other neurons at previous time steps (this is represented with the black square, representing a delay of one time step, on the left). In this way, a recurrent neural network can map an input sequence with elements xt into an output sequence with elements ot, with each ot depending on all the previous xt (for t  t). The same parameters (matrices U,V,W) are used at each time step. Many other architectures are possible, including a variant in which the network can generate a sequence of outputs (for example, words), each of which is used as inputs for the next time step. The backpropagation algorithm (Fig. 1) can be directly applied to the computational graph of the unfolded network on the right, to compute the derivative of a total error (for example, the log-probability of generating the right sequence of outputs) with respect to all the states st and all the parameters.\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "10\n",
      "\n",
      "10\n",
      "\n",
      "10\n",
      "References\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "38\n",
      "\n",
      "38\n",
      "\n",
      "38\n",
      "Abstract References Author information\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "388\n",
      "\n",
      "388\n",
      "\n",
      "388\n",
      "Krizhevsky, A., Sutskever, I. & Hinton, G. ImageNet classification with deep convolutional neural networks. In Proc. Advances in Neural Information Processing Systems 25 10901098 (2012). This report was a breakthrough that used convolutional nets to almost halve the error rate for object recognition, and precipitated the rapid adoption of deep learning by the computer vision community.\n",
      "0\n",
      "0\n",
      "160\n",
      "\n",
      "160\n",
      "\n",
      "160\n",
      "Farabet, C., Couprie, C., Najman, L. & LeCun, Y. Learning hierarchical features for scene labeling. IEEE Trans. Pattern Anal. Mach. Intell. 35, 19151929 (2013).\n",
      "3\n",
      "\n",
      "3\n",
      "\n",
      "9\n",
      "ISI\n",
      "6\n",
      "\n",
      "13\n",
      "PubMed\n",
      "7\n",
      "\n",
      "7\n",
      "Article\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "216\n",
      "\n",
      "216\n",
      "\n",
      "216\n",
      "Tompson, J., Jain, A., LeCun, Y. & Bregler, C. Joint training of a convolutional network and a graphical model for human pose estimation. In Proc. Advances in Neural Information Processing Systems 27 17991807 (2014).\n",
      "0\n",
      "0\n",
      "101\n",
      "\n",
      "101\n",
      "\n",
      "101\n",
      "Szegedy, C. et al. Going deeper with convolutions. Preprint at http://arxiv.org/abs/1409.4842 (2014).\n",
      "0\n",
      "0\n",
      "200\n",
      "\n",
      "200\n",
      "\n",
      "200\n",
      "Mikolov, T., Deoras, A., Povey, D., Burget, L. & Cernocky, J. Strategies for training large scale neural network language models. In Proc. Automatic Speech Recognition and Understanding 196201 (2011).\n",
      "0\n",
      "0\n",
      "388\n",
      "\n",
      "388\n",
      "\n",
      "388\n",
      "Hinton, G. et al. Deep neural networks for acoustic modeling in speech recognition. IEEE Signal Processing Magazine 29, 8297 (2012). This joint paper from the major speech recognition laboratories, summarizing the breakthrough achieved with deep learning on the task of phonetic classification for automatic speech recognition, was the first major industrial application of deep learning.\n",
      "3\n",
      "\n",
      "3\n",
      "\n",
      "10\n",
      "ISI\n",
      "7\n",
      "\n",
      "7\n",
      "Article\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "172\n",
      "\n",
      "172\n",
      "\n",
      "172\n",
      "Sainath, T., Mohamed, A.-R., Kingsbury, B. & Ramabhadran, B. Deep convolutional neural networks for LVCSR. In Proc. Acoustics, Speech and Signal Processing 86148618 (2013).\n",
      "0\n",
      "0\n",
      "180\n",
      "\n",
      "180\n",
      "\n",
      "180\n",
      "Ma, J., Sheridan, R. P., Liaw, A., Dahl, G. E. & Svetnik, V. Deep neural nets as a method for quantitative structure-activity relationships. J. Chem. Inf. Model. 55, 263274 (2015).\n",
      "3\n",
      "\n",
      "3\n",
      "\n",
      "9\n",
      "CAS\n",
      "6\n",
      "\n",
      "13\n",
      "PubMed\n",
      "7\n",
      "\n",
      "7\n",
      "Article\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "185\n",
      "\n",
      "185\n",
      "\n",
      "185\n",
      "Ciodaro, T., Deva, D., de Seixas, J. & Damazio, D. Online particle detection with neural networks based on topological calorimetry information. J. Phys. Conf. Series 368, 012030 (2012).\n",
      "3\n",
      "\n",
      "3\n",
      "\n",
      "10\n",
      "CAS\n",
      "7\n",
      "\n",
      "7\n",
      "Article\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "99\n",
      "\n",
      "99\n",
      "\n",
      "99\n",
      "Kaggle. Higgs boson machine learning challenge. Kaggle https://www.kaggle.com/c/higgs-boson (2014).\n",
      "0\n",
      "0\n",
      "127\n",
      "\n",
      "127\n",
      "\n",
      "127\n",
      "Helmstaedter, M. et al. Connectomic reconstruction of the inner plexiform layer in the mouse retina. Nature 500, 168174 (2013).\n",
      "3\n",
      "\n",
      "3\n",
      "\n",
      "6\n",
      "CAS\n",
      "3\n",
      "\n",
      "9\n",
      "ISI\n",
      "6\n",
      "\n",
      "13\n",
      "PubMed\n",
      "7\n",
      "\n",
      "7\n",
      "Article\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "141\n",
      "\n",
      "141\n",
      "\n",
      "141\n",
      "Leung, M. K., Xiong, H. Y., Lee, L. J. & Frey, B. J. Deep learning of the tissue-regulated splicing code. Bioinformatics 30, i121i129 (2014).\n",
      "3\n",
      "\n",
      "3\n",
      "\n",
      "9\n",
      "CAS\n",
      "6\n",
      "\n",
      "13\n",
      "PubMed\n",
      "7\n",
      "\n",
      "7\n",
      "Article\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "132\n",
      "\n",
      "132\n",
      "\n",
      "132\n",
      "Xiong, H. Y. et al. The human splicing code reveals new insights into the genetic determinants of disease. Science 347, 6218 (2015).\n",
      "3\n",
      "\n",
      "3\n",
      "\n",
      "10\n",
      "CAS\n",
      "7\n",
      "\n",
      "7\n",
      "Article\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "114\n",
      "\n",
      "114\n",
      "\n",
      "114\n",
      "Collobert, R., et al. Natural language processing (almost) from scratch. J. Mach. Learn. Res. 12, 24932537 (2011).\n",
      "3\n",
      "\n",
      "3\n",
      "\n",
      "3\n",
      "ISI\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "179\n",
      "\n",
      "179\n",
      "\n",
      "179\n",
      "Bordes, A., Chopra, S. & Weston, J. Question answering with subgraph embeddings. In Proc. Empirical Methods in Natural Language Processing http://arxiv.org/abs/1406.3676v3 (2014).\n",
      "0\n",
      "0\n",
      "174\n",
      "\n",
      "174\n",
      "\n",
      "174\n",
      "Jean, S., Cho, K., Memisevic, R. & Bengio, Y. On using very large target vocabulary for neural machine translation. In Proc. ACL-IJCNLP http://arxiv.org/abs/1412.2007 (2015).\n",
      "0\n",
      "0\n",
      "443\n",
      "\n",
      "443\n",
      "\n",
      "443\n",
      "Sutskever, I. Vinyals, O. & Le. Q. V. Sequence to sequence learning with neural networks. In Proc. Advances in Neural Information Processing Systems 27 31043112 (2014). This paper showed state-of-the-art machine translation results with the architecture introduced in ref. 72, with a recurrent network trained to read a sentence in one language, produce a semantic representation of its meaning, and generate a translation in another language.\n",
      "0\n",
      "0\n",
      "141\n",
      "\n",
      "141\n",
      "\n",
      "141\n",
      "Bottou, L. & Bousquet, O. The tradeoffs of large scale learning. In Proc. Advances in Neural Information Processing Systems 20 161168 (2007).\n",
      "0\n",
      "0\n",
      "82\n",
      "\n",
      "82\n",
      "\n",
      "82\n",
      "Duda, R. O. & Hart, P. E. Pattern Classification and Scene Analysis (Wiley, 1973).\n",
      "0\n",
      "0\n",
      "65\n",
      "\n",
      "65\n",
      "\n",
      "65\n",
      "Schlkopf, B. & Smola, A. Learning with Kernels (MIT Press, 2002).\n",
      "0\n",
      "0\n",
      "182\n",
      "\n",
      "182\n",
      "\n",
      "182\n",
      "Bengio, Y., Delalleau, O. & Le Roux, N. The curse of highly variable functions for local kernel machines. In Proc. Advances in Neural Information Processing Systems 18 107114 (2005).\n",
      "0\n",
      "0\n",
      "164\n",
      "\n",
      "164\n",
      "\n",
      "164\n",
      "Selfridge, O. G. Pandemonium: a paradigm for learning in mechanisation of thought processes. In Proc. Symposium on Mechanisation of Thought Processes 513526 (1958).\n",
      "0\n",
      "0\n",
      "131\n",
      "\n",
      "131\n",
      "\n",
      "131\n",
      "Rosenblatt, F. The Perceptron  A Perceiving and Recognizing Automaton. Tech. Rep. 85-460-1 (Cornell Aeronautical Laboratory, 1957).\n",
      "0\n",
      "0\n",
      "129\n",
      "\n",
      "129\n",
      "\n",
      "129\n",
      "Werbos, P. Beyond Regression: New Tools for Prediction and Analysis in the Behavioral Sciences. PhD thesis, Harvard Univ. (1974).\n",
      "0\n",
      "0\n",
      "59\n",
      "\n",
      "59\n",
      "\n",
      "59\n",
      "Parker, D. B. Learning Logic Report TR47 (MIT Press, 1985).\n",
      "0\n",
      "0\n",
      "209\n",
      "\n",
      "209\n",
      "\n",
      "209\n",
      "LeCun, Y. Une procdure dapprentissage pour Rseau  seuil assymtrique in Cognitiva 85: a la Frontire de lIntelligence Artificielle, des Sciences de la Connaissance et des Neurosciences [in French] 599604 (1985).\n",
      "0\n",
      "0\n",
      "129\n",
      "\n",
      "129\n",
      "\n",
      "129\n",
      "Rumelhart, D. E., Hinton, G. E. & Williams, R. J. Learning representations by back-propagating errors. Nature 323, 533536 (1986).\n",
      "3\n",
      "\n",
      "3\n",
      "\n",
      "10\n",
      "ISI\n",
      "7\n",
      "\n",
      "7\n",
      "Article\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "300\n",
      "\n",
      "300\n",
      "\n",
      "300\n",
      "Glorot, X., Bordes, A. & Bengio. Y. Deep sparse rectifier neural networks. In Proc. 14th International Conference on Artificial Intelligence and Statistics 315323 (2011). This paper showed that supervised training of very deep neural networks is much faster if the hidden layers are composed of ReLU.\n",
      "0\n",
      "0\n",
      "193\n",
      "\n",
      "193\n",
      "\n",
      "193\n",
      "Dauphin, Y. et al. Identifying and attacking the saddle point problem in high-dimensional non-convex optimization. In Proc. Advances in Neural Information Processing Systems 27 29332941 (2014).\n",
      "0\n",
      "0\n",
      "187\n",
      "\n",
      "187\n",
      "\n",
      "187\n",
      "Choromanska, A., Henaff, M., Mathieu, M., Arous, G. B. & LeCun, Y. The loss surface of multilayer networks. In Proc. Conference on AI and Statistics http://arxiv.org/abs/1412.0233 (2014).\n",
      "0\n",
      "0\n",
      "145\n",
      "\n",
      "145\n",
      "\n",
      "145\n",
      "Hinton, G. E. What kind of graphical model is the brain? In Proc. 19th International Joint Conference on Artificial intelligence 17651775 (2005).\n",
      "0\n",
      "0\n",
      "328\n",
      "\n",
      "328\n",
      "\n",
      "328\n",
      "Hinton, G. E., Osindero, S. & Teh, Y.-W. A fast learning algorithm for deep belief nets. Neural Comp. 18, 15271554 (2006). This paper introduced a novel and effective way of training very deep neural networks by pre-training one hidden layer at a time using the unsupervised learning procedure for restricted Boltzmann machines.\n",
      "7\n",
      "\n",
      "7\n",
      "\n",
      "7\n",
      "Article\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "421\n",
      "\n",
      "421\n",
      "\n",
      "421\n",
      "Bengio, Y., Lamblin, P., Popovici, D. & Larochelle, H. Greedy layer-wise training of deep networks. In Proc. Advances in Neural Information Processing Systems 19 153160 (2006). This report demonstrated that the unsupervised pre-training method introduced in ref. 32 significantly improves performance on test data and generalizes the method to other unsupervised representation-learning techniques, such as auto-encoders.\n",
      "0\n",
      "0\n",
      "201\n",
      "\n",
      "201\n",
      "\n",
      "201\n",
      "Ranzato, M., Poultney, C., Chopra, S. & LeCun, Y. Efficient learning of sparse representations with an energy-based model. In Proc. Advances in Neural Information Processing Systems 19 11371144 (2006).\n",
      "0\n",
      "0\n",
      "119\n",
      "\n",
      "119\n",
      "\n",
      "119\n",
      "Hinton, G. E. & Salakhutdinov, R. Reducing the dimensionality of data with neural networks. Science 313, 504507 (2006).\n",
      "3\n",
      "\n",
      "3\n",
      "\n",
      "6\n",
      "CAS\n",
      "3\n",
      "\n",
      "9\n",
      "ISI\n",
      "6\n",
      "\n",
      "13\n",
      "PubMed\n",
      "7\n",
      "\n",
      "7\n",
      "Article\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "240\n",
      "\n",
      "240\n",
      "\n",
      "240\n",
      "Sermanet, P., Kavukcuoglu, K., Chintala, S. & LeCun, Y. Pedestrian detection with unsupervised multi-stage feature learning. In Proc. International Conference on Computer Vision and Pattern Recognition http://arxiv.org/abs/1212.0142 (2013).\n",
      "0\n",
      "0\n",
      "182\n",
      "\n",
      "182\n",
      "\n",
      "182\n",
      "Raina, R., Madhavan, A. & Ng, A. Y. Large-scale deep unsupervised learning using graphics processors. In Proc. 26th Annual International Conference on Machine Learning 873880 (2009).\n",
      "0\n",
      "0\n",
      "143\n",
      "\n",
      "143\n",
      "\n",
      "143\n",
      "Mohamed, A.-R., Dahl, G. E. & Hinton, G. Acoustic modeling using deep belief networks. IEEE Trans. Audio Speech Lang. Process. 20, 1422 (2012).\n",
      "3\n",
      "\n",
      "3\n",
      "\n",
      "10\n",
      "ISI\n",
      "7\n",
      "\n",
      "7\n",
      "Article\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "190\n",
      "\n",
      "190\n",
      "\n",
      "190\n",
      "Dahl, G. E., Yu, D., Deng, L. & Acero, A. Context-dependent pre-trained deep neural networks for large vocabulary speech recognition. IEEE Trans. Audio Speech Lang. Process. 20, 3342 (2012).\n",
      "7\n",
      "\n",
      "7\n",
      "\n",
      "7\n",
      "Article\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "158\n",
      "\n",
      "158\n",
      "\n",
      "158\n",
      "Bengio, Y., Courville, A. & Vincent, P. Representation learning: a review and new perspectives. IEEE Trans. Pattern Anal. Machine Intell. 35, 17981828 (2013).\n",
      "7\n",
      "\n",
      "7\n",
      "\n",
      "7\n",
      "Article\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "303\n",
      "\n",
      "303\n",
      "\n",
      "303\n",
      "LeCun, Y. et al. Handwritten digit recognition with a back-propagation network. In Proc. Advances in Neural Information Processing Systems 396404 (1990). This is the first paper on convolutional networks trained by backpropagation for the task of classifying low-resolution images of handwritten digits.\n",
      "0\n",
      "0\n",
      "522\n",
      "\n",
      "522\n",
      "\n",
      "522\n",
      "LeCun, Y., Bottou, L., Bengio, Y. & Haffner, P. Gradient-based learning applied to document recognition. Proc. IEEE 86, 22782324 (1998). This overview paper on the principles of end-to-end training of modular systems such as deep neural networks using gradient-based optimization showed how neural networks (and in particular convolutional nets) can be combined with search or inference mechanisms to model complex outputs that are interdependent, such as sequences of characters associated with the content of a document.\n",
      "3\n",
      "\n",
      "3\n",
      "\n",
      "10\n",
      "ISI\n",
      "7\n",
      "\n",
      "7\n",
      "Article\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "156\n",
      "\n",
      "156\n",
      "\n",
      "156\n",
      "Hubel, D. H. & Wiesel, T. N. Receptive fields, binocular interaction, and functional architecture in the cats visual cortex. J. Physiol. 160, 106154 (1962).\n",
      "3\n",
      "\n",
      "3\n",
      "\n",
      "6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CAS\n",
      "3\n",
      "\n",
      "9\n",
      "ISI\n",
      "6\n",
      "\n",
      "13\n",
      "PubMed\n",
      "7\n",
      "\n",
      "7\n",
      "Article\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "130\n",
      "\n",
      "130\n",
      "\n",
      "130\n",
      "Felleman, D. J. & Essen, D. C. V. Distributed hierarchical processing in the primate cerebral cortex. Cereb. Cortex 1, 147 (1991).\n",
      "3\n",
      "\n",
      "3\n",
      "\n",
      "6\n",
      "CAS\n",
      "3\n",
      "\n",
      "9\n",
      "ISI\n",
      "6\n",
      "\n",
      "13\n",
      "PubMed\n",
      "7\n",
      "\n",
      "7\n",
      "Article\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "161\n",
      "\n",
      "161\n",
      "\n",
      "161\n",
      "Cadieu, C. F. et al. Deep neural networks rival the representation of primate it cortex for core visual object recognition. PLoS Comp. Biol. 10, e1003963 (2014).\n",
      "7\n",
      "\n",
      "7\n",
      "\n",
      "7\n",
      "Article\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "168\n",
      "\n",
      "168\n",
      "\n",
      "168\n",
      "Fukushima, K. & Miyake, S. Neocognitron: a new algorithm for pattern recognition tolerant of deformations and shifts in position. Pattern Recognition 15, 455469 (1982).\n",
      "3\n",
      "\n",
      "3\n",
      "\n",
      "10\n",
      "ISI\n",
      "7\n",
      "\n",
      "7\n",
      "Article\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "181\n",
      "\n",
      "181\n",
      "\n",
      "181\n",
      "Waibel, A., Hanazawa, T., Hinton, G. E., Shikano, K. & Lang, K. Phoneme recognition using time-delay neural networks. IEEE Trans. Acoustics Speech Signal Process. 37, 328339 (1989).\n",
      "3\n",
      "\n",
      "3\n",
      "\n",
      "10\n",
      "ISI\n",
      "7\n",
      "\n",
      "7\n",
      "Article\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "210\n",
      "\n",
      "210\n",
      "\n",
      "210\n",
      "Bottou, L., Fogelman-Souli, F., Blanchet, P. & Lienard, J. Experiments with time delay networks and dynamic time warping for speaker independent isolated digit recognition. In Proc. EuroSpeech 89 537540 (1989).\n",
      "0\n",
      "0\n",
      "152\n",
      "\n",
      "152\n",
      "\n",
      "152\n",
      "Simard, D., Steinkraus, P. Y. & Platt, J. C. Best practices for convolutional neural networks. In Proc. Document Analysis and Recognition 958963 (2003).\n",
      "0\n",
      "0\n",
      "164\n",
      "\n",
      "164\n",
      "\n",
      "164\n",
      "Vaillant, R., Monrocq, C. & LeCun, Y. Original approach for the localisation of objects in images. In Proc. Vision, Image, and Signal Processing 141, 245250 (1994).\n",
      "0\n",
      "0\n",
      "78\n",
      "\n",
      "78\n",
      "\n",
      "78\n",
      "Nowlan, S. & Platt, J. in Neural Information Processing Systems 901908 (1995).\n",
      "0\n",
      "0\n",
      "157\n",
      "\n",
      "157\n",
      "\n",
      "157\n",
      "Lawrence, S., Giles, C. L., Tsoi, A. C. & Back, A. D. Face recognition: a convolutional neural-network approach. IEEE Trans. Neural Networks 8, 98113 (1997).\n",
      "3\n",
      "\n",
      "3\n",
      "\n",
      "6\n",
      "CAS\n",
      "3\n",
      "\n",
      "10\n",
      "ISI\n",
      "7\n",
      "\n",
      "7\n",
      "Article\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "151\n",
      "\n",
      "151\n",
      "\n",
      "151\n",
      "Ciresan, D., Meier, U. Masci, J. & Schmidhuber, J. Multi-column deep neural network for traffic sign classification. Neural Networks 32, 333338 (2012).\n",
      "3\n",
      "\n",
      "3\n",
      "\n",
      "9\n",
      "ISI\n",
      "6\n",
      "\n",
      "13\n",
      "PubMed\n",
      "7\n",
      "\n",
      "7\n",
      "Article\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "127\n",
      "\n",
      "127\n",
      "\n",
      "127\n",
      "Ning, F. et al. Toward automatic phenotyping of developing embryos from videos. IEEE Trans. Image Process. 14, 13601371 (2005).\n",
      "3\n",
      "\n",
      "3\n",
      "\n",
      "9\n",
      "ISI\n",
      "6\n",
      "\n",
      "13\n",
      "PubMed\n",
      "7\n",
      "\n",
      "7\n",
      "Article\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "139\n",
      "\n",
      "139\n",
      "\n",
      "139\n",
      "Turaga, S. C. et al. Convolutional networks can learn to generate affinity graphs for image segmentation. Neural Comput. 22, 511538 (2010).\n",
      "3\n",
      "\n",
      "3\n",
      "\n",
      "9\n",
      "ISI\n",
      "6\n",
      "\n",
      "13\n",
      "PubMed\n",
      "7\n",
      "\n",
      "7\n",
      "Article\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "172\n",
      "\n",
      "172\n",
      "\n",
      "172\n",
      "Garcia, C. & Delakis, M. Convolutional face finder: a neural architecture for fast and robust face detection. IEEE Trans. Pattern Anal. Machine Intell. 26, 14081423 (2004).\n",
      "7\n",
      "\n",
      "7\n",
      "\n",
      "7\n",
      "Article\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "149\n",
      "\n",
      "149\n",
      "\n",
      "149\n",
      "Osadchy, M., LeCun, Y. & Miller, M. Synergistic face detection and pose estimation with energy-based models. J. Mach. Learn. Res. 8, 11971215 (2007).\n",
      "3\n",
      "\n",
      "3\n",
      "\n",
      "3\n",
      "ISI\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "231\n",
      "\n",
      "231\n",
      "\n",
      "231\n",
      "Tompson, J., Goroshin, R. R., Jain, A., LeCun, Y. Y. & Bregler, C. C. Efficient object localization using convolutional networks. In Proc. Conference on Computer Vision and Pattern Recognition http://arxiv.org/abs/1411.4280 (2014).\n",
      "0\n",
      "0\n",
      "200\n",
      "\n",
      "200\n",
      "\n",
      "200\n",
      "Taigman, Y., Yang, M., Ranzato, M. & Wolf, L. Deepface: closing the gap to human-level performance in face verification. In Proc. Conference on Computer Vision and Pattern Recognition 17011708 (2014).\n",
      "0\n",
      "0\n",
      "113\n",
      "\n",
      "113\n",
      "\n",
      "113\n",
      "Hadsell, R. et al. Learning long-range vision for autonomous off-road driving. J. Field Robot. 26, 120144 (2009).\n",
      "3\n",
      "\n",
      "3\n",
      "\n",
      "10\n",
      "ISI\n",
      "7\n",
      "\n",
      "7\n",
      "Article\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "223\n",
      "\n",
      "223\n",
      "\n",
      "223\n",
      "Farabet, C., Couprie, C., Najman, L. & LeCun, Y. Scene parsing with multiscale feature learning, purity trees, and optimal covers. In Proc. International Conference on Machine Learning http://arxiv.org/abs/1202.2160 (2012).\n",
      "0\n",
      "0\n",
      "190\n",
      "\n",
      "190\n",
      "\n",
      "190\n",
      "Srivastava, N., Hinton, G., Krizhevsky, A., Sutskever, I. & Salakhutdinov, R. Dropout: a simple way to prevent neural networks from overfitting. J. Machine Learning Res. 15, 19291958 (2014).\n",
      "3\n",
      "\n",
      "3\n",
      "\n",
      "3\n",
      "ISI\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "211\n",
      "\n",
      "211\n",
      "\n",
      "211\n",
      "Sermanet, P. et al. Overfeat: integrated recognition, localization and detection using convolutional networks. In Proc. International Conference on Learning Representations http://arxiv.org/abs/1312.6229 (2014).\n",
      "0\n",
      "0\n",
      "210\n",
      "\n",
      "210\n",
      "\n",
      "210\n",
      "Girshick, R., Donahue, J., Darrell, T. & Malik, J. Rich feature hierarchies for accurate object detection and semantic segmentation. In Proc. Conference on Computer Vision and Pattern Recognition 580587 (2014).\n",
      "0\n",
      "0\n",
      "197\n",
      "\n",
      "197\n",
      "\n",
      "197\n",
      "Simonyan, K. & Zisserman, A. Very deep convolutional networks for large-scale image recognition. In Proc. International Conference on Learning Representations http://arxiv.org/abs/1409.1556 (2014).\n",
      "0\n",
      "0\n",
      "169\n",
      "\n",
      "169\n",
      "\n",
      "169\n",
      "Boser, B., Sackinger, E., Bromley, J., LeCun, Y. & Jackel, L. An analog neural network processor with programmable topology. J. Solid State Circuits 26, 20172025 (1991).\n",
      "3\n",
      "\n",
      "3\n",
      "\n",
      "10\n",
      "ISI\n",
      "7\n",
      "\n",
      "7\n",
      "Article\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "219\n",
      "\n",
      "219\n",
      "\n",
      "219\n",
      "Farabet, C. et al. Large-scale FPGA-based convolutional networks. In Scaling up Machine Learning: Parallel and Distributed Approaches (eds Bekkerman, R., Bilenko, M. & Langford, J.) 399419 (Cambridge Univ. Press, 2011).\n",
      "0\n",
      "0\n",
      "58\n",
      "\n",
      "58\n",
      "\n",
      "58\n",
      "Bengio, Y. Learning Deep Architectures for AI (Now, 2009).\n",
      "0\n",
      "0\n",
      "125\n",
      "\n",
      "125\n",
      "\n",
      "125\n",
      "Montufar, G. & Morton, J. When does a mixture of products contain a product of mixtures? J. Discrete Math. 29, 321347 (2014).\n",
      "7\n",
      "\n",
      "7\n",
      "\n",
      "7\n",
      "Article\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "186\n",
      "\n",
      "186\n",
      "\n",
      "186\n",
      "Montufar, G. F., Pascanu, R., Cho, K. & Bengio, Y. On the number of linear regions of deep neural networks. In Proc. Advances in Neural Information Processing Systems 27 29242932 (2014).\n",
      "0\n",
      "0\n",
      "361\n",
      "\n",
      "361\n",
      "\n",
      "361\n",
      "Bengio, Y., Ducharme, R. & Vincent, P. A neural probabilistic language model. In Proc. Advances in Neural Information Processing Systems 13 932938 (2001). This paper introduced neural language models, which learn to convert a word symbol into a word vector or word embedding composed of learned semantic features in order to predict the next word in a sequence.\n",
      "0\n",
      "0\n",
      "198\n",
      "\n",
      "198\n",
      "\n",
      "198\n",
      "Cho, K. et al. Learning phrase representations using RNN encoder-decoder for statistical machine translation. In Proc. Conference on Empirical Methods in Natural Language Processing 17241734 (2014).\n",
      "0\n",
      "0\n",
      "86\n",
      "\n",
      "86\n",
      "\n",
      "86\n",
      "Schwenk, H. Continuous space language models. Computer Speech Lang. 21, 492518 (2007).\n",
      "3\n",
      "\n",
      "3\n",
      "\n",
      "10\n",
      "ISI\n",
      "7\n",
      "\n",
      "7\n",
      "Article\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "194\n",
      "\n",
      "194\n",
      "\n",
      "194\n",
      "Socher, R., Lin, C. C-Y., Manning, C. & Ng, A. Y. Parsing natural scenes and natural language with recursive neural networks. In Proc. International Conference on Machine Learning 129136 (2011).\n",
      "0\n",
      "0\n",
      "216\n",
      "\n",
      "216\n",
      "\n",
      "216\n",
      "Mikolov, T., Sutskever, I., Chen, K., Corrado, G. & Dean, J. Distributed representations of words and phrases and their compositionality. In Proc. Advances in Neural Information Processing Systems 26 31113119 (2013).\n",
      "0\n",
      "0\n",
      "206\n",
      "\n",
      "206\n",
      "\n",
      "206\n",
      "Bahdanau, D., Cho, K. & Bengio, Y. Neural machine translation by jointly learning to align and translate. In Proc. International Conference on Learning Representations http://arxiv.org/abs/1409.0473 (2015).\n",
      "0\n",
      "0\n",
      "109\n",
      "\n",
      "109\n",
      "\n",
      "109\n",
      "Hochreiter, S. Untersuchungen zu dynamischen neuronalen Netzen [in German] Diploma thesis, T.U. Mnich (1991).\n",
      "0\n",
      "0\n",
      "151\n",
      "\n",
      "151\n",
      "\n",
      "151\n",
      "Bengio, Y., Simard, P. & Frasconi, P. Learning long-term dependencies with gradient descent is difficult. IEEE Trans. Neural Networks 5, 157166 (1994).\n",
      "3\n",
      "\n",
      "3\n",
      "\n",
      "6\n",
      "CAS\n",
      "3\n",
      "\n",
      "10\n",
      "ISI\n",
      "7\n",
      "\n",
      "7\n",
      "Article\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "279\n",
      "\n",
      "279\n",
      "\n",
      "279\n",
      "Hochreiter, S. & Schmidhuber, J. Long short-term memory. Neural Comput. 9, 17351780 (1997). This paper introduced LSTM recurrent networks, which have become a crucial ingredient in recent advances with recurrent networks because they are good at learning long-range dependencies.\n",
      "3\n",
      "\n",
      "3\n",
      "\n",
      "6\n",
      "CAS\n",
      "3\n",
      "\n",
      "9\n",
      "ISI\n",
      "6\n",
      "\n",
      "13\n",
      "PubMed\n",
      "7\n",
      "\n",
      "7\n",
      "Article\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "258\n",
      "\n",
      "258\n",
      "\n",
      "258\n",
      "ElHihi, S. & Bengio, Y. Hierarchical recurrent neural networks for long-term dependencies. In Proc. Advances in Neural Information Processing Systems 8 http://papers.nips.cc/paper/1102-hierarchical-recurrent-neural-networks-for-long-term-dependencies (1995).\n",
      "0\n",
      "0\n",
      "83\n",
      "\n",
      "83\n",
      "\n",
      "83\n",
      "Sutskever, I. Training Recurrent Neural Networks. PhD thesis, Univ. Toronto (2012).\n",
      "0\n",
      "0\n",
      "170\n",
      "\n",
      "170\n",
      "\n",
      "170\n",
      "Pascanu, R., Mikolov, T. & Bengio, Y. On the difficulty of training recurrent neural networks. In Proc. 30th International Conference on Machine Learning 13101318 (2013).\n",
      "0\n",
      "0\n",
      "166\n",
      "\n",
      "166\n",
      "\n",
      "166\n",
      "Sutskever, I., Martens, J. & Hinton, G. E. Generating text with recurrent neural networks. In Proc. 28th International Conference on Machine Learning 10171024 (2011).\n",
      "0\n",
      "0\n",
      "74\n",
      "\n",
      "74\n",
      "\n",
      "74\n",
      "Lakoff, G. & Johnson, M. Metaphors We Live By (Univ. Chicago Press, 2008).\n",
      "0\n",
      "0\n",
      "115\n",
      "\n",
      "115\n",
      "\n",
      "115\n",
      "Rogers, T. T. & McClelland, J. L. Semantic Cognition: A Parallel Distributed Processing Approach (MIT Press, 2004).\n",
      "0\n",
      "0\n",
      "193\n",
      "\n",
      "193\n",
      "\n",
      "193\n",
      "Xu, K. et al. Show, attend and tell: Neural image caption generation with visual attention. In Proc. International Conference on Learning Representations http://arxiv.org/abs/1502.03044 (2015).\n",
      "0\n",
      "0\n",
      "189\n",
      "\n",
      "189\n",
      "\n",
      "189\n",
      "Graves, A., Mohamed, A.-R. & Hinton, G. Speech recognition with deep recurrent neural networks. In Proc. International Conference on Acoustics, Speech and Signal Processing 66456649 (2013).\n",
      "0\n",
      "0\n",
      "100\n",
      "\n",
      "100\n",
      "\n",
      "100\n",
      "Graves, A., Wayne, G. & Danihelka, I. Neural Turing machines. http://arxiv.org/abs/1410.5401 (2014).\n",
      "0\n",
      "0\n",
      "90\n",
      "\n",
      "90\n",
      "\n",
      "90\n",
      "Weston, J. Chopra, S. & Bordes, A. Memory networks. http://arxiv.org/abs/1410.3916 (2014).\n",
      "0\n",
      "0\n",
      "161\n",
      "\n",
      "161\n",
      "\n",
      "161\n",
      "Weston, J., Bordes, A., Chopra, S. & Mikolov, T. Towards AI-complete question answering: a set of prerequisite toy tasks. http://arxiv.org/abs/1502.05698 (2015).\n",
      "0\n",
      "0\n",
      "140\n",
      "\n",
      "140\n",
      "\n",
      "140\n",
      "Hinton, G. E., Dayan, P., Frey, B. J. & Neal, R. M. The wake-sleep algorithm for unsupervised neural networks. Science 268, 15581161 (1995).\n",
      "6\n",
      "\n",
      "6\n",
      "\n",
      "13\n",
      "PubMed\n",
      "7\n",
      "\n",
      "7\n",
      "Article\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "146\n",
      "\n",
      "146\n",
      "\n",
      "146\n",
      "Salakhutdinov, R. & Hinton, G. Deep Boltzmann machines. In Proc. International Conference on Artificial Intelligence and Statistics 448455 (2009).\n",
      "0\n",
      "0\n",
      "203\n",
      "\n",
      "203\n",
      "\n",
      "203\n",
      "Vincent, P., Larochelle, H., Bengio, Y. & Manzagol, P.-A. Extracting and composing robust features with denoising autoencoders. In Proc. 25th International Conference on Machine Learning 10961103 (2008).\n",
      "0\n",
      "0\n",
      "168\n",
      "\n",
      "168\n",
      "\n",
      "168\n",
      "Kavukcuoglu, K. et al. Learning convolutional feature hierarchies for visual recognition. In Proc. Advances in Neural Information Processing Systems 23 10901098 (2010).\n",
      "0\n",
      "0\n",
      "138\n",
      "\n",
      "138\n",
      "\n",
      "138\n",
      "Gregor, K. & LeCun, Y. Learning fast approximations of sparse coding. In Proc. International Conference on Machine Learning 399406 (2010).\n",
      "0\n",
      "0\n",
      "159\n",
      "\n",
      "159\n",
      "\n",
      "159\n",
      "Ranzato, M., Mnih, V., Susskind, J. M. & Hinton, G. E. Modeling natural images using gated MRFs. IEEE Trans. Pattern Anal. Machine Intell. 35, 22062222 (2013).\n",
      "7\n",
      "\n",
      "7\n",
      "\n",
      "7\n",
      "Article\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "191\n",
      "\n",
      "191\n",
      "\n",
      "191\n",
      "Bengio, Y., Thibodeau-Laufer, E., Alain, G. & Yosinski, J. Deep generative stochastic networks trainable by backprop. In Proc. 31st International Conference on Machine Learning 226234 (2014).\n",
      "0\n",
      "0\n",
      "183\n",
      "\n",
      "183\n",
      "\n",
      "183\n",
      "Kingma, D., Rezende, D., Mohamed, S. & Welling, M. Semi-supervised learning with deep generative models. In Proc. Advances in Neural Information Processing Systems 27 35813589 (2014).\n",
      "0\n",
      "0\n",
      "186\n",
      "\n",
      "186\n",
      "\n",
      "186\n",
      "Ba, J., Mnih, V. & Kavukcuoglu, K. Multiple object recognition with visual attention. In Proc. International Conference on Learning Representations http://arxiv.org/abs/1412.7755 (2014).\n",
      "0\n",
      "0\n",
      "99\n",
      "\n",
      "99\n",
      "\n",
      "99\n",
      "Mnih, V. et al. Human-level control through deep reinforcement learning. Nature 518, 529533 (2015).\n",
      "3\n",
      "\n",
      "3\n",
      "\n",
      "6\n",
      "CAS\n",
      "3\n",
      "\n",
      "9\n",
      "ISI\n",
      "6\n",
      "\n",
      "13\n",
      "PubMed\n",
      "7\n",
      "\n",
      "7\n",
      "Article\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "86\n",
      "\n",
      "86\n",
      "\n",
      "86\n",
      "Bottou, L. From machine learning to machine reasoning. Mach. Learn. 94, 133149 (2014).\n",
      "3\n",
      "\n",
      "3\n",
      "\n",
      "10\n",
      "ISI\n",
      "7\n",
      "\n",
      "7\n",
      "Article\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "190\n",
      "\n",
      "190\n",
      "\n",
      "190\n",
      "Vinyals, O., Toshev, A., Bengio, S. & Erhan, D. Show and tell: a neural image caption generator. In Proc. International Conference on Machine Learning http://arxiv.org/abs/1502.03044 (2014).\n",
      "0\n",
      "0\n",
      "108\n",
      "\n",
      "108\n",
      "\n",
      "108\n",
      "van der Maaten, L. & Hinton, G. E. Visualizing data using t-SNE. J. Mach. Learn.Research 9, 25792605 (2008).\n",
      "0\n",
      "0\n",
      "19\n",
      "\n",
      "19\n",
      "\n",
      "19\n",
      "Download references\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "18\n",
      "\n",
      "18\n",
      "\n",
      "18\n",
      "Author information\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "38\n",
      "\n",
      "38\n",
      "\n",
      "38\n",
      "Abstract References Author information\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "12\n",
      "\n",
      "12\n",
      "\n",
      "12\n",
      "Affiliations\n",
      "65\n",
      "\n",
      "65\n",
      "\n",
      "65\n",
      "Facebook AI Research, 770 Broadway, New York, New York 10003 USA.\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "10\n",
      "\n",
      "10\n",
      "\n",
      "10\n",
      "Yann LeCun\n",
      "0\n",
      "65\n",
      "\n",
      "65\n",
      "\n",
      "65\n",
      "New York University, 715 Broadway, New York, New York 10003, USA.\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "10\n",
      "\n",
      "10\n",
      "\n",
      "10\n",
      "Yann LeCun\n",
      "0\n",
      "164\n",
      "\n",
      "164\n",
      "\n",
      "164\n",
      "Department of Computer Science and Operations Research Universit de Montral, Pavillon Andr-Aisenstadt, PO Box 6128 Centre-Ville STN Montral, Quebec H3C 3J7, Canada.\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "13\n",
      "\n",
      "13\n",
      "\n",
      "13\n",
      "Yoshua Bengio\n",
      "0\n",
      "72\n",
      "\n",
      "72\n",
      "\n",
      "72\n",
      "Google, 1600 Amphitheatre Parkway, Mountain View, California 94043, USA.\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "15\n",
      "\n",
      "15\n",
      "\n",
      "15\n",
      "Geoffrey Hinton\n",
      "0\n",
      "110\n",
      "\n",
      "110\n",
      "\n",
      "110\n",
      "Department of Computer Science, University of Toronto, 6 Kings College Road, Toronto, Ontario M5S 3G4, Canada.\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "15\n",
      "\n",
      "15\n",
      "\n",
      "15\n",
      "Geoffrey Hinton\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "29\n",
      "\n",
      "29\n",
      "\n",
      "29\n",
      "Competing financial interests\n",
      "53\n",
      "\n",
      "53\n",
      "\n",
      "53\n",
      "The authors declare no competing financial interests.\n",
      "0\n",
      "20\n",
      "\n",
      "39\n",
      "\n",
      "39\n",
      "Corresponding author\n",
      "29\n",
      "Correspondence to: \n",
      "10\n",
      "\n",
      "10\n",
      "Yann LeCun\n",
      "0\n",
      "77\n",
      "\n",
      "77\n",
      "\n",
      "77\n",
      "Reprints and permissions information is available at www.nature.com/reprints.\n",
      "0\n",
      "14\n",
      "\n",
      "286\n",
      "\n",
      "286\n",
      "Author details\n",
      "272\n",
      "Yann LeCunContact Yann LeCunSearch for this author in:Nature Research journals PubMed Google ScholarYoshua BengioSearch for this author in:Nature Research journals PubMed Google ScholarGeoffrey HintonSearch for this author in:Nature Research journals PubMed Google Scholar\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "15\n",
      "\n",
      "15\n",
      "\n",
      "15\n",
      "Additional data\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "13\n",
      "\n",
      "13\n",
      "\n",
      "13\n",
      "Editors' pick\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "42\n",
      "\n",
      "42\n",
      "\n",
      "42\n",
      "Image credit: NASA/JPL-Caltech/SSI/Cornell\n",
      "22\n",
      "\n",
      "22\n",
      "\n",
      "22\n",
      "13 years around Saturn\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "12\n",
      "\n",
      "12\n",
      "\n",
      "12\n",
      "Science jobs\n",
      "0\n",
      "14\n",
      "\n",
      "14\n",
      "\n",
      "14\n",
      "NatureJobs.com\n",
      "0\n",
      "0\n",
      "110\n",
      "\n",
      "110\n",
      "\n",
      "140\n",
      "Multiple Postdoctoral Fellowships in Cardiac Signal Processing and Instrumentation : Boston, MA, United States\n",
      "30\n",
      "\n",
      "30\n",
      "Massachusetts General Hospital\n",
      "38\n",
      "\n",
      "38\n",
      "\n",
      "60\n",
      "Assistant, Associate or Full Professor\n",
      "22\n",
      "\n",
      "22\n",
      "University of Michigan\n",
      "74\n",
      "\n",
      "74\n",
      "\n",
      "89\n",
      "Associate Editor / Senior Editor roles, Nature Research - Talent Pool 2017\n",
      "15\n",
      "\n",
      "15\n",
      "Springer Nature\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "10\n",
      "\n",
      "27\n",
      "\n",
      "27\n",
      "Post a job\n",
      "17\n",
      "More science jobs\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "14\n",
      "\n",
      "14\n",
      "\n",
      "14\n",
      "Science events\n",
      "0\n",
      "22\n",
      "\n",
      "22\n",
      "\n",
      "22\n",
      "NatureEvents Directory\n",
      "53\n",
      "\n",
      "69\n",
      "\n",
      "87\n",
      "Asia Pacific Vascular Biology Organization Conference\n",
      "34\n",
      "17 November 2017\n",
      "73\n",
      "  20 November 2017\n",
      "55\n",
      "\n",
      "55\n",
      "Pearl River New City, Tianhe District, Guangzhou, China\n",
      "98\n",
      "\n",
      "113\n",
      "\n",
      "130\n",
      "22nd World Congress on Advances in Oncology and 20th International Symposium on Molecular Medicine\n",
      "32\n",
      "05 October 2017\n",
      "31\n",
      "  07 October 2017\n",
      "14\n",
      "\n",
      "14\n",
      "Athens, Greece\n",
      "74\n",
      "\n",
      "89\n",
      "\n",
      "89\n",
      "The 2017 GCHERA World Agriculture Prize Awarding Ceremony & World Dialogue\n",
      "43\n",
      "28 October 2017\n",
      "28\n",
      "\n",
      "28\n",
      "No.1 Weigang, Nanjing, China\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "17\n",
      "\n",
      "36\n",
      "\n",
      "36\n",
      "Post a free event\n",
      "19\n",
      "More science events\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "13\n",
      "\n",
      "13\n",
      "\n",
      "13\n",
      "Discover more\n",
      "0\n",
      "0\n",
      "0\n",
      "86\n",
      "\n",
      "105\n",
      "\n",
      "116\n",
      "Relevance of deep learning to facilitate the diagnosis of HER2 status in breast cancer\n",
      "30\n",
      "Scientific Reports \n",
      "11\n",
      "05 Apr 2017\n",
      "0\n",
      "0\n",
      "0\n",
      "89\n",
      "\n",
      "108\n",
      "\n",
      "119\n",
      "Towards automatic pulmonary nodule management in lung cancer screening with deep learning\n",
      "30\n",
      "Scientific Reports \n",
      "11\n",
      "19 Apr 2017\n",
      "0\n",
      "0\n",
      "0\n",
      "135\n",
      "\n",
      "154\n",
      "\n",
      "165\n",
      "Accurate and reproducible invasive breast cancer detection in whole-slide images: A Deep Learning approach for quantifying tumor extent\n",
      "30\n",
      "Scientific Reports \n",
      "11\n",
      "18 Apr 2017\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "9\n",
      "\n",
      "9\n",
      "\n",
      "9\n",
      "Most read\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "57\n",
      "\n",
      "78\n",
      "\n",
      "78\n",
      "Correction of a pathogenic gene mutation in human embryos\n",
      "21\n",
      "Nature 02 August 2017\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "47\n",
      "\n",
      "68\n",
      "\n",
      "68\n",
      "CDK4/6 inhibition triggers anti-tumour immunity\n",
      "21\n",
      "Nature 16 August 2017\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "55\n",
      "\n",
      "76\n",
      "\n",
      "76\n",
      "Artificial intelligence: The future is superintelligent\n",
      "21\n",
      "Nature 30 August 2017\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "6\n",
      "\n",
      "37\n",
      "\n",
      "37\n",
      "Nature\n",
      "31\n",
      "ISSN: 0028-0836EISSN: 1476-4687\n",
      "0\n",
      "0\n",
      "0\n",
      "8\n",
      "\n",
      "8\n",
      "\n",
      "18\n",
      "About us\n",
      "10\n",
      "\n",
      "33\n",
      "Contact us\n",
      "23\n",
      "\n",
      "27\n",
      "Accessibility statement\n",
      "4\n",
      "\n",
      "4\n",
      "Help\n",
      "0\n",
      "0\n",
      "14\n",
      "\n",
      "14\n",
      "\n",
      "28\n",
      "Privacy policy\n",
      "14\n",
      "\n",
      "26\n",
      "Use of cookies\n",
      "12\n",
      "\n",
      "17\n",
      "Legal notice\n",
      "5\n",
      "\n",
      "5\n",
      "Terms\n",
      "0\n",
      "0\n",
      "11\n",
      "\n",
      "11\n",
      "\n",
      "22\n",
      "Nature jobs\n",
      "11\n",
      "\n",
      "27\n",
      "Nature Asia\n",
      "16\n",
      "\n",
      "29\n",
      "Nature Education\n",
      "13\n",
      "\n",
      "13\n",
      "RSS web feeds\n",
      "0\n",
      "0\n",
      "12\n",
      "\n",
      "12\n",
      "\n",
      "26\n",
      "About Nature\n",
      "14\n",
      "\n",
      "31\n",
      "Contact Nature\n",
      "17\n",
      "\n",
      "30\n",
      "About the Editors\n",
      "13\n",
      "\n",
      "13\n",
      "Nature awards\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "3\n",
      "\n",
      "3\n",
      "\n",
      "3\n",
      " Go\n",
      "0\n",
      "81\n",
      "\n",
      "153\n",
      "\n",
      "153\n",
      " 2015 Macmillan Publishers Limited, part of Springer Nature. All rights reserved.\n",
      "72\n",
      "partner of AGORA, HINARI, OARE, INASP, ORCID, CrossRef, COUNTER and COPE\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "language: True  cnratio: 0.0\n",
      "num of lines:44\n",
      "Deep learning allows computational models that are composed of multiple processing layers to learn representations of data with multiple levels of abstraction. These methods have dramatically improved the state-of-the-art in speech recognition, visual object recognition, object detection and many other domains such as drug discovery and genomics. Deep learning discovers intricate structure in large data sets by using the backpropagation algorithm to indicate how a machine should change its internal parameters that are used to compute the representation in each layer from the representation in the previous layer. Deep convolutional nets have brought about breakthroughs in processing images, video, speech and audio, whereas recurrent nets have shone light on sequential data such as text and speech.\n",
      "a, A multi-layer neural network (shown by the connected dots) can distort the input space to make the classes of data (examples of which are on the red and blue lines) linearly separable. Note how a regular grid (shown on the left) in input space is also transformed (shown in the middle panel) by hidden units. This is an illustrative example with only two input units, two hidden units and one output unit, but the networks used for object recognition or natural language processing contain tens or hundreds of thousands of units. Reproduced with permission from C. Olah (http://colah.github.io/). b, The chain rule of derivatives tells us how two small effects (that of a small change of x on y, and that of y on z) are composed. A small change x in x gets transformed first into a small change y in y by getting multiplied by y/x (that is, the definition of partial derivative). Similarly, the change y creates a change z in z. Substituting one equation into the other gives the chain rule of derivatives  how x gets turned into z through multiplication by the product of y/x and z/x. It also works when x, y and z are vectors (and the derivatives are Jacobian matrices). c, The equations used for computing the forward pass in a neural net with two hidden layers and one output layer, each constituting a module through which one can backpropagate gradients. At each layer, we first compute the total input z to each unit, which is a weighted sum of the outputs of the units in the layer below. Then a non-linear function f(.) is applied to z to get the output of the unit. For simplicity, we have omitted bias terms. The non-linear functions used in neural networks include the rectified linear unit (ReLU) f(z) = max(0,z), commonly used in recent years, as well as the more conventional sigmoids, such as the hyberbolic tangent, f(z) = (exp(z)  exp(z))/(exp(z) + exp(z)) and logistic function logistic, f(z) = 1/(1 + exp(z)). d, The equations used for computing the backward pass. At each hidden layer we compute the error derivative with respect to the output of each unit, which is a weighted sum of the error derivatives with respect to the total inputs to the units in the layer above. We then convert the error derivative with respect to the output into the error derivative with respect to the input by multiplying it by the gradient of f(z). At the output layer, the error derivative with respect to the output of a unit is computed by differentiating the cost function. This gives yl  tl if the cost function for unit l is 0.5(yl  tl)2, where tl is the target value. Once the E/zk is known, the error-derivative for the weight wjk on the connection from unit j in the layer below is just yj E/zk.\n",
      "The outputs (not the filters) of each layer (horizontally) of a typical convolutional network architecture applied to the image of a Samoyed dog (bottom left; and RGB (red, green, blue) inputs, bottom right). Each rectangular image is a feature map corresponding to the output for one of the learned features, detected at each of the image positions. Information flows bottom up, with lower-level features acting as oriented edge detectors, and a score is computed for each image class in output. ReLU, rectified linear unit.\n",
      "Captions generated by a recurrent neural network (RNN) taking, as extra input, the representation extracted by a deep convolution neural network (CNN) from a test image, with the RNN trained to translate high-level representations of images into captions (top). Reproduced with permission from ref. 102. When the RNN is given the ability to focus its attention on a different location in the input image (middle and bottom; the lighter patches were given more attention) as it generates each word (bold), we found86 that it exploits this to achieve better translation of images into captions.\n",
      "On the left is an illustration of word representations learned for modelling language, non-linearly projected to 2D for visualization using the t-SNE algorithm103. On the right is a 2D representation of phrases learned by an English-to-French encoderdecoder recurrent neural network75. One can observe that semantically similar words or sequences of words are mapped to nearby representations. The distributed representations of words are obtained by using backpropagation to jointly learn a representation for each word and a function that predicts a target quantity such as the next word in a sequence (for language modelling) or a whole sequence of translated words (for machine translation)18, 75.\n",
      "The artificial neurons (for example, hidden units grouped under node s with values st at time t) get inputs from other neurons at previous time steps (this is represented with the black square, representing a delay of one time step, on the left). In this way, a recurrent neural network can map an input sequence with elements xt into an output sequence with elements ot, with each ot depending on all the previous xt (for t  t). The same parameters (matrices U,V,W) are used at each time step. Many other architectures are possible, including a variant in which the network can generate a sequence of outputs (for example, words), each of which is used as inputs for the next time step. The backpropagation algorithm (Fig. 1) can be directly applied to the computational graph of the unfolded network on the right, to compute the derivative of a total error (for example, the log-probability of generating the right sequence of outputs) with respect to all the states st and all the parameters.\n",
      "Krizhevsky, A., Sutskever, I. & Hinton, G. ImageNet classification with deep convolutional neural networks. In Proc. Advances in Neural Information Processing Systems 25 10901098 (2012). This report was a breakthrough that used convolutional nets to almost halve the error rate for object recognition, and precipitated the rapid adoption of deep learning by the computer vision community.\n",
      "Tompson, J., Jain, A., LeCun, Y. & Bregler, C. Joint training of a convolutional network and a graphical model for human pose estimation. In Proc. Advances in Neural Information Processing Systems 27 17991807 (2014).\n",
      "Mikolov, T., Deoras, A., Povey, D., Burget, L. & Cernocky, J. Strategies for training large scale neural network language models. In Proc. Automatic Speech Recognition and Understanding 196201 (2011).\n",
      "Hinton, G. et al. Deep neural networks for acoustic modeling in speech recognition. IEEE Signal Processing Magazine 29, 8297 (2012). This joint paper from the major speech recognition laboratories, summarizing the breakthrough achieved with deep learning on the task of phonetic classification for automatic speech recognition, was the first major industrial application of deep learning.\n",
      "Sutskever, I. Vinyals, O. & Le. Q. V. Sequence to sequence learning with neural networks. In Proc. Advances in Neural Information Processing Systems 27 31043112 (2014). This paper showed state-of-the-art machine translation results with the architecture introduced in ref. 72, with a recurrent network trained to read a sentence in one language, produce a semantic representation of its meaning, and generate a translation in another language.\n",
      "LeCun, Y. Une procdure dapprentissage pour Rseau  seuil assymtrique in Cognitiva 85: a la Frontire de lIntelligence Artificielle, des Sciences de la Connaissance et des Neurosciences [in French] 599604 (1985).\n",
      "Glorot, X., Bordes, A. & Bengio. Y. Deep sparse rectifier neural networks. In Proc. 14th International Conference on Artificial Intelligence and Statistics 315323 (2011). This paper showed that supervised training of very deep neural networks is much faster if the hidden layers are composed of ReLU.\n",
      "Dauphin, Y. et al. Identifying and attacking the saddle point problem in high-dimensional non-convex optimization. In Proc. Advances in Neural Information Processing Systems 27 29332941 (2014).\n",
      "Choromanska, A., Henaff, M., Mathieu, M., Arous, G. B. & LeCun, Y. The loss surface of multilayer networks. In Proc. Conference on AI and Statistics http://arxiv.org/abs/1412.0233 (2014).\n",
      "Hinton, G. E., Osindero, S. & Teh, Y.-W. A fast learning algorithm for deep belief nets. Neural Comp. 18, 15271554 (2006). This paper introduced a novel and effective way of training very deep neural networks by pre-training one hidden layer at a time using the unsupervised learning procedure for restricted Boltzmann machines.\n",
      "Bengio, Y., Lamblin, P., Popovici, D. & Larochelle, H. Greedy layer-wise training of deep networks. In Proc. Advances in Neural Information Processing Systems 19 153160 (2006). This report demonstrated that the unsupervised pre-training method introduced in ref. 32 significantly improves performance on test data and generalizes the method to other unsupervised representation-learning techniques, such as auto-encoders.\n",
      "Ranzato, M., Poultney, C., Chopra, S. & LeCun, Y. Efficient learning of sparse representations with an energy-based model. In Proc. Advances in Neural Information Processing Systems 19 11371144 (2006).\n",
      "Sermanet, P., Kavukcuoglu, K., Chintala, S. & LeCun, Y. Pedestrian detection with unsupervised multi-stage feature learning. In Proc. International Conference on Computer Vision and Pattern Recognition http://arxiv.org/abs/1212.0142 (2013).\n",
      "Dahl, G. E., Yu, D., Deng, L. & Acero, A. Context-dependent pre-trained deep neural networks for large vocabulary speech recognition. IEEE Trans. Audio Speech Lang. Process. 20, 3342 (2012).\n",
      "LeCun, Y. et al. Handwritten digit recognition with a back-propagation network. In Proc. Advances in Neural Information Processing Systems 396404 (1990). This is the first paper on convolutional networks trained by backpropagation for the task of classifying low-resolution images of handwritten digits.\n",
      "LeCun, Y., Bottou, L., Bengio, Y. & Haffner, P. Gradient-based learning applied to document recognition. Proc. IEEE 86, 22782324 (1998). This overview paper on the principles of end-to-end training of modular systems such as deep neural networks using gradient-based optimization showed how neural networks (and in particular convolutional nets) can be combined with search or inference mechanisms to model complex outputs that are interdependent, such as sequences of characters associated with the content of a document.\n",
      "Bottou, L., Fogelman-Souli, F., Blanchet, P. & Lienard, J. Experiments with time delay networks and dynamic time warping for speaker independent isolated digit recognition. In Proc. EuroSpeech 89 537540 (1989).\n",
      "Tompson, J., Goroshin, R. R., Jain, A., LeCun, Y. Y. & Bregler, C. C. Efficient object localization using convolutional networks. In Proc. Conference on Computer Vision and Pattern Recognition http://arxiv.org/abs/1411.4280 (2014).\n",
      "Taigman, Y., Yang, M., Ranzato, M. & Wolf, L. Deepface: closing the gap to human-level performance in face verification. In Proc. Conference on Computer Vision and Pattern Recognition 17011708 (2014).\n",
      "Farabet, C., Couprie, C., Najman, L. & LeCun, Y. Scene parsing with multiscale feature learning, purity trees, and optimal covers. In Proc. International Conference on Machine Learning http://arxiv.org/abs/1202.2160 (2012).\n",
      "Srivastava, N., Hinton, G., Krizhevsky, A., Sutskever, I. & Salakhutdinov, R. Dropout: a simple way to prevent neural networks from overfitting. J. Machine Learning Res. 15, 19291958 (2014).\n",
      "Sermanet, P. et al. Overfeat: integrated recognition, localization and detection using convolutional networks. In Proc. International Conference on Learning Representations http://arxiv.org/abs/1312.6229 (2014).\n",
      "Girshick, R., Donahue, J., Darrell, T. & Malik, J. Rich feature hierarchies for accurate object detection and semantic segmentation. In Proc. Conference on Computer Vision and Pattern Recognition 580587 (2014).\n",
      "Simonyan, K. & Zisserman, A. Very deep convolutional networks for large-scale image recognition. In Proc. International Conference on Learning Representations http://arxiv.org/abs/1409.1556 (2014).\n",
      "Farabet, C. et al. Large-scale FPGA-based convolutional networks. In Scaling up Machine Learning: Parallel and Distributed Approaches (eds Bekkerman, R., Bilenko, M. & Langford, J.) 399419 (Cambridge Univ. Press, 2011).\n",
      "Bengio, Y., Ducharme, R. & Vincent, P. A neural probabilistic language model. In Proc. Advances in Neural Information Processing Systems 13 932938 (2001). This paper introduced neural language models, which learn to convert a word symbol into a word vector or word embedding composed of learned semantic features in order to predict the next word in a sequence.\n",
      "Cho, K. et al. Learning phrase representations using RNN encoder-decoder for statistical machine translation. In Proc. Conference on Empirical Methods in Natural Language Processing 17241734 (2014).\n",
      "Socher, R., Lin, C. C-Y., Manning, C. & Ng, A. Y. Parsing natural scenes and natural language with recursive neural networks. In Proc. International Conference on Machine Learning 129136 (2011).\n",
      "Mikolov, T., Sutskever, I., Chen, K., Corrado, G. & Dean, J. Distributed representations of words and phrases and their compositionality. In Proc. Advances in Neural Information Processing Systems 26 31113119 (2013).\n",
      "Bahdanau, D., Cho, K. & Bengio, Y. Neural machine translation by jointly learning to align and translate. In Proc. International Conference on Learning Representations http://arxiv.org/abs/1409.0473 (2015).\n",
      "Hochreiter, S. & Schmidhuber, J. Long short-term memory. Neural Comput. 9, 17351780 (1997). This paper introduced LSTM recurrent networks, which have become a crucial ingredient in recent advances with recurrent networks because they are good at learning long-range dependencies.\n",
      "ElHihi, S. & Bengio, Y. Hierarchical recurrent neural networks for long-term dependencies. In Proc. Advances in Neural Information Processing Systems 8 http://papers.nips.cc/paper/1102-hierarchical-recurrent-neural-networks-for-long-term-dependencies (1995).\n",
      "Xu, K. et al. Show, attend and tell: Neural image caption generation with visual attention. In Proc. International Conference on Learning Representations http://arxiv.org/abs/1502.03044 (2015).\n",
      "Graves, A., Mohamed, A.-R. & Hinton, G. Speech recognition with deep recurrent neural networks. In Proc. International Conference on Acoustics, Speech and Signal Processing 66456649 (2013).\n",
      "Vincent, P., Larochelle, H., Bengio, Y. & Manzagol, P.-A. Extracting and composing robust features with denoising autoencoders. In Proc. 25th International Conference on Machine Learning 10961103 (2008).\n",
      "Bengio, Y., Thibodeau-Laufer, E., Alain, G. & Yosinski, J. Deep generative stochastic networks trainable by backprop. In Proc. 31st International Conference on Machine Learning 226234 (2014).\n",
      "Vinyals, O., Toshev, A., Bengio, S. & Erhan, D. Show and tell: a neural image caption generator. In Proc. International Conference on Machine Learning http://arxiv.org/abs/1502.03044 (2014).\n",
      "Yann LeCunContact Yann LeCunSearch for this author in:Nature Research journals PubMed Google ScholarYoshua BengioSearch for this author in:Nature Research journals PubMed Google ScholarGeoffrey HintonSearch for this author in:Nature Research journals PubMed Google Scholar\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from CxExtractor import CxExtractor\n",
    "cx = CxExtractor(threshold=186)\n",
    "print cx.crawl('http://www.nature.com/nature/journal/v521/n7553/abs/nature14539.html?foxtrotcallback=true')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<generator object gen at 0x7fe229b559b0>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def gen():\n",
    "    yield 3\n",
    "    yield 5\n",
    "s = gen()\n",
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "pop from empty list",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-353a8f813834>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m: pop from empty list"
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
