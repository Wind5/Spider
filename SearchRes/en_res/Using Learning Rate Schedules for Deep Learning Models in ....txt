Training a neural network or large deep learning model is a difficult optimization task.
The classical algorithm to train neural networks is called stochastic gradient descent. It has been well established that you can achieve increased performance and faster training on some problems by using a learning rate that changes during training.
In this post you will discover how you can use different learning rate schedules for your neural network models in Python using the Keras deep learning library.
Update Mar/2017: Updated example for Keras 2.0.2, TensorFlow 1.0.1 and Theano 0.9.0.
Using Learning Rate Schedules for Deep Learning Models in Python with KerasPhoto by Columbia GSAPP, some rights reserved.
Adapting the learning rate for your stochastic gradient descent optimization procedure can increase performance and reduce training time.
Sometimes this is called learning rate annealing or adaptive learning rates. Here we will call this approach a learning rate schedule, were the default schedule is to use a constant learning rate to update network weights for each training epoch.
The simplest and perhaps most used adaptation of learning rate during training are techniques that reduce the learning rate over time. These have the benefit of making large changes at the beginning of the training procedure when larger learning rate values are used, and decreasing the learning rate such that a smaller rate and therefore smaller training updates are made to weights later in the training procedure.
This has the effect of quickly learning good weights early and fine tuning them later.
Two popular and easy to use learning rate schedules are as follows:
Decrease the learning rate using punctuated large drops at specific epochs.
Next, we will look at how you can use each of these learning rate schedules in turn with Keras.
Take my free 2-week email course and discover MLPs, CNNs and LSTMs (with sample code).
Click to sign-up now and also get a free PDF Ebook version of the course.
The stochastic gradient descent optimization algorithm implementation in the SGD class has an argument called decay. This argument is used in the time-based learning rate decay schedule equation as follows:
When the decay argument is specified, it will decrease the learning rate from the previous epoch by the given fixed amount.
For example, if we use the initial learning rate value of 0.1 and the decay of 0.001, the first 5 epochs will adapt the learning rate as follows:
Extending this out to 100 epochs will produce the following graph of learning rate (y axis) versus epoch (x axis):
You can create a nice default schedule by setting the decay value as follows:
The example below demonstrates using the time-based learning rate adaptation schedule in Keras.
It is demonstrated on the Ionosphere binary classification problem. This is a small dataset that you can download from the UCI Machine Learning repository. Place the data file in your working directory with the filename ionosphere.csv.
The ionosphere dataset is good for practicing with neural networks because all of the input values are small numerical values of the same scale.
A small neural network model is constructed with a single hidden layer with 34 neurons and using the rectifier activation function. The output layer has a single neuron and uses the sigmoid activation function in order to output probability-like values.
The learning rate for stochastic gradient descent has been set to a higher value of 0.1. The model is trained for 50 epochs and the decay argument has been set to 0.002, calculated as 0.1/50. Additionally, it can be a good idea to use momentum when using an adaptive learning rate. In this case we use a momentum value of 0.8.
# Time Based Learning Rate Decayfrom pandas import read_csvimport numpyfrom keras.models import Sequentialfrom keras.layers import Densefrom keras.optimizers import SGDfrom sklearn.preprocessing import LabelEncoder# fix random seed for reproducibilityseed = 7numpy.random.seed(seed)# load datasetdataframe = read_csv("ionosphere.csv", header=None)dataset = dataframe.values# split into input (X) and output (Y) variablesX = dataset[:,0:34].astype(float)Y = dataset[:,34]# encode class values as integersencoder = LabelEncoder()encoder.fit(Y)Y = encoder.transform(Y)# create modelmodel = Sequential()model.add(Dense(34, input_dim=34, kernel_initializer='normal', activation='relu'))model.add(Dense(1, kernel_initializer='normal', activation='sigmoid'))# Compile modelepochs = 50learning_rate = 0.1decay_rate = learning_rate / epochsmomentum = 0.8sgd = SGD(lr=learning_rate, momentum=momentum, decay=decay_rate, nesterov=False)model.compile(loss='binary_crossentropy', optimizer=sgd, metrics=['accuracy'])# Fit the modelmodel.fit(X, Y, validation_split=0.33, epochs=epochs, batch_size=28, verbose=2)
The model is trained on 67% of the dataset and evaluated using a 33% validation dataset.
Running the example shows a classification accuracy of 99.14%. This is higher than the baseline of 95.69% without the learning rate decay or momentum.
...Epoch 45/500s - loss: 0.0622 - acc: 0.9830 - val_loss: 0.0929 - val_acc: 0.9914Epoch 46/500s - loss: 0.0695 - acc: 0.9830 - val_loss: 0.0693 - val_acc: 0.9828Epoch 47/500s - loss: 0.0669 - acc: 0.9872 - val_loss: 0.0616 - val_acc: 0.9828Epoch 48/500s - loss: 0.0632 - acc: 0.9830 - val_loss: 0.0824 - val_acc: 0.9914Epoch 49/500s - loss: 0.0590 - acc: 0.9830 - val_loss: 0.0772 - val_acc: 0.9828Epoch 50/500s - loss: 0.0592 - acc: 0.9872 - val_loss: 0.0639 - val_acc: 0.9828
Another popular learning rate schedule used with deep learning models is to systematically drop the learning rate at specific times during training.
Often this method is implemented by dropping the learning rate by half every fixed number of epochs. For example, we may have an initial learning rate of 0.1 and drop it by 0.5 every 10 epochs. The first 10 epochs of training would use a value of 0.1, in the next 10 epochs a learning rate of 0.05 would be used, and so on.
If we plot out the learning rates for this example out to 100 epochs you get the graph below showing learning rate (y axis) versus epoch (x axis).
We can implement this in Keras using a the LearningRateScheduler callback when fitting the model.
The LearningRateScheduler callback allows us to define a function to call that takes the epoch number as an argument and returns the learning rate to use in stochastic gradient descent. When used, the learning rate specified by stochastic gradient descent is ignored.
In the codeÂ below, we use the same example before of a single hidden layer network on the Ionosphere dataset. A new step_decay() function is defined that implements the equation:
Where InitialLearningRate is the initial learning rate such as 0.1, the DropRate is the amount that the learning rate is modified each time it is changed such as 0.5, Epoch is the current epoch number and EpochDrop is how often to change the learning rate such as 10.
Notice that we set the learning rate in the SGD class to 0 to clearly indicate that it is not used. Nevertheless, you can set a momentum term in SGD if you want to use momentum with this learning rate schedule.
# Drop-Based Learning Rate Decayimport pandasfrom pandas import read_csvimport numpyimport mathfrom keras.models import Sequentialfrom keras.layers import Densefrom keras.optimizers import SGDfrom sklearn.preprocessing import LabelEncoderfrom keras.callbacks import LearningRateScheduler # learning rate scheduledef step_decay(epoch):initial_lrate = 0.1drop = 0.5epochs_drop = 10.0lrate = initial_lrate * math.pow(drop, math.floor((1+epoch)/epochs_drop))return lrate # fix random seed for reproducibilityseed = 7numpy.random.seed(seed)# load datasetdataframe = read_csv("ionosphere.csv", header=None)dataset = dataframe.values# split into input (X) and output (Y) variablesX = dataset[:,0:34].astype(float)Y = dataset[:,34]# encode class values as integersencoder = LabelEncoder()encoder.fit(Y)Y = encoder.transform(Y)# create modelmodel = Sequential()model.add(Dense(34, input_dim=34, kernel_initializer='normal', activation='relu'))model.add(Dense(1, kernel_initializer='normal', activation='sigmoid'))# Compile modelsgd = SGD(lr=0.0, momentum=0.9, decay=0.0, nesterov=False)model.compile(loss='binary_crossentropy', optimizer=sgd, metrics=['accuracy'])# learning schedule callbacklrate = LearningRateScheduler(step_decay)callbacks_list = [lrate]# Fit the modelmodel.fit(X, Y, validation_split=0.33, epochs=50, batch_size=28, callbacks=callbacks_list, verbose=2)
...Epoch 45/500s - loss: 0.0546 - acc: 0.9830 - val_loss: 0.0634 - val_acc: 0.9914Epoch 46/500s - loss: 0.0544 - acc: 0.9872 - val_loss: 0.0638 - val_acc: 0.9914Epoch 47/500s - loss: 0.0553 - acc: 0.9872 - val_loss: 0.0696 - val_acc: 0.9914Epoch 48/500s - loss: 0.0537 - acc: 0.9872 - val_loss: 0.0675 - val_acc: 0.9914Epoch 49/500s - loss: 0.0537 - acc: 0.9872 - val_loss: 0.0636 - val_acc: 0.9914Epoch 50/500s - loss: 0.0534 - acc: 0.9872 - val_loss: 0.0679 - val_acc: 0.9914
This section lists some tips and tricks to consider when using learning rate schedules with neural networks.
Increase the initial learning rate. Because the learning rate will very likely decrease, start with a larger value to decrease from. A larger learning rate will result in a lot larger changes to the weights, at least in the beginning, allowing you to benefit from the fine tuning later.
Use a large momentum. Using a larger momentum value will help the optimization algorithm to continue to make updates in the right direction when your learning rate shrinks to small values.
Experiment with different schedules. It will not be clear which learning rate schedule to use so try a few with different configuration options and see what works best on your problem. Also try schedules that change exponentially and even schedules that respond to the accuracy of your model on the training or test datasets.
In this post you discovered learning rate schedules for training neural network models.
How to configure and use a time-based learning rate schedule in Keras.
How to develop your own drop-based learning rate schedule in Keras.
Do you have any questions about learning rate schedules for neural networks or about this post? Ask your question in the comments and I will do my best to answer.
Dr. Jason Brownlee is a husband, proud father, academic researcher, author, professional developer and a machine learning practitioner. He is dedicated to helping developers get started and get good at applied machine learning.
Interesting post. Cant we use this learning rate and momentum for optimizers other than SGD? Since adam is performing good with most of the datasets, i wanna try learning rate and momentum tuning for adam optimizer. Also, I did a quick research on this and found that adam already have decaying learning rate. Is it true? and what about other optimizers?
I want to implement a learning rate that is exponentially decayed with respect to the cost? For example, the learning rate is updated as follows:
Such that a larger learning rate is employed when the cost is large, vice versa.
I would advise you to develop your decay function in excel or something to test and plot it. Use the above linear decay function inputs/outputs as a starting point.
You can then plug your function into one of the examples above. 
Does the weight decay change once per mini-batch or epoch? According to https://groups.google.com/forum/#!topic/keras-users/7KM2AvCurW0, it updates per mini-batch. In my problem, one epoch contains 800 mini-batches.
