 Sparse feature learning for deep belief networksConference Paper · January 2007 with 205 Reads Conference: Advances in Neural Information Processing Systems (NIPS 2007)Cite this publication1st MarcAurelio Ranzato10.56 · Google Inc.2nd Y-Lan Boureau3rd Yann Lecun33.47 · New York UniversityAbstractUnsupervised learning algorithms aim to discover the structure hidden in the data, and to learn representations that are more suitable as input to a supervised machine than the raw input. Many unsupervised methods are based on reconstructing the input from the representation, while constraining the repr esentation to have cer- tain desirable properties (e.g. low dimension, sparsity, e tc). Others are based on approximating density by stochastically reconstructing t he input from the repre- sentation. We describe a novel and efficient algorithm to lea rn sparse represen- tations, and compare it theoretically and experimentally with a similar machine trained probabilistically, namely a Restricted Boltzmann Machine. We propose a simple criterion to compare and select different unsupervised machines based on the trade-off between the reconstruction error and the info rmation content of the representation. We demonstrate this method by extracting features from a dataset of handwritten numerals, and from a dataset of natural image patches. We show that by stacking multiple levels of such machines and by training sequentially, high-order dependencies between the input observed variables can be captured.Discover the worlds research13+ million members100+ million publications700k+ research projectsJoin for free
 SparseFeature LearningforDeepBeliefNetworksMarc’AurelioRanzato1Y-LanBoureau2,1Yann LeCun11CourantInstituteofMathematicalSciences,New YorkUniversity2INRIARocquencourt{ranzato,ylan,yann@courant.nyu.edu}AbstractUnsupervisedlearning algorithmsaimto discoverthestructurehiddeninthedata,and tolearnrepresentationsthataremoresuitable asinput toasupervisedmachinethantherawinput.Many unsupervisedmethodsarebased on reconstructing theinputfromtherepresentation,while constraining therepresentation to have cer-tain desirableproperties(e.g.lowdimension,sparsity,etc).Othersarebased onapproximating density by stochasticallyreconstructing theinputfromtherepre-sentation.Wedescribe a noveland efﬁcientalgorithmtolearnsparserepresen-tations,and compareit theoreticallyand experimentallywithasimilarmachinetrained probabilistically,namelyaRestrictedBoltzmann Machine.Wepropose asimple criterion tocompare and selectdifferentunsupervisedmachinesbased onthetrade-off betweenthereconstruction errorand theinformation contentoftherepresentation.Wedemonstratethismethod by extracting featuresfromadatasetofhandwritten numerals,and fromadatasetofnatural imagepatches.Weshowthatby stacking multiplelevelsofsuchmachinesand by training sequentially,high-orderdependenciesbetweentheinputobserved variablescan be captured.1IntroductionOneofthemain purposesofunsupervisedlearning isto produce good representationsfordata,thatcan beusedfordetection,recognition,prediction,orvisualization.Good representationseliminateirrelevantvariabilitiesoftheinputdata,whilepreserving theinformation that isusefulfortheul-timatetask.One causefortherecentresurgence ofinterest in unsupervisedlearning isthe abilityto produce deepfeaturehierarchiesby stacking unsupervisedmoduleson top ofeach other,aspro-posed by Hinton etal.[1],Bengioetal.[2]and ourgroup [3,4].Theunsupervisedmodule atonelevel inthehierarchy isfedwiththerepresentation vectorsproduced by thelevelbelow.Higher-levelrepresentationscapturehigh-leveldependenciesbetweeninputvariables,thereby improvingthe ability ofthesystemtocaptureunderlying regularitiesinthedata.Theoutputofthelast layerinthehierarchy can befedtoa conventionalsupervisedclassiﬁer.Anaturalwayto design stackableunsupervisedlearning systemsistheencoder-decoderparadigm[5].Anencodertransformstheinput intotherepresentation (also knownasthecodeorthefeaturevector),and adecoderreconstructstheinput(perhaps stochastically) fromtherepre-sentation.PCA,Auto-encoderneuralnets,RestrictedBoltzmann Machines(RBMs),ourprevioussparse energy-basedmodel[3],and themodelproposedin[6] fornoisy overcomplete channelsarejustexamplesofthiskind ofarchitecture.The encoder/decoderarchitectureisattractivefortworea-sons:1.aftertraining,computing the codeisaveryfastprocess thatmerelyconsistsinrunning theinput through the encoder;2.reconstructing theinputwiththedecoderprovidesawaytocheckthatthe codehascapturedtherelevant information inthedata.Somelearning algorithms[7]do nothaveadecoderand mustresort tocomputationallyexpensiveMarkov ChainMonteCarlo(MCMC)sam-pling methodsin orderto providereconstructions.Otherlearning algorithms[8,9]lackanencoder,whichmakesit necessarytorun anexpensiveoptimization algorithmtoﬁnd the code associatedwitheach newinputsample.Inthispaperwewill focusonly on encoder-decoderarchitectures.1
 In general terms,we can viewan unsupervisedmodelasdeﬁning adistribution overinputvectorsYthrough anenergy function E(Y,Z,W):P(Y|W)=ZzP(Y,z|W)=Rze−βE(Y,z,W)Ry,ze−βE(y,z,W)(1)whereZisthe codevector,Wthetrainableparametersofencoderand decoder,and βisanarbitrarypositive constant.The energy function includesthereconstruction error,and perhapsothertermsaswell.Forconvenience,wewill omit Wfromthenotation inthefollowing.Training themachinetomodel theinputdistribution isperformed by ﬁnding the encoderand decoderparametersthatminimize a loss function equal tothenegativelog likelihood ofthetraining dataunderthemodel.Forasingletraining sampleY,theloss function isL(W,Y)=−1βlog Zze−βE(Y,z)+1βlog Zy,ze−βE(y,z)(2)Theﬁrst termistheminimumof thefree energyFβ(Y).Assuming that thedistribution overZisratherpeaked,it can besimplertoapproximatethisdistribution overZby itsmode,whichturnsthemarginalization overZintoaminimization:L(W,Y)=E(Y,Z∗(Y)) +1βlog Zye−βE(y,Z∗(y)) (3)whereZ∗(Y)isthemaximumlikelihood valueZ∗(Y)=argminzE(Y,z),also knownastheoptimalcode.We canthen deﬁne anenergy foreachinputpoint,thatmeasureshow well it isreconstructed by themodel:F∞(Y)=E(Y,Z∗(Y)) =limβ→∞ −1βlog Zze−βE(Y,z)(4)Thesecond terminequation 2 and 3 iscalledthelog partition function,and can beviewedasapenaltytermforlowenergies.Itensuresthat thesystemproduceslowenergy onlyforinputvectorsthathavehigh probabilityinthe(true)datadistribution,and produceshigherenergiesforall otherinputvectors[5].Theoverall loss isthe averageofthe aboveoverthetraining set.Regardless ofwhetheronlyZ∗orthewholedistribution overZisconsidered,themain difﬁcultywiththisframeworkisthat it can bevery hardtocomputethegradientofthelog partition functioninequation 2 or3withrespect totheparametersW.Efﬁcientmethods shortcut the computation bydrasticallyand cleverlyreducing theintegration domain.Forinstance,RestrictedBoltzmann Ma-chines(RBM) [10]approximatethegradientofthelog partition function inequation 2 by samplingvaluesofYwhose energy will bepulled up using anMCMCtechnique.Byrunning theMCMCforashort time,thosesamplesare choseninthevicinity ofthetraining samples,thereby ensuring thatthe energy surface formsaravine around themanifold ofthetraining samples.ThisisthebasisoftheContrastiveDivergence method [10].Theroleofthelog partition function ismerelytoensurethat the energy surface isloweraroundtraining samplesthananywhereelse.Themethod proposed here eliminatesthelog partition functionfromtheloss,and replacesit by atermthatlimitsthe volumeof theinputspace overwhichthe energysurface can take alowvalue.Thisisperformed by adding a penaltytermon the coderatherthan ontheinput.Whilethisclass ofmethodsdoesnotdirectlymaximize thelikelihoodofthedata,it can beseenasa crude approximation ofit.To understand themethod,weﬁrstnotethat if foreach vectorY,there existsa corresponding optimalcodeZ∗(Y)thatmakesthereconstruction error(orenergy)F∞(Y)zero(ornearzero),themodelcan perfectlyreconstructany inputvector.Thismakestheenergy surface ﬂatand indiscriminate.Ontheotherhand,ifZcan onlytake a small numberofdifferentvalues(lowentropy code),thenthe energy F∞(Y)can only belowinalimited numberofplaces(theY’sthatarereconstructedfromthis small numberofZvalues),and the energy cannotbeﬂat.Moregenerally,a convenientmethod through whichﬂatenergy surfacescan be avoidedistolimitthemaximuminformation contentof the code.Hence,minimizing the energyF∞(Y)togetherwiththeinformation contentof the codeisa good substituteforminimizing log thepartition function.2
 Apopularwaytominimize theinformation content inthe codeistomakethe codesparseorlow-dimensional[5].Thistechniqueisusedinanumberofunsupervisedlearning methods,includingPCA,auto-encodersneuralnetwork,and sparse coding methods[6,3,8,9].Insparsemethods,the codeisforcedto haveonlyafewnon-zero unitswhilemostcodeunitsare zeromostofthetime.Sparse-overcompleterepresentationshave a numberoftheoreticaland practicaladvantages,asdemonstratedinanumberofrecentstudies[6,8,3].In particular,they havegood robustness tonoise,and provide a good tiling ofthejointspace oflocation and frequency.Inaddition,theyareadvantageousforclassiﬁersbecause classiﬁcation ismorelikelyto be easierin higherdimensionalspaces.Thismayexplainwhy biology seemstolikesparserepresentations[11].In ourcontext,themainadvantageofsparsityconstraintsistoallowustoreplace a marginalization by aminimization,and tofree ourselvesfromtheneedtominimize thelog partition function explicitly.Inthispaperwepropose a newunsupervisedlearning algorithmcalledSparseEncoding SymmetricMachine(SESM),whichisbased on the encoder-decoderparadigm,and whichisableto producesparseovercompleterepresentationsefﬁcientlywithoutany needforﬁlternormalization [8,12]orcodesaturation [3].Asdescribedinmoredetailsinsec.2and 3,we consideraloss function whichisaweightedsumofthereconstruction errorand asparsity penalty,asinmany otherunsupervisedlearning algorithms[13,14,8].Encoderand decoderare constrainedto besymmetric,and shareasetoflinearﬁlters.Although weonlyconsiderlinearﬁltersinthispaper,themethod allowstheuseofany differentiablefunction forencoderand decoder.Wepropose aniterativeon-linelearning algorithmwhichiscloselyrelatedtothoseproposed by Olshausenand Field[8]and by uspreviously[3].Theﬁrststepcomputestheoptimalcodeby minimizing the energy forthegiveninput.Thesecond step updatestheparametersofthemachinesoastominimize the energy.Insec.4,we compareSESMwithRBMand PCA.Following [15],we evaluatethesemethodsbymeasuring thereconstruction errorforagivenentropy ofthe code.Inanothersetofexperiments,wetraina classiﬁeron thefeaturesextracted by thevariousmethods,and measurethe classiﬁcationerroron theMNISTdatasetofhandwritten numerals.Interestingly,themachine achieving thebestrecognition performance istheonewiththebest trade-off betweenRMSEand entropy.Insec.5,wecomparetheﬁlterslearned by SESMand RBMforhandwritten numeralsand natural imagepatches.Insec.5.1.1,wedescribe a simplewayto produce a deep beliefnetby stacking multiplelevelsofSESMmodules.Therepresentationalpowerofthishierarchicalnon-linearfeature extraction isdemonstratedthrough theunsuperviseddiscovery ofthenumeralclass labelsinthehigh-levelcode.2ArchitectureInthis section wedescribe a SparseEncoding SymmetricMachine(SESM)having asetoflinearﬁl-tersin bothencoderand decoder.However,everything can be easilyextendedtoany otherchoice ofparameterizedfunctionsaslong asthese aredifferentiable and maintainsymmetry betweenencoderand decoder.LetusdenotewithYtheinputdeﬁnedinRN,and withZthe codedeﬁnedinRM,whereMisin generalgreaterthanN(forovercompleterepresentations).Let theﬁltersinencoderand decoderbethe columnsofmatrixW∈RM×N,and let thebiasesinthe encoderand decoderbedenoted by benc∈RMand bdec∈RN,respectively.Then,encoderand decodercompute:fenc(Y)=WTY+benc,fdec(Z)=Wl(Z)+bdec(5)wherethefunction lisapoint-wiselogisticnon-linearity oftheform:l(x)=1/(1+exp(−gx)),withgﬁxed gain.Thesystemischaracterized by anenergy measuring the compatibility betweenpairsofinputYand latentcodeZ,E(Y,Z)[16].Thelowerthe energy,themore compatible(orlikely)isthepair.Wedeﬁnethe energy as:E(Y,Z)=αekZ−fenc(Y)k22+kY−fdec(Z)k22(6)During training weminimize thefollowing loss:L(W)=E(Y,Z)+αsh(Z)+αrkWk1=αekZ−fenc(Y)k22+kY−fdec(Z)k22+αsh(Z)+αrkWk1(7)Theﬁrst termtriestomaketheoutputofthe encoderas similaraspossibletothe codeZ.Thesecond termisthemean-squarederrorbetweentheinputYand thereconstruction provided by thedecoder.Thethirdtermensuresthesparsityofthe codeby penalizing non zero valuesofcodeunits;3
 thistermactsindependently on eachcodeunit and it isdeﬁnedash(Z)=PMi=1log(1+l2(zi)),(corresponding toafactorizedStudent-tpriordistribution on the codeunits[8]).Thelast termisanL1regularization on theﬁlterstosuppress noise and favormorelocalizedﬁlters.Thelossformulatedinequation 7 combinestermsthatcharacterize also othermethods.Forinstance,theﬁrst twotermsappearin ourpreviousmodel[3],but inthatwork,theweightsofencoderanddecoderwerenot tiedand theparametersinthelogisticwereupdated using running averages.Thesecond and thirdtermsarepresent inthe “decoder-only”modelproposedin[8].Thethirdtermwasusedinthe “encoder-only”modelof [7].Besidesthe already-mentionedadvantagesofusinganencoder-decoderarchitecture,wepointoutanothergood featureofthisalgorithmduetoitssymmetry.Acommon idiosyncrasyforsparse-overcompletemethodsusing bothareconstructionand asparsity penaltyintheobjectivefunction (second and thirdterminequation 7),istheneedtonormalizethenormofthebasisfunctionsinthedecoderduring learning [8,12]withsomewhatad-hoctechnique,otherwisesomeofthebasisfunctionscollapsetozero,and someblowup toinﬁnity.Becauseofthesparsity penaltyand thelinear reconstruction,codeunitsbecometiny andare compensated by theﬁltersinthedecoderthatgrow withoutbound.Eventhough theoverallloss decreases,training isunsuccessful.Unfortunately,simply normalizing theﬁltersmakeslessclearwhich objectivefunction isminimized.Some authorshaveproposed quite expensivemethodstosolvethisissue:by making betterapproximationsoftheposteriordistribution [15],orby usingsampling techniques[17].Inthiswork,weproposetoenforce symmetrybetweenencoderanddecoder(through weightsharing)soasto have automaticscaling ofﬁlters.Theirnormcannotpossibly belargebecause codeunits,produced by the encoderweights,would havelargevaluesaswell,producing badreconstructionsand increasing the energy (thesecond terminequation 6 and7).3LearningAlgorithmLearning consistsofdetermining theparametersinW,benc,and bdecthatminimize theloss inequation 7.Asindicatedintheintroduction,the energy augmentedwiththesparsityconstraint isminimizedwithrespect tothe codetoﬁnd theoptimalcode.Nomarginalization overcodedistribu-tion isperformed.Thisisakinto using theloss function inequation 3.However,thelog partitionfunction termisdropped.Instead,werely on the codesparsityconstraintstoensurethat the energysurface isnotﬂat.Since thesecond terminequation 7 couplesbothZand Wand bdec,it isnotstraightforwardtominimize thisenergy withrespect to both.Ontheotherhand,once Zisgiven,theminimizationwithrespect toWisa convex quadraticproblem.Vice versa,iftheparametersWareﬁxed,theoptimalcodeZ∗thatminimizesLcan be computedeasilythrough gradientdescent.This suggeststhefollowing iterativeon-line coordinatedescent learning algorithm:1.foragivensampleYand parametersetting,minimize theloss inequation 7 withrespect toZbygradientdescent to obtaintheoptimalcodeZ∗2.clamping boththeinputYand theoptimalcodeZ∗found at theprevious step,do onestepofgradientdescent to updatetheparameters.Unlikeothermethods[8,12],no column normalization ofWisrequired.Also,all theparametersareupdated by gradientdescentunlikein ourpreviouswork[3]wheresomeparametersareupdatedusing amoving average.Aftertraining,thesystemconvergestoastatewherethedecoderproducesgoodreconstructionsfromasparse(rectiﬁed)code,and theoptimalcodeispredicted by asimplefeed-forward propagationthrough the encoder.4ComparativeCodingAnalysisInthefollowing sections,wemainlycompareSESMwithRBMin orderto betterunderstand theirdifferencesintermsofmaximumlikelihood approximation,and intermsofcoding efﬁciencyandrobustness.RBMAsexplainedintheintroduction,RBMsminimize anapproximation ofthenegativeloglikelihood ofthedataunderthemodel.AnRBMisabinarystochasticsymmetricmachinedeﬁnedby anenergy function oftheform:E(Y,Z)=−ZTWTY−bTencZ−bTdecY.Although thisisnot4
 obviousatﬁrstglance,thisenergy can beseenasaspecialcaseofthe encoder-decoderarchitecturethatpertainsto binary datavectorsand codevectors[5].Training anRBMminimizesanapproxima-tion ofthenegativelog likelihood loss function 2,averaged overthetraining set,through agradientdescentprocedure.Instead ofestimating thegradientofthelog partition function,RBMtrainingusescontrastivedivergence [10],whichtakesrandomsamplesdrawn overalimitedregion Ωaroundthetraining samples.Theloss becomes:L(W,Y)=−1βlog Xze−βE(Y,z)+1βlog Xy∈ΩXze−βE(y,z)(8)BecauseoftheRBMarchitecture,givenaY,the componentsofZareindependent,hence thesumoverconﬁgurationsofZcan bedoneindependentlyforeachcomponentofZ.Sampling yintheneighborhoodΩisperformedwith one,orafewalternatedMCMCstepsoverY,and Z.Thismeansthatonlythe energy ofpointsaround training samplesispulled up.Hence,thelikelihood functiontakestherightshape around thetraining samples,butnotnecessarilyeverywhere.However,thecodevectorinanRBMisbinaryand noisy,and onemaywonderwhetherthisdoesnothavetheeffectofsurreptitiouslylimiting theinformation contentofthe code,thereby furtherminimizing thelog partition function asabonus.SESMRBMand SESMhave almost thesame architecturebecausethey both have a symmetricencoderand decoder,and alogisticnon-linearity on thetop ofthe encoder.However,RBMistrainedusing (approximate)maximumlikelihood,whileSESMistrained by simplyminimizing the averageenergy F∞(Y)ofequation 4 withanadditionalcodesparsityterm.SESMrelieson thesparsitytermto preventﬂatenergy surfaces,whileRBMrelieson anexplicit contrastivetermintheloss,anapproximation ofthelog partition function.Also,the coding strategy isvery differentbecause codeunitsare “noisy” and binaryinRBM,whiletheyarequasi-binaryand sparseinSESM.Featuresextracted by SESMlook likeobjectparts(see nextsection),whilefeaturesproduced by RBMlackanintuitiveinterpretation becausetheyaimatmodeling theinputdistribution and theyareusedinadistributedrepresentation.4.1ExperimentalComparisonIntheﬁrstexperimentwehavetrainedSESM,RBM,and PCAon theﬁrst20000 digitsintheMNISTtraining dataset[18]in orderto produce codeswith 200 components.Similarlyto[15]wehave collectedtest image codesafter rectiﬁcation (exceptforPCA whichislinear),and wehavemeasuredtherootmeansquare error(RMSE)and the entropy.SESMwasrun fordifferentvaluesofthesparsitycoefﬁcientαsinequation 7 (while all otherparametersareleftunchanged,see nextsection fordetails).TheRMSEisdeﬁnedas1σq1PNkY−fdec(¯Z)k22,where¯Zistheuniformlyquantizedcodeproduced by the encoder,Pisthenumberoftestsamples,and σisthe estimatedvariance ofunitsintheinputY.Assuming toencodethe(quantized)codeunitsindependentlyandwiththesamedistribution,thelowerbound on thenumberofbitsrequiredtoencode each ofthemisgiven by:Hc.u.=−PQi=1ciPMlog2ciPM,whereciisthenumberofcountsinthei-th bin,and Qisthenumberofquantization levels.Thenumberofbitsperpixelisthenequal to:MNHc.u..Unlikein[15,12],thereconstruction isdonetaking thequantizedcodein ordertomeasuretherobustnessofthe codetothequantization noise.As showninﬁg.1-C,RBMisveryrobust to noiseinthecodebecauseit istrained by sampling.TheoppositeistrueforPCA whichachievesthelowestRMSEwhen using high precision codes,but thehighestRMSEwhen using a coarsequantization.SESMseemsto givethebest trade-off betweenRMSEand entropy.Fig.1-D/Fcomparethefeatureslearned by SESMand RBM.Despitethesimilaritiesinthe architecture,ﬁlterslook quitedifferentin general,revealing two differentcoding strategies:distributedforRBM,and sparseforSESM.Inthesecondexperiment,wehave comparedthesemethodsby meansofasupervisedtaskin ordertoassess whichmethod producesthemostdiscriminativerepresentation.Since wehave available alsothelabelsintheMNIST,wehaveusedthe codes(produced by thesemachinestrained unsupervised)asinput tothesamelinearclassiﬁer.Thisisrun for100 epochstominimize thesquarederrorbetween outputsand targets,and hasamildridgeregularizer.Fig.1-A/Bshowtheresult oftheseexperimentsinaddition towhatcan be achieved by alinearclassiﬁertrained on therawpixeldata.Notethat: 1)training on featuresinstead ofrawdataimprovestherecognition (exceptforPCAwhenthenumberoftraining samplesis small),2)RBMperformance iscompetitiveoverall when5
 (A)0.5 11.50510152025303540ENTROPY (bits/pixel)ERROR RATE %10 samples0.5 11.5246810121416ENTROPY (bits/pixel)ERROR RATE %100 samples0.5 11.5456789ENTROPY (bits/pixel)ERROR RATE %1000 samples RAW: trainRAW: testPCA: trainPCA: testRBM: trainRBM: testSESM: trainSESM: test(B)00.1 0.20510152025303540RMSEERROR RATE %10 samples00.1 0.2246810121416RMSEERROR RATE %100 samples00.1 0.2456789RMSEERROR RATE %1000 samples(C)00.5 11.5 20.050.10.150.20.250.30.350.40.45RMSEEntropy (bits/pixel)Symmetric Sparse Coding − RBM − PCA PCA: quantization in 5 binsPCA: quantization in 256 binsRBM: quantization in 5 binsRBM: quantization in 256 binsSparse Coding: quantization in 5 binsSparse Coding: quantization in 256 bins(D)(E) (F)(G) (H)Figure1:(A)-(B)Error rateon MNISTtraining (with 10,100 and 1000 samplesperclass)and testsetproduced by alinearclassiﬁertrained on thenon-linearlytransformedcodesproduced by SESM,RBM,and PCA.The entropy and RMSEreferstoaquantization into 256 bins.The comparisonhasbeenextendedalsotothesame classiﬁertrained on rawpixeldata(showing the advantageofextracting features).The errorbarsreferto 1 std.dev.ofthe error ratefor10 randomchoicesoftraining datasets(samesplitsforall methods).Theparameterαsineq.7takesvalues:1,0.5,0.2,0.1,0.05.(C)Comparison betweenSESM,RBM,and PCA when quantizing the codeinto 5 and256 bins.(D)Randomselection fromthe200 linearﬁltersthatwerelearned by SESM(αs=0.2).(E)Somepairsoforiginaland reconstructed digit fromthe codeproduced by the encoderinSESM(feed-forward propagation through encoderand decoder).(F)Randomselection ofﬁlterslearned byRBM.(G)Back-projectioninimagespace oftheﬁlterslearnedinthesecond stageofthehierarchicalfeature extractor.Thesecond stagewastrained on therectiﬁedcodesproduced by theﬁrststagemachine.Theback-projection hasbeen performed by using a1-of-10 codeinthesecond stagemachine,and propagating thisthrough thesecond stagedecoderand ﬁrststagedecoder.Theﬁltersat thesecond stagediscoverthe class-prototypes(manually orderedforvisualconvenience)eventhough no class labelwaseverused during training.(H)Feature extraction from8x8 natural imagepatches:someﬁltersthatwerelearned.6
 fewtraining samplesare available,3)thebestperformance isachieved by SESMforasparsitylevelwhichtradesoff RMSEforentropy (overall forlargetraining sets),4)themethod withthebestRMSEisnottheonewithlowesterror rate,5)comparedtoaSESMhaving thesame error rateRBMismore costlyintermsofentropy.5ExperimentsThis section describes some experimentswehavedonewithSESM.The coefﬁcientαeinequation 7hasalwaysbeensetequal to 1,and thegaininthelogistichavebeensetequal to 7 in ordertoachieveaquasi-binarycoding.Theparameterαshasto besetby cross-validation toavaluewhich dependson thelevelofsparsityrequired by thespeciﬁc application.5.1HandwrittenDigitsFig.1-B/Eshowstheresult oftraining aSESMwithαsisequal to 0.2.Training wasperformedon 20000 digits scaled between 0 and 1,by setting αrto 0.01 (inequation 7)withalearning rateequal to 0.001 (decreasedexponentially).Filtersdetect thestrokesthatcan be combinedtoformadigit.Evenifthe codeunit activation hasaverysparsedistribution,reconstructionsarevery good(no minimization incodespace wasperformed).5.1.1HierarchicalFeaturesAhierarchicalfeature extractorcan betrainedlayer-by-layersimilarlytowhathasbeen proposedin[19,1] fortraining deep beliefnets(DBNs).Wehavetrainedasecond (higher)stagemachineontherectiﬁedcodesproduced by theﬁrst(lower)stagemachinedescribedinthepreviousexample.Againtraining wasperformed on 20000 codesto produce a higherlevelrepresentation withjust10components.Since we aimedtoﬁnd a1-of-10 codeweincreasedthesparsitylevel(inthesecondstagemachine)by setting αsto 1.Despitethe completelyunsupervisedtraining procedure,thefeaturedetectorsinthesecond stagemachinelook likedigit prototypesascan beseeninﬁg.1-G.Thehierarchicalunsupervisedfeature extractorisabletocapturehigherordercorrelationsamongtheinputpixel intensities,and to discoverthehighly non-linearmapping fromrawpixeldatatotheclass labels.Changing therandominitialization cansometimesleadtothediscoveroftwo differentshapesof“9”withoutaunit encoding the “4”,forinstance.Nevertheless,resultsarequalitativelyverysimilartothisone.Forcomparison,whentraining aDBN,prototypesarenotrecoveredbecausethelearnedcodeisdistributedamong units.5.2NaturalImagePatchesASESMwithabout thesamesetup wastrained on adatasetof30000 8x8 natural imagepatchesrandomlyextractedfromtheBerkeleysegmentation dataset[20].Theinput imagesweresimplyscaled downtotherange[0,1.7],withoutevensubtracting themean.Wehave considereda2timesovercomplete codewith 128 units.Theparametersαs,αrand thelearning ratewereset to0.4,0.001,and 0.003 respectively.SomeﬁltersarelocalizedGabor-like edgedetectorsin differentpositionsand orientations,otheraremoreglobal,and some encodethemean value(see ﬁg.1-H).6ConclusionsThere aretwostrategiestotrain unsupervisedmachines:1)having a contrastiveterminthelossfunction minimized during training,2)constraining theinternalrepresentation insuchawaythattraining samplescan bebetter reconstructedthan otherpointsininputspace.WehaveshownthatRBM,whichfallsintheﬁrstclass ofmethods,isparticularlyrobusttochannelnoise,it achievesverylowRMSEand good recognition rate.Wehave also proposedanovelsymmetricsparse encodingmethod following thesecond strategy which: isparticularlyefﬁcient totrain,hasfast inference,workswithoutrequiring any withening orevenmeanremovalfromtheinput,can providethebestrecognition performance and trade-off betweenentropy/RMSE,and can be easilyextendedtoahierarchy discovering hiddenstructureinthedata.Wehaveproposedanevaluation protocol tocomparedifferentmachineswhichisbased on RMSE,entropy and,eventually,error ratewhenalsolabelsare available.Interestingly,themachine achieving thebestperformanceinclassiﬁcation isthe7
 onewiththebest trade-off betweenreconstruction errorand entropy.Afuture avenueofworkistounderstand thereasonsforthis“coincidence”,and deeperconnectionsbetweenthesetwostrategies.AcknowledgmentsWewishtothank JonathanGoodman,GeoffreyHinton,and YoshuaBengioforhelpfuldiscussions.Thisworkwas supportedin partby NSF grantIIS-0535166 “towardcategory-levelobjectrecognition”,NSF ITR-0325463“newdirectionsin predictivelearning”,and ONRgrantN00014-07-1-0535 “integration and representation ofhigh dimensionaldata”.References[1]G.E.Hinton and R.RSalakhutdinov.Reducing thedimensionality ofdatawith neuralnetworks.Science,313(5786):504–507,2006.[2]Y.Bengio,P.Lamblin,D.Popovici,and H.Larochelle.Greedy layer-wisetraining ofdeep networks.InNIPS,2006.[3]M.Ranzato,C.Poultney,S.Chopra,and Y.LeCun.Efﬁcient learning ofsparserepresentationswithanenergy-basedmodel.InNIPS 2006.MITPress,2006.[4]Y.Bengioand Y.LeCun.Scaling learning algorithmstowarsai.InD.DeCosteL.Bottou,O.Chapelleand J.Weston,editors,Large-ScaleKernelMachines.MITPress,2007.[5]M.Ranzato,Y.Boureau,S.Chopra,and Y.LeCun.Auniﬁedenergy-basedframeworkforunsupervisedlearning.InProc.Conference on AIand Statistics(AI-Stats),2007.[6]E.Doi,D.C.Balcan,and M.S.Lewicki.Atheoreticalanalysisofrobustcoding overnoisy overcompletechannels.InNIPS.MITPress,2006.[7]Y.W.Teh,M.Welling,S.Osindero,and G.E.Hinton.Energy-basedmodelsforsparseovercompleterepresentations.JournalofMachineLearning Research,4:1235–1260,2003.[8]B.A.Olshausenand D.J.Field.Sparse coding withan overcompletebasis set: astrategy employed byv1?Vision Research,37:3311–3325,1997.[9]D.D.Lee and H.S.Seung.Learning thepartsofobjectsby non-negativematrixfactorization.Nature,401:788–791,1999.[10]G.E.Hinton.Training productsofexpertsby minimizing contrastivedivergence.NeuralComputation,14:1771–1800,2002.[11]P.Lennie.The costofcorticalcomputation.Currentbiology,13:493–497,2003.[12]J.F.Murrayand K.Kreutz-Delgado.Learning sparseovercomplete codesforimages.The JournalofVLSISignalProcessing,45:97–110,2008.[13]G.E.Hinton and R.S.Zemel.Autoencoders,minimumdescription length,and helmholtzfree energy.InNIPS,1994.[14]G.E.Hinton,P.Dayan,and M.Revow.Modeling themanifoldsofimagesofhandwritten digits.IEEETransactionson NeuralNetworks,8:65–74,1997.[15]M.S.Lewickiand T.J.Sejnowski.Learning overcompleterepresentations.NeuralComputation,12:337–365,2000.[16]Y.LeCun,S.Chopra,R.Hadsell,M.Ranzato,and F.J.Huang.Atutorialon energy-basedlearning.InG.Bakirand al.., editors,Predicting StructuredData.MITPress,2006.[17]P.Sallee and B.A.Olshausen.Learning sparsemultiscaleimagerepresentations.InNIPS.MITPress,2002.[18]http://yann.lecun.com/exdb/mnist/.[19]G.E.Hinton,S.Osindero,and Y.-W.Teh.Afast learning algorithmfordeep beliefnets.NeuralCompu-tation,18:1527–1554,2006.[20]http://www.cs.berkeley.edu/projects/vision/grouping/segbench/.8
 CitationsCitations207ReferencesReferences25To further study the performance of our approach, we compare SSA-AE with the following popular dimension reduction methods, including subspace learning: PCA [Turk andStacked autoencoders number SSA-AEPentland,1991], NPE[He et al., 2005], KPCA[Schlkopf et al., 1998]; dictionary learning: SCC[Cai et al., 2011]; nonnegative matrix factorization: SDNMF[Qian et al., 2016]. In addition, three variants of the conventional autoencoder including Sparse-AE[Boureau et al., 2008], Denoise-AE[Vincent et al., 2010]and Graph-AE[Yu et al., 2013]are added. The experiments with different numbers of used objects (20, 40, 60, 80 and 100) are conducted to evaluate the scalability of our method. Stacked Similarity-Aware Autoencoders[Show abstract] [Hide abstract] ABSTRACT: As one of the most popular unsupervised learning approaches, the autoencoder aims at transforming the inputs to the outputs with the least discrepancy. The conventional autoencoder and most of its variants only consider the one-to-one reconstruction, which ignores the intrinsic structure of the data and may lead to overfitting. In order to preserve the latent geometric information in the data, we propose the stacked similarity-aware autoencoders. To train each single autoencoder, we first obtain the pseudo class label of each sample by clustering the input features. Then the hidden codes of those samples sharing the same category label will be required to satisfy an additional similarity constraint. Specifically, the similarity constraint is implemented based on an extension of the recently proposed center loss. With this joint supervision of the autoencoder reconstruction error and the center loss, the learned feature representations not only can reconstruct the original data, but also preserve the geometric structure of the data. Furthermore, a stacked framework is introduced to boost the representation capacity. The experimental results on several benchmark datasets show the remarkable performance improvement of the proposed algorithm compared with other autoencoder based approaches.Conference Paper · Aug 2017 Wenqing ChuDeng CaiReadState-of-the-art approaches to visual recognition tasks are mostly based on learning techniques. Some use mid-level representations[Singh et al., 2012;Lobel et al., 2013], others deep hierarchical layers of composable features[Ranzato et al., 2008;Krizhevsky et al., 2012]. Their goal is to uncover visual spaces where visual similarities carry enough information to achieve robust visual recognition. How a General-Purpose Commonsense Ontology can Improve Performance of Learning-Based Image Retrieval[Show abstract] [Hide abstract] ABSTRACT: The knowledge representation community has built general-purpose ontologies which contain large amounts of commonsense knowledge over relevant aspects of the world, including useful visual information, e.g.: "a ball is used by a football player", "a tennis player is located at a tennis court". Current state-of-the-art approaches for visual recognition do not exploit these rule-based knowledge sources. Instead, they learn recognition models directly from training examples. In this paper, we study how general-purpose ontologies---specifically, MITs ConceptNet ontology---can improve the performance of state-of-the-art vision systems. As a testbed, we tackle the problem of sentence-based image retrieval. Our retrieval approach incorporates knowledge from ConceptNet on top of a large pool of object detectors derived from a deep learning technique. In our experiments, we show that ConceptNet can improve performance on a common benchmark dataset. Key to our performance is the use of the ESPGAME dataset to select visually relevant relations from ConceptNet. Consequently, a main conclusion of this work is that general-purpose commonsense ontologies improve performance on visual reasoning tasks when properly filtered to select meaningful visual relations.Article · May 2017 Rodrigo Toro IcarteJorge A. BaierCristian RuzAlvaro SotoAlvaro SotoReadIn short, we can learn more information at low temperatures, thus obtaining better results (obviously, we are constrained to a minimum bound concerning the temperature). According to Ranzato et al. [20], sparsity in the neurons activity favours the power of generalization of a network, which is somehow related to dropping neurons out in order to avoid overfitting [27]. We have observed the following statement: the lower the temperature, the higher the probability of turning " on " hidden units (Equation 22), which forces DBM to push down the weights (W) looking at sparsity. Temperature-Based Deep Boltzmann Machines[Show abstract] [Hide abstract] ABSTRACT: Deep learning techniques have been paramount in the last years, mainly due to their outstanding results in a number of applications, that range from speech recognition to face-based user identification. Despite other techniques employed for such purposes, Deep Boltzmann Machines are among the most used ones, which are composed of layers of Restricted Boltzmann Machines (RBMs) stacked on top of each other. In this work, we evaluate the concept of temperature in DBMs, which play a key role in Boltzmann-related distributions, but it has never been considered in this context up to date. Therefore, the main contribution of this paper is to take into account this information and to evaluate its influence in DBMs considering the task of binary image reconstruction. We expect this work can foster future research considering the usage of different temperatures during learning in DBMs. Full-text · Article · Aug 2016 Leandro Aparecido Passos JúniorJoão Paulo PapaRead full-textFor neural network research, Fukushima and Miyake [25] proposed the influential neocognitron with aspects of pooling. Average pooling is attributed to LeCun et al. [26], and max pooling was proposed by Ranzato et al. [27]. Lazebnik et al. [28] were the first to use pooling over pyramid structures, which profiled (counted) features over various rectangular divisions of an image. Positional Binding with Distributed Representations[Show abstract] [Hide abstract] ABSTRACT: Positional binding specifies feature positions for an image (or for text). We show how to incorporate position into a fully distributed vector formed from Vector Quantization, or add position to a vector formed from a Vector Symbolic Architecture. The method guarantees that small shifts in position result in small changes to the representation vector, and does not require an increase in vector size. The incorporation of positional binding improves performance on CIFAR-10 and on a new database of noisy abstract face images, which we hereby make public. For Deep Learning approaches, we emphasize the importance of positional binding, and this sheds light on why multiple layers and pooling are beneficial. Full-text · Conference Paper · Aug 2016 Steve GallantPhil CullitonRead full-textTraining deep CNNs requires a large number of labeled training data, which is not always available. Several studies [45, 46, 47, 48] showed that it is possible to use unsupervised learning to train each layer individually, one after another, starting from the first layer. This unsupervised pre-training could be used to initialize CNN weights, reducing significantly the required training data, and improving the overall performance of the trained CNN. Using mid- and high-level visual features for surgical workflow detection in cholecystectomy procedures[Show abstract] [Hide abstract] ABSTRACT: We present a method that uses visual information from the video of laparoscopic cholecystectomy
procedure to detect the surgical workflow. This task aims at recognizing the
corresponding surgical phase for each frame of the laparoscopic video. In our method, we
fine-tune a Convolutional Neural Network (CNN) and use it to extract mid-level features
representing the surgical phases. Additionally, we train object detectors based on Deformable
Parts Models (DPM) to detect the used surgical tools, then we utilize this information to
provide discriminative high-level features. We present a pipeline that employs these mid- and
high- level features to infer the surgical workflow. Our method uses one-vs-all Support
Vector Machines (SVM) trained on the mid-level features to do initial assignment of phases’
probabilities to each video frame. Afterwards, we concatenate the inferred phases’ probabilities
with the high-level features and feed these signals as observations for a Hierarchical
Hidden Markov Model (HHMM). We use the HHMM to enforce the temporal constraints of
Our major contribution is the set of visual features we use in our method. Most related
work relies on rich external information regarding surgical tools usage. This information is
generated using manual labeling or captured using additional equipment that are not available
in common laparoscopic cholecystectomy procedures. On the contrary, our method relies
only on visual features extracted from the laparoscopic video that is a basic component of all
using a deep CNN in the task of detecting the surgical workflow. As far as we know, this
is the first time that deep learning is used in this task. Using a deep CNN provides rich
representations of the visual information inherent in the laparoscopic video, which helps in
Furthermore, we present detailed experiments on a relatively large dataset, called Cholec80
dataset, which contains 80 laparoscopic cholecystectomy videos recorded and labeled at
Strasbourg University. This dataset is 4-folds larger than the datasets used in previous studies.
Our best approach, using only visual information, reaches state-of-the-art results on the
Cholec80 dataset. Our approach achieves 90% detection accuracy in onine mode, where we
process the full surgery video to infer the surgical workflow. As for the case of online mode,
where video frames are processed without knowledge of future frames, our approach reaches
80% detection accuracy. Full-text · Thesis · Jul 2016 Sherif ShehataRead full-text(DCT, wavelet, or Gabors) [20], but the recent interest in sparse coding revolves around the ability to learn an appropriate dictionary matrix from data [21]. Dictionary learning for image feature extraction has recently led to state-of-the-art results in image denoising [19] and image classification (see [3, 7, 17, 23] to name a few). Learning the dictionary allows us to adapt D to the statistics of the input signal, but it also allows us to enforce task-dependent constraints [18, 17, 16, 3]. What is the Best Feature Learning Procedure in Hierarchical Recognition Architectures?[Show abstract] [Hide abstract] ABSTRACT: (This paper was written in November 2011 and never published. It is posted on arXiv.org in its original form in June 2016). Many recent object recognition systems have proposed using a two phase training procedure to learn sparse convolutional feature hierarchies: unsupervised pre-training followed by supervised fine-tuning. Recent results suggest that these methods provide little improvement over purely supervised systems when the appropriate nonlinearities are included. This paper presents an empirical exploration of the space of learning procedures for sparse convolutional networks to assess which method produces the best performance. In our study, we introduce an augmentation of the Predictive Sparse Decomposition method that includes a discriminative term (DPSD). We also introduce a new single phase supervised learning procedure that places an L1 penalty on the output state of each layer of the network. This forces the network to produce sparse codes without the expensive pre-training phase. Using DPSD with a new, complex predictor that incorporates lateral inhibition, combined with multi-scale feature pooling, and supervised refinement, the system achieves a 70.6\% recognition rate on Caltech-101. With the addition of convolutional training, a 77\% recognition was obtained on the CIfAR-10 dataset. Full-text · Article · Jun 2016 Kevin JarrettKoray KvukcuogluKarol GregorYann LecunYann LecunRead full-textShow moreRecommendationsDiscover more publications, questions and projects in Deep Belief NetworkProjectMoDeepArjun JainJonathan TompsonYann LecunView projectConference PaperA Unified Energy-Based Framework for Unsupervised LearningJanuary 2007We introduce a view of unsupervised learning that integrates probabilistic and nonprobabilistic methods for clustering, dimensionality reduction, and feature extraction in a unified framework. In this framework, an energy function associates low energies to input points that are similar to training samples, and high energies to unobserved points. Learning consists in minimizing the energies of... [Show full abstract]Read moreConference PaperEfficient Learning of Sparse Representations with an Energy-Based ModelJanuary 2006Abstract We describe a novel unsupervised method for learning sparse, overcomplete features. The model uses a linear encoder, and a linear decoder p receded by a sparsifying non-linearity that turns a code vector into a quasibinary sparse code vector. Given an input, the optimal code minimizes the distance between the output of the decoder and the input patch while being as similar as possible... [Show full abstract]Read moreConference PaperEnergy-Based Models in Document Recognition and Computer VisionSeptember 2007The machine learning and pattern recognition communities are facing two challenges: solving the normalization problem, and solving the deep learning problem. The normalization problem is related to the difficulty of training probabilistic models over large spaces while keeping them properly normalized. In recent years, the ML and natural language communities have devoted considerable efforts... [Show full abstract]Read moreConference PaperA Sparse and Locally Shift Invariant Feature Extractor Applied to Document ImagesOctober 2007We describe an unsupervised learning algorithm for extracting sparse and locally shift-invariant features. We also devise a principled procedure for learning hierarchies of invariant features. Each feature detector is composed of a set of trainable convolutional filters followed by a max-pooling layer over non-overlapping windows, and a point-wise sigmoid non-linearity. A second stage of more... [Show full abstract]Read moreConference PaperSemi-supervised learning of compact document representations with deep networksJanuary 2008Finding good representations of text docu- ments is crucial in information retrieval and classification systems. Today the most pop- ular document representation is based on a vector of word counts in the document. This representation neither captures dependencies between related words, nor handles synonyms or polysemous words. In this paper, we pro- pose an algorithm to learn text document... [Show full abstract]Read moreDiscover moreData provided are for informational purposes only. Although carefully collected, accuracy cannot be guaranteed. Publisher conditions are provided by RoMEO. Differing provisions from the publishers actual policy or licence agreement may be applicable.This publication is from a journal that may support self archiving.Learn more 
