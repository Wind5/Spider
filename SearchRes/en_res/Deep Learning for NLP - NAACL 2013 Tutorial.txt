Machine learning is everywhere in today's NLP, but by and large
 learning is to explore how computers can take advantage of data to
 shown to perform very well on various NLP tasks such as language
 of these techniques is that they can perform well without any
 not familiar with these methods. Our focus is on insight and
 derivations. The goal of the tutorial is to make the inner workings
 first part of the tutorial presents the basics of neural networks,
 neural word vectors, several simple models based on local windows and
 the math and algorithms of training via backpropagation. In this
 the second section we present recursive neural networks which can
 learn structured tree outputs as well as vector representations for
 phrases and sentences. We cover both equations as well as
 applications. We show how training can be achieved by a modified
 detection. We also draw connections to recent work in semantic
 compositionality in vector spaces. The principle goal, again, is to
 mathematically confusing. By this point in the tutorial, the audience
 members should have a clear understanding of how to build a deep
 last part of the tutorial gives a general overview of the different
 applications of deep learning in NLP, including bag of words
 models. We will provide a discussion of NLP-oriented issues in
