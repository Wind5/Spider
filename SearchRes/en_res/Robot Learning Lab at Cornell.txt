The basic skills of perception, planning and language understanding are critical for robots to perform tasks in the human environments. For example, an industrial robot needs to detect objects to be manipulated, plan its motions and communicate with the human operator. A self-driving robot needs to detect objects on the road, plan where to drive and also communicate with the passenger.
In our lab, instead of manually "programming" our robots, we take a machine learning approach where we use variety of data and learning methods to train our robots. Our robots learn from watching (3D) images on the Internet, from observing people via RGB-D cameras, from observing users playing video games, and from humans giving feedback to the robot.
Large-scale Knowledge-Engine for Robots. It learns concepts by searching the Internet. 
It can interpret natural language text, images, and videos. It watches humans and learn things from interacting with them.
Using deep learning, robot learns to transfer manipulation trajectories to novel objects
taking into account ambiguity in the language and variations in the environment. Learn from users playing video games!
Robot learns context-driven user preferences on motion plans via sub-optimal feedback
in Co-Active Learning setting. Learn from online rating feedback, and interactive feedback on the robots.
Anticipate the activities a human will do next (and how!) to enable robots to plan ahead for reactive responses. Learn by watching people!
Segment and Detect objects (and their attributes) in a 3D Scene by reasoning
about their shape, appearance, and geometric properties, as well as physics-based reasoning.
