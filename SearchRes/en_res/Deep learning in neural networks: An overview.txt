Screen reader users, click here to load entire articleThis page uses JavaScript to progressively load the article content as a user scrolls. Screen reader users, click the load entire article button to bypass dynamically loaded article content.
 JavaScript is disabled on your browser. Please enable JavaScript to use all the features on this page. This page uses JavaScript to progressively load the article content as a user scrolls. Click the View full text link to bypass dynamically loaded article content. View full text
 ReviewDeep learning in neural networks: An overviewJrgen Schmidhuber The Swiss AI Lab IDSIA, Istituto Dalle Molle di Studi sullIntelligenza Artificiale, University of Lugano & SUPSI, Galleria 2, 6928 Manno-Lugano, SwitzerlandReceived 2 May 2014, Revised 12 September 2014, Accepted 14 September 2014, Available online 13 October 2014AbstractIn recent years, deep artificial neural networks (including recurrent ones) have won numerous contests in pattern recognition and machine learning. This historical survey compactly summarizes relevant work, much of it from the previous millennium. Shallow and Deep Learners are distinguished by the depth of their credit assignment paths, which are chains of possibly learnable, causal links between actions and effects. I review deep supervised learning (also recapitulating the history of backpropagation), unsupervised learning, reinforcement learning & evolutionary computation, and indirect search for short programs encoding deep and large networks.KeywordsDeep learning; Supervised learning; Unsupervised learning; Reinforcement learning; Evolutionary computationAbbreviations in alphabetical orderAE:AutoencoderAI:Artificial IntelligenceANN:Artificial Neural NetworkBFGS:BroydenFletcherGoldfarbShannoBNN:Biological Neural NetworkBM:Boltzmann MachineBP:BackpropagationBRNN:Bi-directional Recurrent Neural NetworkCAP:Credit Assignment PathCEC:Constant Error CarouselCFL:Context Free LanguageCMA-ES:Covariance Matrix Estimation ESCNN:Convolutional Neural NetworkCoSyNE:Co-Synaptic Neuro-EvolutionCSL:Context Sensitive LanguageCTC:Connectionist Temporal ClassificationDBN:Deep Belief NetworkDCT:Discrete Cosine TransformDL:Deep LearningDP:Dynamic ProgrammingDS:Direct Policy SearchEA:Evolutionary AlgorithmEM:Expectation MaximizationES:Evolution StrategyFMS:Flat Minimum SearchFNN:Feedforward Neural NetworkFSA:Finite State AutomatonGMDH:Group Method of Data HandlingGOFAI:Good Old-Fashioned AIGP:Genetic ProgrammingGPU:Graphics Processing UnitGPU-MPCNN:GPU-Based MPCNNHMM:Hidden Markov ModelHRL:Hierarchical Reinforcement LearningHTM:Hierarchical Temporal MemoryHMAX:Hierarchical Model and XLSTM:Long Short-Term Memory (RNN)MDL:Minimum Description LengthMDP:Markov Decision ProcessMNIST:Mixed National Institute of Standards and Technology DatabaseMP:Max-PoolingMPCNN:Max-Pooling CNNNE:NeuroEvolutionNEAT:NE of Augmenting TopologiesNES:Natural Evolution StrategiesNFQ:Neural Fitted Q-LearningNN:Neural NetworkOCR:Optical Character RecognitionPCC:Potential Causal ConnectionPDCC:Potential Direct Causal ConnectionPM:Predictability MinimizationPOMDP:Partially Observable MDPRAAM:Recursive Auto-Associative MemoryRBM:Restricted Boltzmann MachineReLU:Rectified Linear UnitRL:Reinforcement LearningRNN:Recurrent Neural NetworkR-prop:Resilient BackpropagationSL:Supervised LearningSLIM NN:Self-Delimiting Neural NetworkSOTA:Self-Organizing Tree AlgorithmSVM:Support Vector MachineTDNN:Time-Delay Neural NetworkTIMIT:TI/SRI/MIT Acoustic-Phonetic Continuous Speech CorpusUL:Unsupervised LearningWTA:Winner-Take-AllPrefaceThis is the preprint of an invited Deep Learning (DL) overview. One of its goals is to assign credit to those who contributed to the present state of the art. I acknowledge the limitations of attempting to achieve this goal. The DL research community itself may be viewed as a continually evolving, deep network of scientists who have influenced each other in complex ways. Starting from recent DL results, I tried to trace back the origins of relevant ideas through the past half century and beyond, sometimes using local search to follow citations of citations backwards in time. Since not all DL publications properly acknowledge earlier relevant work, additional global search strategies were employed, aided by consulting numerous neural network experts. As a result, the present preprint mostly consists of references. Nevertheless, through an expert selection bias I may have missed important work. A related bias was surely introduced by my special familiarity with the work of my own DL research group in the past quarter-century. For these reasons, this work should be viewed as merely a snapshot of an ongoing credit assignment process. To help improve it, please do not hesitate to send corrections and suggestions to juergen@idsia.ch.1. Introduction to Deep Learning (DL) in Neural Networks (NNs)Which modifiable components of a learning system are responsible for its success or failure? What changes to them improve performance? This has been called the fundamental credit assignment problem ( Minsky, 1963). There are general credit assignment methods for universal problem solvers that are time-optimal in various theoretical senses (Section  6.8). The present survey, however, will focus on the narrower, but now commercially important, subfield of Deep Learning (DL) in Artificial Neural Networks (NNs).A standard neural network (NN) consists of many simple, connected processors called neurons, each producing a sequence of real-valued activations. Input neurons get activated through sensors perceiving the environment, other neurons get activated through weighted connections from previously active neurons (details in Section  2). Some neurons may influence the environment by triggering actions. Learning or credit assignment is about finding weights that make the NN exhibit desired behavior, such as driving a car. Depending on the problem and how the neurons are connected, such behavior may require long causal chains of computational stages (Section  3), where each stage transforms (often in a non-linear way) the aggregate activation of the network. Deep Learning is about accurately assigning credit across many such stages.Shallow NN-like models with few such stages have been around for many decades if not centuries (Section  5.1). Models with several successive nonlinear layers of neurons date back at least to the 1960s (Section  5.3) and 1970s (Section  5.5). An efficient gradient descent method for teacher-based Supervised Learning (SL) in discrete, differentiable networks of arbitrary depth called backpropagation (BP) was developed in the 1960s and 1970s, and applied to NNs in 1981 (Section  5.5). BP-based training of deep NNs with many layers, however, had been found to be difficult in practice by the late 1980s (Section  5.6), and had become an explicit research subject by the early 1990s (Section  5.9). DL became practically feasible to some extent through the help of Unsupervised Learning (UL), e.g., Section  5.10 (1991), Section  5.15 (2006). The 1990s and 2000s also saw many improvements of purely supervised DL (Section  5). In the new millennium, deep NNs have finally attracted wide-spread attention, mainly by outperforming alternative machine learning methods such as kernel machines ( Schölkopf et al., 1998; Vapnik, 1995) in numerous important applications. In fact, since 2009, supervised deep NNs have won many official international pattern recognition competitions (e.g., Sections  5.17, 5.19, 5.21; 5.22), achieving the first superhuman visual pattern recognition results in limited domains (Section  5.19, 2011). Deep NNs also have become relevant for the more general field of Reinforcement Learning (RL) where there is no supervising teacher (Section  6).
