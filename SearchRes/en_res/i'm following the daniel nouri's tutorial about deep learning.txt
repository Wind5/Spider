Some of these methods will help us improve our results quite a bit.
I'll assume that you already know a fair bit about neural nets.
That's because we won't talk about much of the background of how
neural nets work; there's a few of good books and videos for that,
like the Neural Networks and Deep Learning online book. Alec Radford's talk
You don't need to type the code and execute it yourself if you just
those who have access to a CUDA-capable GPU and want to run the
I assume you have the CUDA toolkit, Python 2.7.x, numpy, pandas,
This command will start printing out stuff after thirty seconds or so.
The reason it takes a while is that Lasagne uses Theano to do the
and it will generate C code that needs to be compiled before training
can happen. Luckily, we have to pay the price for this overhead only
If you let training run long enough, you'll notice that after about 75
If you have a GPU, you'll want to configure Theano to use it.
You'll want to create a ~/.theanorc file in your home directory
supposed learn to find the correct position (the x and y coordinates)
An example of one of the faces with three keypoints marked.
An interesting twist with the dataset is that for some of the
keypoints we only have about 2,000 labels, while other keypoints have
Let's write some Python code that loads the data from the CSV files
We'll write a function that can load both the training and the test
data. These two datasets differ in that the test data doesn't contain
the target values; it's the goal of the challenge to predict these.
 """Loads data from FTEST if *test* is True, otherwise from FTRAIN.
 Pass a list of *cols* if youre only interested in a subset of the
 # The Image column has pixel values separated by space; convert
 print(df.count()) # prints the number of values for each column
 df = df.dropna() # drop all rows that have missing values in them
 X = np.vstack(df[Image].values) / 255. # scale pixel values to [0, 1]
 if not test: # only FTRAIN has any target columns
 y = (y - 48) / 48 # scale target coordinates to [-1, 1]
 X, y = shuffle(X, y, random_state=42) # shuffle train data
First it's printing a list of all columns in the CSV file along with
the number of available values for each. So while we have an
Image for all rows in the training data, we only have 2,267 values
load() returns a tuple (X, y) where y is the target matrix.
y has shape n x m with n being the number of samples in the
dataset that have all m keypoints. Dropping all rows with missing
df = df.dropna() # drop all rows that have missing values in them
The script's output y.shape == (2140, 30) tells us that there's
only 2,140 images in the dataset that have all 30 target values
leaves us with many more input dimensions (9,216) than samples; an
course it's a bad idea to throw away 70% of the training data just
Another feature of the load() function is that it scales the
intensity values of the image pixels to be in the interval [0, 1],
instead of 0 to 255. The target values (x and y coordinates) are
scaled to [-1, 1]; before they were between 0 to 95.
Now that we're done with the legwork of loading the data, let's use
Lasagne and create a neural net with a single hidden layer. We'll
 regression=True, # flag to indicate were dealing with regression problem
defines the hidden layer's num_units and so on. (It may seem a
little odd that we have to specify the parameters like this, but the
upshot is it buys us better compatibility with scikit-learn's pipeline and parameter search
which is simply max(0, x). It's the most popular choice of
with a cleverly chosen interval. That is, Lasagne figures out this
The update function will update the weights of our network after each
optimization method to do the job. There's a number of other methods
the gradient descent updates to be. We'll talk a bit more about the
The star denotes the global minimum on the error surface. Notice
The last set of parameters declare that we're dealing with a
number of epochs we're willing to train, and that we want to print out
 regression=True, # flag to indicate were dealing with regression problem
Running these two lines will output a table that grows one row per
training epoch. In each row, we'll see the current loss (MSE) on the
train set and on the validation set and the ratio between the two.
training and a validation set, using 20% of the samples for
 Epoch | Train loss | Valid loss | Train / Val
On a reasonably fast GPU, we're able to train for 400 epochs in under
a minute. Notice that the validation loss keeps improving until the
end. (If you let it train longer, it will improve a little more.)
Now how good is a validation loss of 0.0032? How does it compare to
or the other entries in the leaderboard? Remember that we divided the
target coordinates by 48 when we scaled them to be in the interval
what's used in the challenge's leaderboard, based on our MSE loss of
0.003255, we'll take the square root and multiply by 48 again:
This is reasonable proxy for what our score would be on the Kaggle
leaderboard; at the same time it's assuming that the subset of the
data that we chose to train with follows the same distribution as the
test set, which isn't really the case. My guess is that the score is
good enough to earn us a top ten place in the leaderboard at the time
of writing. Certainly not a bad start! (And for those of you that
are crying out right now because of the lack of a proper test set:
The net1 object actually keeps a record of the data that it prints
out in the table. We can access that record through the
We can see that our net overfits, but it's not that bad. In
particular, we don't see a point where the validation error gets worse
that's commonly used to avoid overfitting, would be very useful at
apart from choosing a small number of neurons in the hidden layer, a
How do the net's predictions look like, then? Let's pick a few
 axis.scatter(y[0::2] * 48 + 48, y[1::2] * 48 + 48, marker=x, s=10)
neural nets are at the heart of deep learning's recent breakthrough in
connected layers; they use a few tricks to reduce the number of
weight sharing: weights are shared between a subset of neurons in
When using convolutional layers in Lasagne, we have to prepare the
input data such that each sample is no longer a flat vector of 9,216
1), where c is the number of channels (colors), and 0 and 1
correspond to the x and y dimensions of the input image. In our case,
the concrete shape will be (1, 96, 96), because we're dealing with a
and two fully connected layers. Each conv layer is followed by a 2x2
max-pooling layer. Starting with 32 filters, we double the number of
There's again no regularization in the form of weight decay or
such as our 3x3 and 2x2 filters, is again a pretty good regularizer by
# Training for 1000 epochs will take a while. Well pickle the
# trained model so that we can load it back later:
Training this neural net is much more computationally costly than the
first one we trained. It takes around 15x as long to train; those
1000 epochs take more than 20 minutes on even a powerful GPU.
However, our patience is rewarded with what's already a much better
model than the one we had before. Let's take a look at the output
when running the script. First comes the list of layers with their
output shapes. Note that the first conv layer produces 32 output
images of size (94, 94), that's one 94x94 output image per filter:
This looks pretty good, I like the smoothness of the new error curves.
But we do notice that towards the end, the validation error of net2
flattens out much more quickly than the training error. I bet we
could improve that by using more training examples. What if we
flipped the input images horizontically; would we be able to improve
An overfitting net can generally be made to perform better by using
more training data. (And if your unregularized net does not overfit,
obviously more economic than having to go out and collect more
examples by hand. Augmentation is a very useful tool to have in your
iterator's job to take a matrix of samples, and split it up in
batches, in our case of size 128. While it does the splitting, the
batch iterator can also apply transformations to the data on the fly.
So to produce those horizontal flips, we don't actually have to double
the amount of training data in the input matrix. Rather, we will just
over the data. This is convenient, and for some problems it allows us
to produce an infinite number of examples, without blowing up the
memory usage. Also, transformations to the input images can be done
while the GPU is busy processing a previous batch, so they come at
Flipping the images horizontically is just a matter of using slicing:
X_flipped = X[:, :, :, ::-1] # simple slice to flip all images
In the picture on the right, notice that the target value keypoints
images, we'll have to make sure we also flip the target values. To do
this, not only do we have to flip the coordinates, we'll also have to
left_eye_center_x no longer points to the left eye in our flipped
target vector need to swap places when we flip the image
the number of epochs to train. While each one of our training epochs
will still look at the same number of examples as before (after all,
we haven't changed the size of X), it turns out that training
nevertheless takes quite a bit longer when we use our transforming
generalizes better this time, and it's arguably harder to learn things
So this will take maybe take an hour to train. Let's make sure we
pickle the model at the end of training, and then we're ready to go
Comparing the learning with that of net2, we notice that the error on
the validation set after 3000 epochs is indeed about 5% smaller for
the data augmented net. We can see how net2 stops learning anything
useful after 2000 or so epochs, and gets pretty noisy, while net3
What's annoying about our last model is that it took already an hour
to train it, and it's not exactly inspiring to have to wait for your
experiment's results for so long. In this section, we'll talk about a
combination of two tricks to fix that and make the net train much
decreasing it during the course of training is this: As we start
training, we're far away from the optimum, and we want to take big
steps towards it and learn quickly. But the closer we get to the
optimum, the lighter we want to step. It's like taking the train
home, but when you enter your door you do it by foot, not by train.
is the title of a talk and a paper by Ilya Sutskever et al. It's
there that we learn about another useful trick to boost deep learning:
Remember that in our previous model, we initialized learning rate and
momentum with a static 0.01 and 0.9 respectively. Let's change that
such that the learning rate decreases linearly with the number of
epochs 500 and 1000 is half of what it used to be in net2, before
generalization seems to stop improving after 750 or so epochs already;
 Epoch | Train loss | Valid loss | Train / Val
better results. After 1000 epochs, we're better off than net3 was
augmentation is now about 10% better with regard to validation error
if we have a network that's overfitting, which is clearly the case for
the net5 network that we trained in the previous section. It's
important to remember to get your net to train nicely and overfit
the existing layers and assign dropout probabilities to each one of
them. Here's the complete definition of our new net. I've added a
# ! comment at the end of those lines that were added between this
Also overfitting doesn't seem to be nearly as bad. Though we'll have
to be careful with those numbers: the ratio between training and
validation has a slightly different meaning now since the train error
without dropout. A more comparable value for the train error is
In our previous model without dropout, the error on the train set was
0.000373. So not only does our dropout net perform slightly better,
it overfits much less than what we had before. That's great news,
because it means that we can expect even better performance when we
make the net larger (and more expressive). And that's what we'll try
next: we increase the number of units in the last two hidden layers
Remember those 70% of training data that we threw away in the
beginning? Turns out that's a very bad idea if we want to get a
competitive score in the Kaggle leaderboard. There's quite a bit of
variance in those 70% of data and in the challenge's test set that our
So instead of training a single model, let's train a few specialists,
with each one predicting a different set of target values. We'll
we'll have six models. This will allow us to use the full training
The six specialists are all going to use exactly the same network
training is bound to take much longer now than before, let's think
about a strategy so that we don't have to wait for max_epochs to
You can see that there's two branches inside the __call__: the
first where the current validation score is better than what we've
previously seen, and the second where the best validation epoch was
more than self.patience epochs in the past. In the first case we
We'll make use of this feature when we fit the specialist models in a
 # set number of epochs relative to number of training examples:
 # an option kwargs in the settings list may be used to
and persisting a single model, we do it with a list of models that are
saved in a dictionary that maps columns to the trained NeuralNet
forever to train (though by forever I don't mean Google-forever, I mean
maybe half a day on a single GPU); I don't recommend that you actually
across GPUs, but maybe you don't have the luxury of access to a box
with multiple CUDA GPUs. In the next section we'll talk about another
way to cut down on training time. But let's take a look at the
represent RMSE on the validation set, the dashed lines errors on
the train set. mean is the mean validation error of all nets
weighted by number of target values. All curves have been scaled
score of 2.17 RMSE, which corresponds to the second place at the
In the last section of this tutorial, we'll discuss a way to make
initialize them with the weights that were learned in net6 or
copying weights from one network to another is as simple as using the
method to do just that. I'm again marking the lines that changed
 # an option kwargs in the settings list may be used to
 # if a pretrain model was given, use it to initialize the
 # this time were persisting a dictionary with all models:
It turns out that initializing those nets not at random, but by
re-using weights from one of the networks we learned earlier has in
fact two big advantages: One is that training converges much faster;
maybe four times faster in this case. The second advantage is that it
regularizer. Here's the same learning curves as before, but now for
Finally, the score for this solution on the challenge's leaderboard is
There's probably a dozen ideas that you have that you want to try out.
You can find the source code for the final solution here
to download and play around with. It also includes the bit that
kfkd.py to find out how to use the script on the command-line.
Here's a couple of the more obvious things that you could try out at
specialist networks; this is something that we haven't done so far.
Observe that the six nets that we trained all have different levels of
overfitting. If they're not or hardly overfitting, like for the green
and the yellow net above, you could try to decrease the amount of
purple nets, you could try increasing the amount of dropout. In the
net-specific settings; so say we wanted to add more regularization to
the second net, then we could change the second entry of the list to
And there's a ton of other things that you could try to tweak. Maybe
curious to hear about improvements that you're able to come up with in
where they've included instructions on how to use Amazon GPU instances
to run the tutorial, which is useful if you don't own a CUDA-capable
It's been a while since I started my little adventure of building an
plant species from photos. I'm pretty happy with what the results are
so far, and these days I'm even successfully scratching my own itch
with it. Which is how I came up with the idea in the first place: I
suck at identifying birds, butterflies, and plants, but I want to
learn about them. And knowing an animal or plant's name is really the
So here's a painless way of identifying these species. Say you want
to identify one bird that you see (and you're lucky to have a camera
with a big zoom with you). You take a photo of it and then send it to
own image upload. After a minute or two, you'll get the automatic
reply, with hopefully the right species name. id_birds can with a
There's two other bots that are also available to use for free:
three bots know species occurring natively in Europe only, at least
"But does it actually work?" I hear you say. And the answer is: it
works pretty well, especially if you send good quality photos where
Seeing is believing though, so to prove that it works well, let's try
and identify the ten birds that are in The Wildlife Trust's river
Image credits: Grey heron (c) Amy Lewis / Moorhen and mallard (c)
Steve Waterhouse / Kingfisher (c) Margaret Holland / Mute swan (c)
Neil Aldridge / Dipper and grey wagtail (c) Tom Marshall / Coot (c)
David Longshaw / Great crested grebe (c) Don Sutherland / Tufted
Let's start with the first bird, a grey heron. To the left we have me
addressing the Twitter bot and sending the image. To the right is
(If you're not seeing the tweets below, I suggest you view this on my
@id_birds hey can you ID me this bird from @wildlifetrusts river birds spotter sheet? pic.twitter.com/z5JnE3WIb3 Daniel Nouri (@dnouri) September 13, 2014 @dnouri1. Ardea cinerea http://t.co/gGuu8ombMs2. Anthropoides virgo http://t.co/exqiChamSg id_birds (@id_birds) September 13, 2014 Notice how id_birds will always respond with the two most likely
The next bird on the river bird spotter sheet is the common moorhen
@id_birds how about this one? pic.twitter.com/9csoNaAaJw Daniel Nouri (@dnouri) September 13, 2014 @dnouri1. Anas platyrhynchos http://t.co/9xoDyXJGMJ2. Gallinula chloropus http://t.co/ISQ0N7m2Lu id_birds (@id_birds) September 13, 2014 And already we have id_birds' first mis-classification. It thinks
that this moorhen is a mallard (Anas platyrhynchos). Only the second
guess is correct. Thus so far we're counting one correct, one error.
The next bird we want to identify is incidentally a mallard:
@id_birds pic.twitter.com/x9h89dKkkb id_mushrooms (@id_mushrooms) September 13, 2014 @id_mushrooms1. Anas platyrhynchos http://t.co/9xoDyXJGMJ2. Coracias garrulus http://t.co/MKJTOTfeOv id_birds (@id_birds) September 13, 2014 And the response is correct. Notice how the second guess (Coracias
garrulus) seems a little weird. I guess it's the similarities in
color here. We don't count the second guesses here though, so it's
What about the kingfisher? That's a bird with a pretty unique
appearance among birds native to Europe. So it's probably one of the
@id_birds pic.twitter.com/NNy1t6izLQ id_butterflies (@id_butterflies) September 13, 2014 @id_butterflies1. Alcedo atthis http://t.co/Q1fUKAlYSt2. Hirundo rustica http://t.co/mtVqrMC8oG id_birds (@id_birds) September 13, 2014 And sure enough, id_birds knows what it is. The latin name for the
The fifth bird on the spotter sheet is the mute swan (Cygnus olor).
@id_birds and this? pic.twitter.com/RiFk3pP38B id_butterflies (@id_butterflies) September 13, 2014 @id_butterflies1. Cygnus olor http://t.co/jz7DLdqVW22. Cygnus cygnus http://t.co/wx8MSFv3sy id_birds (@id_birds) September 13, 2014 Yes, the answer is correct. Note that the second guess is also a
swan, namely the whooper swan, quite similar in appearance. A good
So out of the first five birds we got four correct, not bad. But
let's see about the rest of the birds from the spotter sheet.
I'll post the responses to the remaining five birds without further
comment. But do note that of the remaining ones, only the coot
@id_birds I found this one in @wildlifetrusts river birds spotter sheet. Do you know what it is? pic.twitter.com/R9Qf5s2sSf id_wildflowers (@id_wildflowers) September 13, 2014 @id_wildflowers1. Cinclus cinclus http://t.co/u2W2R8NYZX2. Corvus corone http://t.co/1JjU7HtcSj id_birds (@id_birds) September 13, 2014 @id_birds Heres another cute bird from that same spotter sheet. Can you ID it? pic.twitter.com/onxOeALtaL Daniel Nouri (@dnouri) September 13, 2014 @dnouri1. Motacilla cinerea http://t.co/RhZyl5ltwy2. Motacilla flava http://t.co/TWG5YgTxBQ id_birds (@id_birds) September 13, 2014 @id_birds pic.twitter.com/qYVqQGKa4I id_wildflowers (@id_wildflowers) September 13, 2014 @id_wildflowers1. Gallinula chloropus http://t.co/ISQ0N7m2Lu2. Podiceps nigricollis http://t.co/WgflyFJyCU id_birds (@id_birds) September 13, 2014 @id_birds Can you ID this as Podiceps cristatus? pic.twitter.com/jQwS5cjuEi id_wildflowers (@id_wildflowers) September 13, 2014 @id_wildflowers1. Podiceps cristatus http://t.co/JzrmePWuf92. Branta ruficollis http://t.co/X6fCvqF0xo id_birds (@id_birds) September 13, 2014 @id_birds heres the last one: a tufted duck (Aythya fuligula) pic.twitter.com/S8VwHd6rry id_butterflies (@id_butterflies) September 13, 2014 @id_butterflies1. Aythya fuligula http://t.co/n4O4tTIwtv2. Aythya marila http://t.co/H9g0mXy6Eu id_birds (@id_birds) September 13, 2014 So all in all we have two mis-classifications out of ten. That's 8
out of 10 accuracy, which is pretty remarkable when you consider that
Wikipedia, the grey wagtail "looks similar to the yellow wagtail but
has the yellow on its underside restricted to the throat and vent."
Of course ten picture is a tiny test set, but the score roughly
matches what I see with the much larger test set that I use for
I'll write about how the computer vision works under the hood another
We call the stack that we've developed and that's running behind
id_birds and the other bots PhotoID. So far PhotoID has proven to
Let's conclude this post with two other IDs, one from id_butterflies,
#WansteadFlats Its an upside down world sometimes. Maybe a Meadow Brown? @id_butterflies @wildlife_uk pic.twitter.com/yDnkTUTMO8 TheNatureOfTheFlats (@RoseStephensArt) August 22, 2014 @id_butterflies1. Maniola jurtina http://t.co/4YGYG6WQHc2. Cupido minimus http://t.co/2hbLKCYzpF id_butterflies (@id_butterflies) August 22, 2014 Can @id_wildflowers identify this wildflower photographed by @LornaCrowcroft?Wondering what it is? See the reply. pic.twitter.com/WI31e7aN8x id_wildflowers (@id_wildflowers) September 4, 2014 @id_wildflowers1. Solanum nigrum http://t.co/kvMbrC7KB42. Cyclamen hederifolium http://t.co/mcZHVmRecq id_wildflowers (@id_wildflowers) September 4, 2014 Happy ID'ing!
(Plug: Feel free to contact me through my company if you have a challenging image
