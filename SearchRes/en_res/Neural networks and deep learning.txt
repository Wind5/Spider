Neural Networks and Deep LearningWhat this book is aboutOn the exercises and problemsUsing neural nets to recognize handwritten digitsPerceptronsSigmoid neuronsThe architecture of neural networksA simple network to classify handwritten digitsLearning with gradient descentImplementing our network to classify digitsToward deep learning
How the backpropagation algorithm worksWarm up: a fast matrix-based approach to computing the output from a neural networkThe two assumptions we need about the cost functionThe Hadamard product, $s \odot t$The four fundamental equations behind backpropagationProof of the four fundamental equations (optional)The backpropagation algorithmThe code for backpropagationIn what sense is backpropagation a fast algorithm?Backpropagation: the big picture
Improving the way neural networks learnThe cross-entropy cost functionOverfitting and regularizationWeight initializationHandwriting recognition revisited: the codeHow to choose a neural network's hyper-parameters?Other techniques
A visual proof that neural nets can compute any functionTwo caveatsUniversality with one input and one outputMany input variablesExtension beyond sigmoid neuronsFixing up the step functionsConclusion
Why are deep neural networks hard to train?The vanishing gradient problemWhat's causing the vanishing gradient problem? Unstable gradients in deep neural netsUnstable gradients in more complex networksOther obstacles to deep learning
Deep learningIntroducing convolutional networksConvolutional neural networks in practiceThe code for our convolutional networksRecent progress in image recognitionOther approaches to deep neural netsOn the future of neural networks
 If you benefit from the book, please make a small
In the last chapter we saw how neural networks canlearn their weights and biases using the gradient descent algorithm.There was, however, a gap in our explanation: we didn't discuss how tocompute the gradient of the cost function. That's quite a gap! Inthis chapter I'll explain a fast algorithm for computing suchgradients, an algorithm known as backpropagation. The backpropagation algorithm was originally introduced in the 1970s,but its importance wasn't fully appreciated until afamous 1986 paper byDavid Rumelhart,Geoffrey Hinton, andRonald Williams. That paper describes severalneural networks where backpropagation works far faster than earlierapproaches to learning, making it possible to use neural nets to solveproblems which had previously been insoluble. Today, thebackpropagation algorithm is the workhorse of learning in neuralnetworks.This chapter is more mathematically involved than the rest of thebook. If you're not crazy about mathematics you may be tempted toskip the chapter, and to treat backpropagation as a black box whosedetails you're willing to ignore. Why take the time to study thosedetails?The reason, of course, is understanding. At the heart ofbackpropagation is an expression for the partial derivative $\partialC / \partial w$ of the cost function $C$ with respect to any weight$w$ (or bias $b$) in the network. The expression tells us how quicklythe cost changes when we change the weights and biases. And while theexpression is somewhat complex, it also has a beauty to it, with eachelement having a natural, intuitive interpretation. And sobackpropagation isn't just a fast algorithm for learning. It actuallygives us detailed insights into how changing the weights and biaseschanges the overall behaviour of the network. That's well worthstudying in detail.With that said, if you want to skim the chapter, or jump straight tothe next chapter, that's fine. I've written the rest of the book tobe accessible even if you treat backpropagation as a black box. Thereare, of course, points later in the book where I refer back to resultsfrom this chapter. But at those points you should still be able tounderstand the main conclusions, even if you don't follow all thereasoning.Warm up: a fast matrix-based approach to computing the output from a neural networkBefore discussing backpropagation, let's warm up with a fastmatrix-based algorithm to compute the output from a neural network.We actually already briefly saw this algorithmnear the end of the last chapter, but I described it quickly, so it'sworth revisiting in detail. In particular, this is a good way ofgetting comfortable with the notation used in backpropagation, in afamiliar context.Let's begin with a notation which lets us refer to weights in thenetwork in an unambiguous way. We'll use $w^l_{jk}$ to denote theweight for the connection from the $k^{\rm th}$ neuron in the$(l-1)^{\rm th}$ layer to the $j^{\rm th}$ neuron in the $l^{\rm th}$layer. So, for example, the diagram below shows the weight on aconnection from the fourth neuron in the second layer to the secondneuron in the third layer of a network:This notation is cumbersome at first, and it does take some work tomaster. But with a little effort you'll find the notation becomeseasy and natural. One quirk of the notation is the ordering of the$j$ and $k$ indices. You might think that it makes more sense to use$j$ to refer to the input neuron, and $k$ to the output neuron, notvice versa, as is actually done. I'll explain the reason for thisquirk below.We use a similar notation for the network's biases and activations.Explicitly, we use $b^l_j$ for the bias of the $j^{\rm th}$ neuron inthe $l^{\rm th}$ layer. And we use $a^l_j$ for the activation of the$j^{\rm th}$ neuron in the $l^{\rm th}$ layer. The following diagramshows examples of these notations in use:With these notations, the activation $a^{l}_j$ of the $j^{\rm th}$neuron in the $l^{\rm th}$ layer is related to the activations in the$(l-1)^{\rm th}$ layer by the equation (compareEquation (4)\begin{eqnarray} \frac{1}{1+\exp(-um_j w_j x_j-b)} \nonumber\end{eqnarray} and surroundingdiscussion in the last chapter)\begin{eqnarray} a^{l}_j = igma\left( um_k w^{l}_{jk} a^{l-1}_k + b^l_j \right),\tag{23}\end{eqnarray}where the sum is over all neurons $k$ in the $(l-1)^{\rm th}$ layer. Torewrite this expression in a matrix form we define a weight matrix $w^l$ for each layer, $l$. The entries of the weight matrix$w^l$ are just the weights connecting to the $l^{\rm th}$ layer of neurons,that is, the entry in the $j^{\rm th}$ row and $k^{\rm th}$ column is $w^l_{jk}$.Similarly, for each layer $l$ we define a bias vector, $b^l$.You can probably guess how this works - the components of the biasvector are just the values $b^l_j$, one component for each neuron inthe $l^{\rm th}$ layer. And finally, we define an activation vector $a^l$whose components are the activations $a^l_j$.The last ingredient we need to rewrite (23)\begin{eqnarray} a^{l}_j = igma\left( um_k w^{l}_{jk} a^{l-1}_k + b^l_j \right) \nonumber\end{eqnarray} in amatrix form is the idea of vectorizing a function such as $igma$.We met vectorization briefly in the last chapter, but to recap, theidea is that we want to apply a function such as $igma$ to everyelement in a vector $v$. We use the obvious notation $igma(v)$ todenote this kind of elementwise application of a function. That is,the components of $igma(v)$ are just $igma(v)_j = igma(v_j)$.As an example, if we have the function $f(x) = x^2$ then thevectorized form of $f$ has the effect\begin{eqnarray} f\left(\left[ \begin{array}{c} 2 \\ 3 \end{array} \right] \right) = \left[ \begin{array}{c} f(2) \\ f(3) \end{array} \right] = \left[ \begin{array}{c} 4 \\ 9 \end{array} \right],\tag{24}\end{eqnarray}that is, the vectorized $f$ just squares every element of the vector.With these notations in mind, Equation (23)\begin{eqnarray} a^{l}_j = igma\left( um_k w^{l}_{jk} a^{l-1}_k + b^l_j \right) \nonumber\end{eqnarray} canbe rewritten in the beautiful and compact vectorized form\begin{eqnarray} a^{l} = igma(w^l a^{l-1}+b^l).\tag{25}\end{eqnarray}This expression gives us a much more global way of thinking about howthe activations in one layer relate to activations in the previouslayer: we just apply the weight matrix to the activations, then addthe bias vector, and finally apply the $igma$ function**By the way, it's this expression that motivates the quirk in the $w^l_{jk}$ notation mentioned earlier. If we used $j$ to index the input neuron, and $k$ to index the output neuron, then we'd need to replace the weight matrix in Equation (25)\begin{eqnarray} a^{l} = igma(w^l a^{l-1}+b^l) \nonumber\end{eqnarray} by the transpose of the weight matrix. That's a small change, but annoying, and we'd lose the easy simplicity of saying (and thinking) "apply the weight matrix to the activations".. That global view is often easier andmore succinct (and involves fewer indices!) than the neuron-by-neuronview we've taken to now. Think of it as a way of escaping index hell,while remaining precise about what's going on. The expression is alsouseful in practice, because most matrix libraries provide fast ways ofimplementing matrix multiplication, vector addition, andvectorization. Indeed, thecodein the last chapter made implicit use of this expression to computethe behaviour of the network.When using Equation (25)\begin{eqnarray} a^{l} = igma(w^l a^{l-1}+b^l) \nonumber\end{eqnarray} to compute $a^l$,we compute the intermediate quantity $z^l \equiv w^l a^{l-1}+b^l$along the way. This quantity turns out to be useful enough to beworth naming: we call $z^l$ the weighted input to the neuronsin layer $l$. We'll make considerable use of the weighted input $z^l$later in the chapter. Equation (25)\begin{eqnarray} a^{l} = igma(w^l a^{l-1}+b^l) \nonumber\end{eqnarray} issometimes written in terms of the weighted input, as $a^l =igma(z^l)$. It's also worth noting that $z^l$ has components $z^l_j= um_k w^l_{jk} a^{l-1}_k+b^l_j$, that is, $z^l_j$ is just theweighted input to the activation function for neuron $j$ in layer $l$.The two assumptions we need about the cost functionThe goal of backpropagation is to compute the partial derivatives$\partial C / \partial w$ and $\partial C / \partial b$ of the costfunction $C$ with respect to any weight $w$ or bias $b$ in thenetwork. For backpropagation to work we need to make two mainassumptions about the form of the cost function. Before stating thoseassumptions, though, it's useful to have an example cost function inmind. We'll use the quadratic cost function from last chapter(c.f. Equation (6)\begin{eqnarray} C(w,b) \equiv \frac{1}{2n} um_x \| y(x) - a\|^2 \nonumber\end{eqnarray}). In the notation ofthe last section, the quadratic cost has the form\begin{eqnarray} C = \frac{1}{2n} um_x \|y(x)-a^L(x)\|^2,\tag{26}\end{eqnarray}where: $n$ is the total number of training examples; the sum is overindividual training examples, $x$; $y = y(x)$ is the correspondingdesired output; $L$ denotes the number of layers in the network; and$a^L = a^L(x)$ is the vector of activations output from the networkwhen $x$ is input.Okay, so what assumptions do we need to make about our cost function,$C$, in order that backpropagation can be applied? The firstassumption we need is that the cost function can be written as anaverage $C = \frac{1}{n} um_x C_x$ over cost functions $C_x$ forindividual training examples, $x$. This is the case for the quadraticcost function, where the cost for a single training example is $C_x =\frac{1}{2} \|y-a^L \|^2$. This assumption will also hold true forall the other cost functions we'll meet in this book.The reason we need this assumption is because what backpropagationactually lets us do is compute the partial derivatives $\partial C_x/ \partial w$ and $\partial C_x / \partial b$ for a single trainingexample. We then recover $\partial C / \partial w$ and $\partial C/ \partial b$ by averaging over training examples. In fact, with thisassumption in mind, we'll suppose the training example $x$ has beenfixed, and drop the $x$ subscript, writing the cost $C_x$ as $C$.We'll eventually put the $x$ back in, but for now it's a notationalnuisance that is better left implicit.The second assumption we make about the cost is that it can be writtenas a function of the outputs from the neural network:For example, the quadratic cost function satisfies this requirement,since the quadratic cost for a single training example $x$ may bewritten as\begin{eqnarray} C = \frac{1}{2} \|y-a^L\|^2 = \frac{1}{2} um_j (y_j-a^L_j)^2,\tag{27}\end{eqnarray}and thus is a function of the output activations. Of course, thiscost function also depends on the desired output $y$, and you maywonder why we're not regarding the cost also as a function of $y$.Remember, though, that the input training example $x$ is fixed, and sothe output $y$ is also a fixed parameter. In particular, it's notsomething we can modify by changing the weights and biases in any way,i.e., it's not something which the neural network learns. And so itmakes sense to regard $C$ as a function of the output activations$a^L$ alone, with $y$ merely a parameter that helps define thatfunction.The Hadamard product, $s \odot t$The backpropagation algorithm is based on common linear algebraicoperations - things like vector addition, multiplying a vector by amatrix, and so on. But one of the operations is a little lesscommonly used. In particular, suppose $s$ and $t$ are two vectors ofthe same dimension. Then we use $s \odot t$ to denote theelementwise product of the two vectors. Thus the components of$s \odot t$ are just $(s \odot t)_j = s_j t_j$. As an example,\begin{eqnarray}\left[\begin{array}{c} 1 \\ 2 \end{array}\right] \odot \left[\begin{array}{c} 3 \\ 4\end{array} \right]= \left[ \begin{array}{c} 1 * 3 \\ 2 * 4 \end{array} \right]= \left[ \begin{array}{c} 3 \\ 8 \end{array} \right].\tag{28}\end{eqnarray}This kind of elementwise multiplication is sometimes called theHadamard product or Schur product. We'll refer to it asthe Hadamard product. Good matrix libraries usually provide fastimplementations of the Hadamard product, and that comes in handy whenimplementing backpropagation.The four fundamental equations behind backpropagationBackpropagation is about understanding how changing the weights andbiases in a network changes the cost function. Ultimately, this meanscomputing the partial derivatives $\partial C / \partial w^l_{jk}$ and$\partial C / \partial b^l_j$. But to compute those, we firstintroduce an intermediate quantity, $\delta^l_j$, which we call theerror in the $j^{\rm th}$ neuron in the $l^{\rm th}$ layer.Backpropagation will give us a procedure to compute the error$\delta^l_j$, and then will relate $\delta^l_j$ to $\partial C/ \partial w^l_{jk}$ and $\partial C / \partial b^l_j$.To understand how the error is defined, imagine there is a demon inour neural network:The demon sits at the $j^{\rm th}$ neuron in layer $l$. As the input to theneuron comes in, the demon messes with the neuron's operation. Itadds a little change $\Delta z^l_j$ to the neuron's weighted input, sothat instead of outputting $igma(z^l_j)$, the neuron instead outputs$igma(z^l_j+\Delta z^l_j)$. This change propagates through laterlayers in the network, finally causing the overall cost to change byan amount $\frac{\partial C}{\partial z^l_j} \Delta z^l_j$.Now, this demon is a good demon, and is trying to help you improve thecost, i.e., they're trying to find a $\Delta z^l_j$ which makes thecost smaller. Suppose $\frac{\partial C}{\partial z^l_j}$ has a largevalue (either positive or negative). Then the demon can lower thecost quite a bit by choosing $\Delta z^l_j$ to have the opposite signto $\frac{\partial C}{\partial z^l_j}$. By contrast, if$\frac{\partial C}{\partial z^l_j}$ is close to zero, then the demoncan't improve the cost much at all by perturbing the weighted input$z^l_j$. So far as the demon can tell, the neuron is already prettynear optimal**This is only the case for small changes $\Delta z^l_j$, of course. We'll assume that the demon is constrained to make such small changes.. And so there's a heuristic sense inwhich $\frac{\partial C}{\partial z^l_j}$ is a measure of the error inthe neuron.Motivated by this story, we define the error $\delta^l_j$ of neuron$j$ in layer $l$ by\begin{eqnarray} \delta^l_j \equiv \frac{\partial C}{\partial z^l_j}.\tag{29}\end{eqnarray}As per our usual conventions, we use $\delta^l$ to denote the vectorof errors associated with layer $l$. Backpropagation will give us away of computing $\delta^l$ for every layer, and then relating thoseerrors to the quantities of real interest, $\partial C / \partialw^l_{jk}$ and $\partial C / \partial b^l_j$.You might wonder why the demon is changing the weighted input $z^l_j$.Surely it'd be more natural to imagine the demon changing the outputactivation $a^l_j$, with the result that we'd be using $\frac{\partial C}{\partial a^l_j}$ as our measure of error. In fact, if you dothis things work out quite similarly to the discussion below. But itturns out to make the presentation of backpropagation a little morealgebraically complicated. So we'll stick with $\delta^l_j =\frac{\partial C}{\partial z^l_j}$ as our measure of error**In classification problems like MNIST the term "error" is sometimes used to mean the classification failure rate. E.g., if the neural net correctly classifies 96.0 percent of the digits, then the error is 4.0 percent. Obviously, this has quite a different meaning from our $\delta$ vectors. In practice, you shouldn't have trouble telling which meaning is intended in any given usage..Plan of attack: Backpropagation is based around fourfundamental equations. Together, those equations give us a way ofcomputing both the error $\delta^l$ and the gradient of the costfunction. I state the four equations below. Be warned, though: youshouldn't expect to instantaneously assimilate the equations. Such anexpectation will lead to disappointment. In fact, the backpropagationequations are so rich that understanding them well requiresconsiderable time and patience as you gradually delve deeper into theequations. The good news is that such patience is repaid many timesover. And so the discussion in this section is merely a beginning,helping you on the way to a thorough understanding of the equations.Here's a preview of the ways we'll delve more deeply into theequations later in the chapter: I'llgive a short proof of the equations, which helps explain why they aretrue; we'll restate the equations in algorithmic form as pseudocode, andsee how thepseudocode can be implemented as real, running Python code; and, inthe final section of the chapter, we'll develop an intuitive picture of whatthe backpropagation equations mean, and how someone might discoverthem from scratch. Along the way we'll return repeatedly to the fourfundamental equations, and as you deepen your understanding thoseequations will come to seem comfortable and, perhaps, even beautifuland natural.An equation for the error in the output layer, $\delta^L$:The components of $\delta^L$ are given by\begin{eqnarray} \delta^L_j = \frac{\partial C}{\partial a^L_j} igma'(z^L_j).\tag{BP1}\end{eqnarray}This is a very natural expression. The first term on the right,$\partial C / \partial a^L_j$, just measures how fast the cost ischanging as a function of the $j^{\rm th}$ output activation. If, forexample, $C$ doesn't depend much on a particular output neuron, $j$,then $\delta^L_j$ will be small, which is what we'd expect. Thesecond term on the right, $igma'(z^L_j)$, measures how fast theactivation function $igma$ is changing at $z^L_j$.Notice that everything in (BP1)\begin{eqnarray} \delta^L_j = \frac{\partial C}{\partial a^L_j} igma'(z^L_j) \nonumber\end{eqnarray} is easily computed. Inparticular, we compute $z^L_j$ while computing the behaviour of thenetwork, and it's only a small additional overhead to compute$igma'(z^L_j)$. The exact form of $\partial C / \partial a^L_j$will, of course, depend on the form of the cost function. However,provided the cost function is known there should be little troublecomputing $\partial C / \partial a^L_j$. For example, if we're usingthe quadratic cost function then $C = \frac{1}{2} um_j(y_j-a^L_j)^2$, and so $\partial C / \partial a^L_j = (a_j^L-y_j)$,which obviously is easily computable.Equation (BP1)\begin{eqnarray} \delta^L_j = \frac{\partial C}{\partial a^L_j} igma'(z^L_j) \nonumber\end{eqnarray} is a componentwise expression for $\delta^L$.It's a perfectly good expression, but not the matrix-based form wewant for backpropagation. However, it's easy to rewrite the equationin a matrix-based form, as\begin{eqnarray} \delta^L = \nabla_a C \odot igma'(z^L).\tag{BP1a}\end{eqnarray}Here, $\nabla_a C$ is defined to be a vector whose components are thepartial derivatives $\partial C / \partial a^L_j$. You can think of$\nabla_a C$ as expressing the rate of change of $C$ with respect tothe output activations. It's easy to see that Equations (BP1a)\begin{eqnarray} \delta^L = \nabla_a C \odot igma'(z^L) \nonumber\end{eqnarray}and (BP1)\begin{eqnarray} \delta^L_j = \frac{\partial C}{\partial a^L_j} igma'(z^L_j) \nonumber\end{eqnarray} are equivalent, and for that reason from now on we'lluse (BP1)\begin{eqnarray} \delta^L_j = \frac{\partial C}{\partial a^L_j} igma'(z^L_j) \nonumber\end{eqnarray} interchangeably to refer to both equations. As anexample, in the case of the quadratic cost we have $\nabla_a C =(a^L-y)$, and so the fully matrix-based form of (BP1)\begin{eqnarray} \delta^L_j = \frac{\partial C}{\partial a^L_j} igma'(z^L_j) \nonumber\end{eqnarray} becomes\begin{eqnarray} \delta^L = (a^L-y) \odot igma'(z^L).\tag{30}\end{eqnarray}As you can see, everything in this expression has a nice vector form,and is easily computed using a library such as Numpy.An equation for the error $\delta^l$ in terms of the error in the next layer, $\delta^{l+1}$: In particular\begin{eqnarray} \delta^l = ((w^{l+1})^T \delta^{l+1}) \odot igma'(z^l),\tag{BP2}\end{eqnarray}where $(w^{l+1})^T$ is the transpose of the weight matrix $w^{l+1}$ forthe $(l+1)^{\rm th}$ layer. This equation appears complicated, buteach element has a nice interpretation. Suppose we know the error$\delta^{l+1}$ at the $l+1^{\rm th}$ layer. When we apply thetranspose weight matrix, $(w^{l+1})^T$, we can think intuitively ofthis as moving the error backward through the network, givingus some sort of measure of the error at the output of the $l^{\rm th}$layer. We then take the Hadamard product $\odot igma'(z^l)$. Thismoves the error backward through the activation function in layer $l$,giving us the error $\delta^l$ in the weighted input to layer $l$.By combining (BP2)\begin{eqnarray} \delta^l = ((w^{l+1})^T \delta^{l+1}) \odot igma'(z^l) \nonumber\end{eqnarray} with (BP1)\begin{eqnarray} \delta^L_j = \frac{\partial C}{\partial a^L_j} igma'(z^L_j) \nonumber\end{eqnarray} we can compute the error$\delta^l$ for any layer in the network. We start byusing (BP1)\begin{eqnarray} \delta^L_j = \frac{\partial C}{\partial a^L_j} igma'(z^L_j) \nonumber\end{eqnarray} to compute $\delta^L$, then applyEquation (BP2)\begin{eqnarray} \delta^l = ((w^{l+1})^T \delta^{l+1}) \odot igma'(z^l) \nonumber\end{eqnarray} to compute $\delta^{L-1}$, thenEquation (BP2)\begin{eqnarray} \delta^l = ((w^{l+1})^T \delta^{l+1}) \odot igma'(z^l) \nonumber\end{eqnarray} again to compute $\delta^{L-2}$, and so on, allthe way back through the network.An equation for the rate of change of the cost with respect to any bias in the network: In particular:\begin{eqnarray} \frac{\partial C}{\partial b^l_j} = \delta^l_j.\tag{BP3}\end{eqnarray}That is, the error $\delta^l_j$ is exactly equal to the rate ofchange $\partial C / \partial b^l_j$. This is great news, since(BP1)\begin{eqnarray} \delta^L_j = \frac{\partial C}{\partial a^L_j} igma'(z^L_j) \nonumber\end{eqnarray} and (BP2)\begin{eqnarray} \delta^l = ((w^{l+1})^T \delta^{l+1}) \odot igma'(z^l) \nonumber\end{eqnarray} have already told us how to compute$\delta^l_j$. We can rewrite (BP3)\begin{eqnarray} \frac{\partial C}{\partial b^l_j} = \delta^l_j \nonumber\end{eqnarray} in shorthand as\begin{eqnarray} \frac{\partial C}{\partial b} = \delta,\tag{31}\end{eqnarray}where it is understood that $\delta$ is being evaluated at the sameneuron as the bias $b$.An equation for the rate of change of the cost with respect to any weight in the network: In particular:\begin{eqnarray} \frac{\partial C}{\partial w^l_{jk}} = a^{l-1}_k \delta^l_j.\tag{BP4}\end{eqnarray}This tells us how to compute the partial derivatives $\partial C/ \partial w^l_{jk}$ in terms of the quantities $\delta^l$ and$a^{l-1}$, which we already know how to compute. The equation can berewritten in a less index-heavy notation as\begin{eqnarray} \frac{\partial C}{\partial w} = a_{\rm in} \delta_{\rm out},\tag{32}\end{eqnarray}where it's understood that $a_{\rm in}$ is the activation of theneuron input to the weight $w$, and $\delta_{\rm out}$ is the error ofthe neuron output from the weight $w$. Zooming in to look at just theweight $w$, and the two neurons connected by that weight, we candepict this as:A nice consequence of Equation (32)\begin{eqnarray} \frac{\partial C}{\partial w} = a_{\rm in} \delta_{\rm out} \nonumber\end{eqnarray} isthat when the activation $a_{\rm in}$ is small, $a_{\rm in} \approx0$, the gradient term $\partial C / \partial w$ will also tend to besmall. In this case, we'll say the weight learns slowly,meaning that it's not changing much during gradient descent. In otherwords, one consequence of (BP4)\begin{eqnarray} \frac{\partial C}{\partial w^l_{jk}} = a^{l-1}_k \delta^l_j \nonumber\end{eqnarray} is that weights output fromlow-activation neurons learn slowly.There are other insights along these lines which can be obtainedfrom (BP1)\begin{eqnarray} \delta^L_j = \frac{\partial C}{\partial a^L_j} igma'(z^L_j) \nonumber\end{eqnarray}-(BP4)\begin{eqnarray} \frac{\partial C}{\partial w^l_{jk}} = a^{l-1}_k \delta^l_j \nonumber\end{eqnarray}. Let's start by looking at the outputlayer. Consider the term $igma'(z^L_j)$ in (BP1)\begin{eqnarray} \delta^L_j = \frac{\partial C}{\partial a^L_j} igma'(z^L_j) \nonumber\end{eqnarray}. Recallfrom the graph of the sigmoid function in the last chapter that the $igma$ function becomesvery flat when $igma(z^L_j)$ is approximately $0$ or $1$. When thisoccurs we will have $igma'(z^L_j) \approx 0$. And so the lesson isthat a weight in the final layer will learn slowly if the outputneuron is either low activation ($\approx 0$) or high activation($\approx 1$). In this case it's common to say the output neuron hassaturated and, as a result, the weight has stopped learning (oris learning slowly). Similar remarks hold also for the biases ofoutput neuron.We can obtain similar insights for earlier layers. In particular,note the $igma'(z^l)$ term in (BP2)\begin{eqnarray} \delta^l = ((w^{l+1})^T \delta^{l+1}) \odot igma'(z^l) \nonumber\end{eqnarray}. This means that$\delta^l_j$ is likely to get small if the neuron is near saturation.And this, in turn, means that any weights input to a saturated neuronwill learn slowly**This reasoning won't hold if ${w^{l+1}}^T \delta^{l+1}$ has large enough entries to compensate for the smallness of $igma'(z^l_j)$. But I'm speaking of the general tendency..Summing up, we've learnt that a weight will learn slowly if either theinput neuron is low-activation, or if the output neuron has saturated,i.e., is either high- or low-activation. None of these observations is too greatly surprising. Still, theyhelp improve our mental model of what's going on as a neural networklearns. Furthermore, we can turn this type of reasoning around. Thefour fundamental equations turn out to hold for any activationfunction, not just the standard sigmoid function (that's because, aswe'll see in a moment, the proofs don't use any special properties of$igma$). And so we can use these equations to designactivation functions which have particular desired learningproperties. As an example to give you the idea, suppose we were tochoose a (non-sigmoid) activation function $igma$ so that $igma'$is always positive, and never gets close to zero. That would preventthe slow-down of learning that occurs when ordinary sigmoid neuronssaturate. Later in the book we'll see examples where this kind ofmodification is made to the activation function. Keeping the fourequations (BP1)\begin{eqnarray} \delta^L_j = \frac{\partial C}{\partial a^L_j} igma'(z^L_j) \nonumber\end{eqnarray}-(BP4)\begin{eqnarray} \frac{\partial C}{\partial w^l_{jk}} = a^{l-1}_k \delta^l_j \nonumber\end{eqnarray} in mind can help explain why suchmodifications are tried, and what impact they can have.ProblemAlternate presentation of the equations of backpropagation: I've stated the equations of backpropagation (notably (BP1)\begin{eqnarray} \delta^L_j = \frac{\partial C}{\partial a^L_j} igma'(z^L_j) \nonumber\end{eqnarray} and (BP2)\begin{eqnarray} \delta^l = ((w^{l+1})^T \delta^{l+1}) \odot igma'(z^l) \nonumber\end{eqnarray}) using the Hadamard product. This presentation may be disconcerting if you're unused to the Hadamard product. There's an alternative approach, based on conventional matrix multiplication, which some readers may find enlightening. (1) Show that (BP1)\begin{eqnarray} \delta^L_j = \frac{\partial C}{\partial a^L_j} igma'(z^L_j) \nonumber\end{eqnarray} may be rewritten as \begin{eqnarray} \delta^L = \Sigma'(z^L) \nabla_a C, \tag{33}\end{eqnarray} where $\Sigma'(z^L)$ is a square matrix whose diagonal entries are the values $igma'(z^L_j)$, and whose off-diagonal entries are zero. Note that this matrix acts on $\nabla_a C$ by conventional matrix multiplication. (2) Show that (BP2)\begin{eqnarray} \delta^l = ((w^{l+1})^T \delta^{l+1}) \odot igma'(z^l) \nonumber\end{eqnarray} may be rewritten as \begin{eqnarray} \delta^l = \Sigma'(z^l) (w^{l+1})^T \delta^{l+1}. \tag{34}\end{eqnarray} (3) By combining observations (1) and (2) show that \begin{eqnarray} \delta^l = \Sigma'(z^l) (w^{l+1})^T \ldots \Sigma'(z^{L-1}) (w^L)^T \Sigma'(z^L) \nabla_a C \tag{35}\end{eqnarray} For readers comfortable with matrix multiplication this equation may be easier to understand than (BP1)\begin{eqnarray} \delta^L_j = \frac{\partial C}{\partial a^L_j} igma'(z^L_j) \nonumber\end{eqnarray} and (BP2)\begin{eqnarray} \delta^l = ((w^{l+1})^T \delta^{l+1}) \odot igma'(z^l) \nonumber\end{eqnarray}. The reason I've focused on (BP1)\begin{eqnarray} \delta^L_j = \frac{\partial C}{\partial a^L_j} igma'(z^L_j) \nonumber\end{eqnarray} and (BP2)\begin{eqnarray} \delta^l = ((w^{l+1})^T \delta^{l+1}) \odot igma'(z^l) \nonumber\end{eqnarray} is because that approach turns out to be faster to implement numerically.Proof of the four fundamental equations (optional) We'll now prove the four fundamentalequations (BP1)\begin{eqnarray} \delta^L_j = \frac{\partial C}{\partial a^L_j} igma'(z^L_j) \nonumber\end{eqnarray}-(BP4)\begin{eqnarray} \frac{\partial C}{\partial w^l_{jk}} = a^{l-1}_k \delta^l_j \nonumber\end{eqnarray}. All four are consequences of thechain rule from multivariable calculus. If you're comfortable withthe chain rule, then I strongly encourage you to attempt thederivation yourself before reading on.Let's begin with Equation (BP1)\begin{eqnarray} \delta^L_j = \frac{\partial C}{\partial a^L_j} igma'(z^L_j) \nonumber\end{eqnarray}, which gives an expression forthe output error, $\delta^L$. To prove this equation, recall that bydefinition\begin{eqnarray} \delta^L_j = \frac{\partial C}{\partial z^L_j}.\tag{36}\end{eqnarray}Applying the chain rule, we can re-express the partial derivativeabove in terms of partial derivatives with respect to the outputactivations,\begin{eqnarray} \delta^L_j = um_k \frac{\partial C}{\partial a^L_k} \frac{\partial a^L_k}{\partial z^L_j},\tag{37}\end{eqnarray}where the sum is over all neurons $k$ in the output layer. Of course,the output activation $a^L_k$ of the $k^{\rm th}$ neuron depends onlyon the weighted input $z^L_j$ for the $j^{\rm th}$ neuron when $k =j$. And so $\partial a^L_k / \partial z^L_j$ vanishes when $k \neqj$. As a result we can simplify the previous equation to\begin{eqnarray} \delta^L_j = \frac{\partial C}{\partial a^L_j} \frac{\partial a^L_j}{\partial z^L_j}.\tag{38}\end{eqnarray}Recalling that $a^L_j = igma(z^L_j)$ the second term on the rightcan be written as $igma'(z^L_j)$, and the equation becomes\begin{eqnarray} \delta^L_j = \frac{\partial C}{\partial a^L_j} igma'(z^L_j),\tag{39}\end{eqnarray}which is just (BP1)\begin{eqnarray} \delta^L_j = \frac{\partial C}{\partial a^L_j} igma'(z^L_j) \nonumber\end{eqnarray}, in component form.Next, we'll prove (BP2)\begin{eqnarray} \delta^l = ((w^{l+1})^T \delta^{l+1}) \odot igma'(z^l) \nonumber\end{eqnarray}, which gives an equation for the error$\delta^l$ in terms of the error in the next layer, $\delta^{l+1}$.To do this, we want to rewrite $\delta^l_j = \partial C / \partialz^l_j$ in terms of $\delta^{l+1}_k = \partial C / \partial z^{l+1}_k$.We can do this using the chain rule,\begin{eqnarray} \delta^l_j & = & \frac{\partial C}{\partial z^l_j} \tag{40}\\ & = & um_k \frac{\partial C}{\partial z^{l+1}_k} \frac{\partial z^{l+1}_k}{\partial z^l_j} \tag{41}\\ & = & um_k \frac{\partial z^{l+1}_k}{\partial z^l_j} \delta^{l+1}_k,\tag{42}\end{eqnarray}where in the last line we have interchanged the two terms on theright-hand side, and substituted the definition of $\delta^{l+1}_k$.To evaluate the first term on the last line, note that\begin{eqnarray} z^{l+1}_k = um_j w^{l+1}_{kj} a^l_j +b^{l+1}_k = um_j w^{l+1}_{kj} igma(z^l_j) +b^{l+1}_k.\tag{43}\end{eqnarray}Differentiating, we obtain\begin{eqnarray} \frac{\partial z^{l+1}_k}{\partial z^l_j} = w^{l+1}_{kj} igma'(z^l_j).\tag{44}\end{eqnarray}Substituting back into (42)\begin{eqnarray} & = & um_k \frac{\partial z^{l+1}_k}{\partial z^l_j} \delta^{l+1}_k \nonumber\end{eqnarray} we obtain\begin{eqnarray} \delta^l_j = um_k w^{l+1}_{kj} \delta^{l+1}_k igma'(z^l_j).\tag{45}\end{eqnarray}This is just (BP2)\begin{eqnarray} \delta^l = ((w^{l+1})^T \delta^{l+1}) \odot igma'(z^l) \nonumber\end{eqnarray} written in component form.The final two equations we want to prove are (BP3)\begin{eqnarray} \frac{\partial C}{\partial b^l_j} = \delta^l_j \nonumber\end{eqnarray}and (BP4)\begin{eqnarray} \frac{\partial C}{\partial w^l_{jk}} = a^{l-1}_k \delta^l_j \nonumber\end{eqnarray}. These also follow from the chain rule, in a mannersimilar to the proofs of the two equations above. I leave them to youas an exercise. Exercise Prove Equations (BP3)\begin{eqnarray} \frac{\partial C}{\partial b^l_j} = \delta^l_j \nonumber\end{eqnarray} and (BP4)\begin{eqnarray} \frac{\partial C}{\partial w^l_{jk}} = a^{l-1}_k \delta^l_j \nonumber\end{eqnarray}.That completes the proof of the four fundamental equations ofbackpropagation. The proof may seem complicated. But it's reallyjust the outcome of carefully applying the chain rule. A little lesssuccinctly, we can think of backpropagation as a way of computing thegradient of the cost function by systematically applying the chainrule from multi-variable calculus. That's all there really is tobackpropagation - the rest is details.The backpropagation algorithmThe backpropagation equations provide us with a way of computing thegradient of the cost function. Let's explicitly write this out in theform of an algorithm: Input $x$: Set the corresponding activation $a^{1}$ for the input layer. Feedforward: For each $l = 2, 3, \ldots, L$ compute $z^{l} = w^l a^{l-1}+b^l$ and $a^{l} = igma(z^{l})$. Output error $\delta^L$: Compute the vector $\delta^{L} = \nabla_a C \odot igma'(z^L)$. Backpropagate the error: For each $l = L-1, L-2, \ldots, 2$ compute $\delta^{l} = ((w^{l+1})^T \delta^{l+1}) \odot igma'(z^{l})$. Output: The gradient of the cost function is given by $\frac{\partial C}{\partial w^l_{jk}} = a^{l-1}_k \delta^l_j$ and $\frac{\partial C}{\partial b^l_j} = \delta^l_j$.Examining the algorithm you can see why it's calledbackpropagation. We compute the error vectors $\delta^l$backward, starting from the final layer. It may seem peculiar thatwe're going through the network backward. But if you think about theproof of backpropagation, the backward movement is a consequence ofthe fact that the cost is a function of outputs from the network. Tounderstand how the cost varies with earlier weights and biases we needto repeatedly apply the chain rule, working backward through thelayers to obtain usable expressions.ExercisesBackpropagation with a single modified neuron Suppose we modify a single neuron in a feedforward network so that the output from the neuron is given by $f(um_j w_j x_j + b)$, where $f$ is some function other than the sigmoid. How should we modify the backpropagation algorithm in this case?Backpropagation with linear neurons Suppose we replace the usual non-linear $igma$ function with $igma(z) = z$ throughout the network. Rewrite the backpropagation algorithm for this case.As I've described it above, the backpropagation algorithm computes thegradient of the cost function for a single training example, $C =C_x$. In practice, it's common to combine backpropagation with alearning algorithm such as stochastic gradient descent, in which wecompute the gradient for many training examples. In particular, givena mini-batch of $m$ training examples, the following algorithm appliesa gradient descent learning step based on that mini-batch: Input a set of training examples For each training example $x$: Set the corresponding input activation $a^{x,1}$, and perform the following steps: Feedforward: For each $l = 2, 3, \ldots, L$ compute $z^{x,l} = w^l a^{x,l-1}+b^l$ and $a^{x,l} = igma(z^{x,l})$. Output error $\delta^{x,L}$: Compute the vector $\delta^{x,L} = \nabla_a C_x \odot igma'(z^{x,L})$. Backpropagate the error: For each $l = L-1, L-2, \ldots, 2$ compute $\delta^{x,l} = ((w^{l+1})^T \delta^{x,l+1}) \odot igma'(z^{x,l})$. Gradient descent: For each $l = L, L-1, \ldots, 2$ update the weights according to the rule $w^l \rightarrow w^l-\frac{\eta}{m} um_x \delta^{x,l} (a^{x,l-1})^T$, and the biases according to the rule $b^l \rightarrow b^l-\frac{\eta}{m} um_x \delta^{x,l}$.Of course, to implement stochastic gradient descent in practice youalso need an outer loop generating mini-batches of training examples,and an outer loop stepping through multiple epochs of training. I'veomitted those for simplicity. The code for backpropagationHaving understood backpropagation in the abstract, we can nowunderstand the code used in the last chapter to implementbackpropagation. Recall fromthat chapter that the code was contained in the update_mini_batchand backprop methods of the Network class. The code forthese methods is a direct translation of the algorithm describedabove. In particular, the update_mini_batch method updates theNetwork's weights and biases by computing the gradient for thecurrent mini_batch of training examples:class Network(object):
 The "mini_batch" is a list of tuples "(x, y)", and "eta"
Most of the work is done by the linedelta_nabla_b, delta_nabla_w = self.backprop(x, y) which usesthe backprop method to figure out the partial derivatives$\partial C_x / \partial b^l_j$ and $\partial C_x / \partialw^l_{jk}$. The backprop method follows the algorithm in thelast section closely. There is one small change - we use a slightlydifferent approach to indexing the layers. This change is made totake advantage of a feature of Python, namely the use of negative listindices to count backward from the end of a list, so, e.g.,l[-3] is the third last entry in a list l. The code forbackprop is below, together with a few helper functions, whichare used to compute the $igma$ function, the derivative $igma'$,and the derivative of the cost function. With these inclusions youshould be able to understand the code in a self-contained way. Ifsomething's tripping you up, you may find it helpful to consultthe original description (and complete listing) of the code.class Network(object):
 activations = [x] # list to store all the activations, layer by layer
 zs = [] # list to store all the z vectors, layer by layer
 # Note that the variable l in the loop below is used a little
 # differently to the notation in Chapter 2 of the book. Here,
 # l = 1 means the last layer of neurons, l = 2 is the
 # second-last layer, and so on. Its a renumbering of the
 # scheme in the book, used here to take advantage of the fact
ProblemFully matrix-based approach to backpropagation over a mini-batch Our implementation of stochastic gradient descent loops over training examples in a mini-batch. It's possible to modify the backpropagation algorithm so that it computes the gradients for all training examples in a mini-batch simultaneously. The idea is that instead of beginning with a single input vector, $x$, we can begin with a matrix $X = [x_1 x_2 \ldots x_m]$ whose columns are the vectors in the mini-batch. We forward-propagate by multiplying by the weight matrices, adding a suitable matrix for the bias terms, and applying the sigmoid function everywhere. We backpropagate along similar lines. Explicitly write out pseudocode for this approach to the backpropagation algorithm. Modify network.py so that it uses this fully matrix-based approach. The advantage of this approach is that it takes full advantage of modern libraries for linear algebra. As a result it can be quite a bit faster than looping over the mini-batch. (On my laptop, for example, the speedup is about a factor of two when run on MNIST classification problems like those we considered in the last chapter.) In practice, all serious libraries for backpropagation use this fully matrix-based approach or some variant.In what sense is backpropagation a fast algorithm?In what sense is backpropagation a fast algorithm? To answer thisquestion, let's consider another approach to computing the gradient.Imagine it's the early days of neural networks research. Maybe it'sthe 1950s or 1960s, and you're the first person in the world to thinkof using gradient descent to learn! But to make the idea work youneed a way of computing the gradient of the cost function. You thinkback to your knowledge of calculus, and decide to see if you can usethe chain rule to compute the gradient. But after playing around abit, the algebra looks complicated, and you get discouraged. So youtry to find another approach. You decide to regard the cost as afunction of the weights $C = C(w)$ alone (we'll get back to the biasesin a moment). You number the weights $w_1, w_2, \ldots$, and want tocompute $\partial C / \partial w_j$ for some particular weight $w_j$.An obvious way of doing that is to use the approximation\begin{eqnarray} \frac{\partial C}{\partial w_{j}} \approx \frac{C(w+\epsilon e_j)-C(w)}{\epsilon},\tag{46}\end{eqnarray}where $\epsilon > 0$ is a small positive number, and $e_j$ is the unitvector in the $j^{\rm th}$ direction. In other words, we can estimate$\partial C / \partial w_j$ by computing the cost $C$ for two slightlydifferent values of $w_j$, and then applyingEquation (46)\begin{eqnarray} \frac{\partial C}{\partial w_{j}} \approx \frac{C(w+\epsilon e_j)-C(w)}{\epsilon} \nonumber\end{eqnarray}. The same idea will let uscompute the partial derivatives $\partial C / \partial b$ with respectto the biases.This approach looks very promising. It's simple conceptually, andextremely easy to implement, using just a few lines of code.Certainly, it looks much more promising than the idea of using thechain rule to compute the gradient!Unfortunately, while this approach appears promising, when youimplement the code it turns out to be extremely slow. To understandwhy, imagine we have a million weights in our network. Then for eachdistinct weight $w_j$ we need to compute $C(w+\epsilon e_j)$ in orderto compute $\partial C / \partial w_j$. That means that to computethe gradient we need to compute the cost function a million differenttimes, requiring a million forward passes through the network (pertraining example). We need to compute $C(w)$ as well, so that's atotal of a million and one passes through the network.What's clever about backpropagation is that it enables us tosimultaneously compute all the partial derivatives $\partial C/ \partial w_j$ using just one forward pass through the network,followed by one backward pass through the network. Roughly speaking,the computational cost of the backward pass is about the same as theforward pass**This should be plausible, but it requires some analysis to make a careful statement. It's plausible because the dominant computational cost in the forward pass is multiplying by the weight matrices, while in the backward pass it's multiplying by the transposes of the weight matrices. These operations obviously have similar computational cost.. And so the total cost ofbackpropagation is roughly the same as making just two forward passesthrough the network. Compare that to the million and one forwardpasses we needed for the approach basedon (46)\begin{eqnarray} \frac{\partial C}{\partial w_{j}} \approx \frac{C(w+\epsilon e_j)-C(w)}{\epsilon} \nonumber\end{eqnarray}! And so even though backpropagationappears superficially more complex than the approach basedon (46)\begin{eqnarray} \frac{\partial C}{\partial w_{j}} \approx \frac{C(w+\epsilon e_j)-C(w)}{\epsilon} \nonumber\end{eqnarray}, it's actually much, much faster.This speedup was first fully appreciated in 1986, and it greatlyexpanded the range of problems that neural networks could solve.That, in turn, caused a rush of people using neural networks. Ofcourse, backpropagation is not a panacea. Even in the late 1980speople ran up against limits, especially when attempting to usebackpropagation to train deep neural networks, i.e., networks withmany hidden layers. Later in the book we'll see how modern computersand some clever new ideas now make it possible to use backpropagationto train such deep neural networks.Backpropagation: the big pictureAs I've explained it, backpropagation presents two mysteries. First,what's the algorithm really doing? We've developed a picture of theerror being backpropagated from the output. But can we go any deeper,and build up more intuition about what is going on when we do allthese matrix and vector multiplications? The second mystery is howsomeone could ever have discovered backpropagation in the first place?It's one thing to follow the steps in an algorithm, or even to followthe proof that the algorithm works. But that doesn't mean youunderstand the problem so well that you could have discovered thealgorithm in the first place. Is there a plausible line of reasoningthat could have led you to discover the backpropagation algorithm? Inthis section I'll address both these mysteries.To improve our intuition about what the algorithm is doing, let'simagine that we've made a small change $\Delta w^l_{jk}$ to someweight in the network, $w^l_{jk}$:That change in weight will cause a change in the output activationfrom the corresponding neuron:That, in turn, will cause a change in all the activations inthe next layer:Those changes will in turn cause changes in the next layer, and thenthe next, and so on all the way through to causing a change in thefinal layer, and then in the cost function:The change $\Delta C$ in the cost is related to the change $\Deltaw^l_{jk}$ in the weight by the equation\begin{eqnarray} \Delta C \approx \frac{\partial C}{\partial w^l_{jk}} \Delta w^l_{jk}.\tag{47}\end{eqnarray}This suggests that a possible approach to computing $\frac{\partial C}{\partial w^l_{jk}}$ is to carefully track how a small change in$w^l_{jk}$ propagates to cause a small change in $C$. If we can dothat, being careful to express everything along the way in terms ofeasily computable quantities, then we should be able to compute$\partial C / \partial w^l_{jk}$.Let's try to carry this out. The change $\Delta w^l_{jk}$ causes asmall change $\Delta a^{l}_j$ in the activation of the $j^{\rm th}$ neuron inthe $l^{\rm th}$ layer. This change is given by\begin{eqnarray} \Delta a^l_j \approx \frac{\partial a^l_j}{\partial w^l_{jk}} \Delta w^l_{jk}.\tag{48}\end{eqnarray}The change in activation $\Delta a^l_{j}$ will cause changes inall the activations in the next layer, i.e., the $(l+1)^{\rm th}$ layer. We'll concentrate on the way just a single one of thoseactivations is affected, say $a^{l+1}_q$,In fact, it'll cause the following change:\begin{eqnarray} \Delta a^{l+1}_q \approx \frac{\partial a^{l+1}_q}{\partial a^l_j} \Delta a^l_j.\tag{49}\end{eqnarray}Substituting in the expression from Equation (48)\begin{eqnarray} \Delta a^l_j \approx \frac{\partial a^l_j}{\partial w^l_{jk}} \Delta w^l_{jk} \nonumber\end{eqnarray},we get:\begin{eqnarray} \Delta a^{l+1}_q \approx \frac{\partial a^{l+1}_q}{\partial a^l_j} \frac{\partial a^l_j}{\partial w^l_{jk}} \Delta w^l_{jk}.\tag{50}\end{eqnarray}Of course, the change $\Delta a^{l+1}_q$ will, in turn, cause changesin the activations in the next layer. In fact, we can imagine a pathall the way through the network from $w^l_{jk}$ to $C$, with eachchange in activation causing a change in the next activation, and,finally, a change in the cost at the output. If the path goes throughactivations $a^l_j, a^{l+1}_q, \ldots, a^{L-1}_n, a^L_m$ then theresulting expression is\begin{eqnarray} \Delta C \approx \frac{\partial C}{\partial a^L_m} \frac{\partial a^L_m}{\partial a^{L-1}_n} \frac{\partial a^{L-1}_n}{\partial a^{L-2}_p} \ldots \frac{\partial a^{l+1}_q}{\partial a^l_j} \frac{\partial a^l_j}{\partial w^l_{jk}} \Delta w^l_{jk},\tag{51}\end{eqnarray}that is, we've picked up a $\partial a / \partial a$ type term foreach additional neuron we've passed through, as well as the $\partialC/\partial a^L_m$ term at the end. This represents the change in $C$due to changes in the activations along this particular path throughthe network. Of course, there's many paths by which a change in$w^l_{jk}$ can propagate to affect the cost, and we've beenconsidering just a single path. To compute the total change in $C$ itis plausible that we should sum over all the possible paths betweenthe weight and the final cost, i.e.,\begin{eqnarray} \Delta C \approx um_{mnp\ldots q} \frac{\partial C}{\partial a^L_m} \frac{\partial a^L_m}{\partial a^{L-1}_n} \frac{\partial a^{L-1}_n}{\partial a^{L-2}_p} \ldots \frac{\partial a^{l+1}_q}{\partial a^l_j} \frac{\partial a^l_j}{\partial w^l_{jk}} \Delta w^l_{jk},\tag{52}\end{eqnarray}where we've summed over all possible choices for the intermediateneurons along the path. Comparing with (47)\begin{eqnarray} \Delta C \approx \frac{\partial C}{\partial w^l_{jk}} \Delta w^l_{jk} \nonumber\end{eqnarray} wesee that\begin{eqnarray} \frac{\partial C}{\partial w^l_{jk}} = um_{mnp\ldots q} \frac{\partial C}{\partial a^L_m} \frac{\partial a^L_m}{\partial a^{L-1}_n} \frac{\partial a^{L-1}_n}{\partial a^{L-2}_p} \ldots \frac{\partial a^{l+1}_q}{\partial a^l_j} \frac{\partial a^l_j}{\partial w^l_{jk}}.\tag{53}\end{eqnarray}Now, Equation (53)\begin{eqnarray} \frac{\partial C}{\partial w^l_{jk}} = um_{mnp\ldots q} \frac{\partial C}{\partial a^L_m} \frac{\partial a^L_m}{\partial a^{L-1}_n} \frac{\partial a^{L-1}_n}{\partial a^{L-2}_p} \ldots \frac{\partial a^{l+1}_q}{\partial a^l_j} \frac{\partial a^l_j}{\partial w^l_{jk}} \nonumber\end{eqnarray} looks complicated. However,it has a nice intuitive interpretation. We're computing the rate ofchange of $C$ with respect to a weight in the network. What theequation tells us is that every edge between two neurons in thenetwork is associated with a rate factor which is just the partialderivative of one neuron's activation with respect to the otherneuron's activation. The edge from the first weight to the firstneuron has a rate factor $\partial a^{l}_j / \partial w^l_{jk}$. Therate factor for a path is just the product of the rate factors alongthe path. And the total rate of change $\partial C / \partialw^l_{jk}$ is just the sum of the rate factors of all paths from theinitial weight to the final cost. This procedure is illustrated here,for a single path:What I've been providing up to now is a heuristic argument, a way ofthinking about what's going on when you perturb a weight in a network.Let me sketch out a line of thinking you could use to further developthis argument. First, you could derive explicit expressions for allthe individual partial derivatives inEquation (53)\begin{eqnarray} \frac{\partial C}{\partial w^l_{jk}} = um_{mnp\ldots q} \frac{\partial C}{\partial a^L_m} \frac{\partial a^L_m}{\partial a^{L-1}_n} \frac{\partial a^{L-1}_n}{\partial a^{L-2}_p} \ldots \frac{\partial a^{l+1}_q}{\partial a^l_j} \frac{\partial a^l_j}{\partial w^l_{jk}} \nonumber\end{eqnarray}. That's easy to do with a bit ofcalculus. Having done that, you could then try to figure out how towrite all the sums over indices as matrix multiplications. This turnsout to be tedious, and requires some persistence, but notextraordinary insight. After doing all this, and then simplifying asmuch as possible, what you discover is that you end up with exactlythe backpropagation algorithm! And so you can think of thebackpropagation algorithm as providing a way of computing the sum overthe rate factor for all these paths. Or, to put it slightlydifferently, the backpropagation algorithm is a clever way of keepingtrack of small perturbations to the weights (and biases) as theypropagate through the network, reach the output, and then affect thecost.Now, I'm not going to work through all this here. It's messy andrequires considerable care to work through all the details. If you'reup for a challenge, you may enjoy attempting it. And even if not, Ihope this line of thinking gives you some insight into whatbackpropagation is accomplishing.What about the other mystery - how backpropagation could have beendiscovered in the first place? In fact, if you follow the approach Ijust sketched you will discover a proof of backpropagation.Unfortunately, the proof is quite a bit longer and more complicatedthan the one I described earlier in this chapter. So how was thatshort (but more mysterious) proof discovered? What you find when youwrite out all the details of the long proof is that, after the fact,there are several obvious simplifications staring you in the face.You make those simplifications, get a shorter proof, and write thatout. And then several more obvious simplifications jump out atyou. So you repeat again. The result after a few iterations is theproof we saw earlier**There is one clever step required. In Equation (53)\begin{eqnarray} \frac{\partial C}{\partial w^l_{jk}} = um_{mnp\ldots q} \frac{\partial C}{\partial a^L_m} \frac{\partial a^L_m}{\partial a^{L-1}_n} \frac{\partial a^{L-1}_n}{\partial a^{L-2}_p} \ldots \frac{\partial a^{l+1}_q}{\partial a^l_j} \frac{\partial a^l_j}{\partial w^l_{jk}} \nonumber\end{eqnarray} the intermediate variables are activations like $a_q^{l+1}$. The clever idea is to switch to using weighted inputs, like $z^{l+1}_q$, as the intermediate variables. If you don't have this idea, and instead continue using the activations $a^{l+1}_q$, the proof you obtain turns out to be slightly more complex than the proof given earlier in the chapter. - short, butsomewhat obscure, because all the signposts to its construction havebeen removed! I am, of course, asking you to trust me on this, butthere really is no great mystery to the origin of the earlier proof.It's just a lot of hard work simplifying the proof I've sketched inthis section. In academic work,
please cite this book as: Michael A. Nielsen, "Neural Networks and
build on this book, but not to sell it. If you're interested in
