Abstract: Classifying videos according to content semantics is an important problem
with a wide range of applications. In this paper, we propose a hybrid deep
learning framework for video classification, which is able to model static
spatial information, short-term motion, as well as long-term temporal clues in
the videos. Specifically, the spatial and the short-term motion features are
types of CNN-based features are then combined in a regularized feature fusion
network for classification, which is able to learn and utilize feature
(LSTM) networks are applied on top of the two features to further model
longer-term temporal clues. The main contribution of this work is the hybrid
learning framework that can model several important aspects of the video data.
We also show that (1) combining the spatial and the short-term motion features
in the regularized fusion network is better than direct classification and
fusion using the CNN with a softmax layer, and (2) the sequence-based LSTM is
two popular and challenging benchmarks, the UCF-101 Human Actions and the
to-date the best reported performance: $91.3\%$ on the UCF-101 and $83.5\%$ on
