 Deep Learning Identity-Preserving Face SpaceConference Paper · December 2013 with 262 ReadsDOI: 10.1109/ICCV.2013.21 Conference: Proceedings of the 2013 IEEE International Conference on Computer VisionCite this publication1st Zhenyao Zhu1.11 · Baidu Silicon Valley AI Lab2nd Ping Luo15.93 · The Chinese University of Hong Kong3rd Xiaogang Wang4th Xiaoou Tang40.11 · The Chinese University of Hong KongAbstractFace recognition with large pose and illumination variations is a challenging problem in computer vision. This paper addresses this challenge by proposing a new learning based face representation: the face identity-preserving (FIP) features. Unlike conventional face descriptors, the FIP features can significantly reduce intra-identity variances, while maintaining discriminative ness between identities. Moreover, the FIP features extracted from an image under any pose and illumination can be used to reconstruct its face image in the canonical view. This property makes it possible to improve the performance of traditional descriptors, such as LBP [2] and Gabor [31], which can be extracted from our reconstructed images in the canonical view to eliminate variations. In order to learn the FIP features, we carefully design a deep network that combines the feature extraction layers and the reconstruction layer. The former encodes a face image into the FIP features, while the latter transforms them to an image in the canonical view. Extensive experiments on the large MultiPIE face database [7] demonstrate that it significantly outperforms the state-of-the-art face recognition methods.Discover the worlds research13+ million members100+ million publications700k+ research projectsJoin for free
 Deep Learning Identity-Preserving Face SpaceZhenyao Zhu1,∗Ping Luo1,3,∗Xiaogang Wang2Xiaoou Tang1,3,†1Department of Information Engineering, The Chinese University of Hong Kong2Department of Electronic Engineering, The Chinese University of Hong Kong3Shenzhen Institutes of Advanced Technology, Chinese Academy of Scienceszz012@ie.cuhk.edu.hk pluo.lhi@gmail.com xgwang@ee.cuhk.edu.hk xtang@ie.cuhk.edu.hkAbstractFace recognition with large pose and illumination varia-tions is a challenging problem in computer vision. This pa-per addresses this challenge by proposing a new learning-based face representation: the face identity-preserving(FIP) features. Unlike conventional face descriptors,the FIP features can signiﬁcantly reduce intra-identityvariances, while maintaining discriminativeness betweenidentities. Moreover, the FIP features extracted from animage under any pose and illumination can be used toreconstruct its face image in the canonical view. Thisproperty makes it possible to improve the performance oftraditional descriptors, such as LBP [2] and Gabor [31],which can be extracted from our reconstructed images inthe canonical view to eliminate variations. In order tolearn the FIP features, we carefully design a deep networkthat combines the feature extraction layers and the recon-struction layer. The former encodes a face image into theFIP features, while the latter transforms them to an imagein the canonical view. Extensive experiments on the largeMultiPIE face database [7] demonstrate that it signiﬁcantlyoutperforms the state-of-the-art face recognition methods.1. IntroductionIn many practical applications, the pose and illuminationchanges become the bottleneck for face recognition [36].Many existing works have been proposed to account forsuch variations. The pose-invariant methods can be gen-erally separated into two categories: 2D-based [17, 5, 23]and 3D-based [18, 3]. In the ﬁrst category, poses areeither handled by 2D image matching or by encoding atest image using some bases or exemplars. For example,∗indicates equal contribution.†This work is supported by the General Research Fund sponsored bythe Research Grants Council of the Kong Kong SAR (Project No. CUHK416312 and CUHK 416510) and Guangdong Innovative Research TeamProgram (No.201001D0104648280).(a)(b)Figure 1. Three face images under different poses and illuminationsof two identities are shown in (a). The FIP features extracted fromthese images are also visualized. The FIP features of the same identityare similar, although the original images are captured in different posesand illuminations. These examples indicate that FIP features are sparseand identity-preserving (blue indicates zero value). (b) shows someimages of two identities, including the original image (left) and thereconstructed image in the canonical view (right) from the FIP features.The reconstructed images remove the pose and illumination variations andretain the intrinsic face structures of the identities. Best viewed in color.Carlos et al. [5] used stereo matching to compute thesimilarity between two faces. Li et al. [17] representeda test face as a linear combination of training images, andutilized the linear regression coefﬁcients as features for facerecognition. 3D-based methods usually capture 3D facedata or estimate 3D models from 2D input, and try to matchthem to a 2D probe face image. Such methods make itpossible to synthesize any view of the probe face, whichmakes them generally more robust to pose variation. Forinstance, Li et al. [18] ﬁrst generated a virtual view for theprobe face by using a set of 3D displacement ﬁelds sampledfrom a 3D face database, and then matched the synthesizedface with the gallery faces. Similarly, Asthana et al. [3]matched the 3D model to a 2D image using the view-basedactive appearance model.The illumination-invariant methods [26, 17] typically1
 (a) LBP (b) LE(c) CRBM (d) FIPFigure 2. The LBP (a), LE (b), CRBM (c), and FIP (d) features of 50identities, each of which has 6 images in different poses and illuminationsare projected into two dimensions using Multidimensional scaling (MDS).Images of the same identity are visualized in the same color. It shows thatFIP has the best representative power. Best viewed in color.make assumptions about how illumination affects the faceimages, and use these assumptions to model and removethe illumination effect. For example, Wagner et al. [26]designed a projector-based system to capture images ofeach subject in the gallery under a few illuminations, whichcan be linearly combined to generate images under arbitraryilluminations. With this augmented gallery, they adoptedsparse coding to perform face recognition.The above methods have certain limitations. For ex-ample, capturing 3D data requires additional cost andresources [18]. Inferring 3D models from 2D data is an ill-posed problem [23]. As the statistical illumination models[26] are often summarized from controlled environment,they cannot be well generalized in practical applications.In this paper, unlike previous works that either buildphysical models or make statistical assumptions, wepropose a novel face representation, the face identity-preserving (FIP) features, which are directly extractedfrom face images with arbitrary poses and illuminations.This new representation can signiﬁcantly remove pose andillumination variations, while maintaining the discrimina-tiveness across identities, as shown in Fig.1 (a). Fur-thermore, unlike traditional face descriptors, e.g. LBP [2],Gabor [31], and LE [4], which cannot recover the originalimages, the FIP features can reconstruct face images in thefrontal pose and with neutral illumination (we call it thecanonical view) of the same identity, as shown in Fig.1 (b).With this attractive property, the conventional descriptorsand learning algorithms can utilize our reconstructed faceimages in the canonical view as input so as to eliminate thenegative effects from poses and illuminations.Speciﬁcally, we present a new deep network to learnthe FIP features. It utilizes face images with arbitrarypose and illumination variations of an identity as input,and reconstructs a face in the canonical view of the sameidentity as the target (see Fig.3). First, input images areencoded through feature extraction layers, which have threelocally connected layers and two pooling layers stackedalternately. Each layer captures face features at a differentscale. As shown in Fig.3, the ﬁrst locally connectedlayer outputs 32 feature maps. Each map has a largenumber of high responses outside the face region, whichmainly capture pose information, and some high responsesinside the face region, which capture face structures (redindicates large response and blue indicates no response).On the output feature maps of the second locally connectedlayer, high responses outside the face region have beensigniﬁcantly reduced, which indicates that it discards mostpose variations while retain the face structures. The thirdlocally connected layer outputs the FIP features, which issparse and identity-preserving.Second, the FIP features recover the face image in thecanonical view using a fully-connected reconstruction layer.As there are large amount of parameters, our network ishard to train using tranditional training methods [14, 12].We propose a new training strategy, which contains twosteps: parameter initialization and parameter update. First,we initialize the parameters based on the least squaredictionary learning. We then update all the parameters byback-propagating the summed squared reconstruction errorbetween the reconstructed image and the ground truth.Existing deep learning methods for face recognitionare generally in two categories: (1) unsupervised learningfeatures with deep models and then using discriminativemethods (e.g. SVM) for classiﬁcation [21, 10, 15]; (2)directly using class labels as supervision of deep models[6, 24]. In the ﬁrst category, features related to identity,poses, and lightings are coupled when learned by deepmodels. It is too late to rely on SVM to separate them later.Our supervised model makes it possible to discard pose andlighting features from the very bottom layer. In the secondcategory, a ‘0/1’ class label is a much weaker supervision,compared with ours using a face image (with thousandsof pixels) of the canonical view as supervision. Werequire the deep model to fully reconstruct the face in thecanonical view rather than simply predicting class labels,and this strong regularization is more effective to avoidoverﬁtting. This design is suitable for face recognition,where a canonical view exists. Different from convolutionalneural networks whose ﬁlters share weights, our ﬁlersare localized and do not share weights since we assumedifferent face regions should employ different features.This work makes three key contributions. (1) We pro-pose a new deep network that combines the feature extrac-tion layers and the reconstruction layer. Its architecture iscarefully designed to learn the FIP features. These featurescan eliminate the poses and illumination variations, and
 n2=24×24×32 n2=24×24×325×5 Locally Connected and Pooling Fully Connected W1, V1 W3W4FIP W2, V2 Feature Extraction Layers Reconstruction Layer x0 x1 x2 x3yy5×5 Locally Connected and Pooling 5×5 Locally Connected n0=96×96 n0=96×96n1=48×48×32242424244848Figure 3. Architecture of the deep network. It combines the feature extraction layers and reconstruction layer. The feature extraction layers include threelocally connected layers and two pooling layers. They encode an input face x0into FIP features x3.x1,x2are the output feature maps of the ﬁrst andsecond locally connected layers. FIP features can be used to recover the face image yin the canonical view. yis the ground truth. Best viewed in color.maintain discriminativeness between different identities.(2) Unlike conventional face descriptors, the FIP featurescan be used to reconstruct a face image in the canonicalview. We also demonstrate signiﬁcant improvement of theexisting methods, when they are applied on our reconstruct-ed face images. (3) Unlike existing works that need to knowthe pose of a probe face, so as to build models for differentposes speciﬁcally, our method can extract the FIP featureswithout knowing information on pose and illumination.The FIP features outperform the state-of-the-art methods,including both 2D-based and 3D-based methods, on theMultiPIE database [7].2. Related WorkThis section reviews related works on learning-basedface descriptors and deep models for feature learning.Learning-based descriptors. Cao et al. [4] devised anunsupervised feature learning method (LE) with random-projection trees and PCA trees, and adopted PCA to gaina compact face descriptor. Zhang et al. [35] extended [4]by introducing an inter-modality encoding method, whichcan match face images in two modalities, e.g. photos andsketches, signiﬁcantly outperforming traditional methods[25, 30]. There are studies that learn the ﬁlters and patternsfor the existing handcrafted descriptors. For example, Guoet al. [8] proposed a supervised learning approach withthe Fisher separation criterion to learn the patterns of LBP[2]. Zhen et al. [16] adopted a strategy similar to LDAto learn the ﬁlters of LBP. Our FIP features are learnedwith a multi-layer deep model in a supervised manner, andhave more discriminative and representative power thanthe above works. We illustrate the feature space of FIPcompared with LE [4] and LBP [2] in Fig.2 (a), (b) and (d),respectively, which show that the FIP space better maintainsboth the intra-identity consistency and the inter-identitydiscriminativeness.Deep models. The deep models learn representationsby stacking many hidden layers, which are layer-wiselytrained in an unsupervised manner. For example, the deepbelief networks [9] (DBN) and deep Boltzmann machine[22] (DBM) stack many layers of restricted Boltzmannmachines (RBM) and can extract different levels of features.Recently, Huang et al. [10] introduced the convolutionalrestricted Boltzmann machine (CRBM), which incorporateslocal ﬁlters into RBM. Their learned ﬁlters can preserve thelocal structures of data. Sun et al. [24] proposed a hy-brid Convolutional Neural Network-Restricted BoltzmannMachine (CNN-RBM) model to learn relational featuresfor comparing face similarity. Unlike DBN and DBMemploy fully connected layers, our deep network combinesboth locally and fully connected layers, which enables it toextract both the local and global information. The locallyconnected architecture of our deep network is similar toCRBM [10], but we learn the network with a supervisedscheme and the FIP features are required to recover thefrontal face image. Therefore, this method is more robustto pose and illumination variations, as shown in Fig.2 (d).3. Network ArchitectureFig.3 shows the architecture of our deep model. Theinput is a face image x0under an arbitrary pose andillumination, and the output is a frontal face image underneutral illumination y. They both have n0= 96 ×96 =9216 dimensions. The feature extraction layers have three
 locally connected layers and two pooling layers, whichencode x0into FIP features x3.In the ﬁrst layer,x0is transformed to 32 feature mapsthrough a weight matrix W1that contains 32 sub-matricesW1= [W11;W12;. . . ;W132],∀W1i∈Rn0,n01, each ofwhich is sparse to retain the locally connected structure[13]. Intuitively, each row of W1irepresents a small ﬁltercentered at a pixel of x0, so that all of the elements in thisrow equal zeros except for the elements belonging to theﬁlter. As our weights are not shared, the non-zero values ofthese rows are not the same2. Therefore, the weight matrixW1results in 32 feature maps {x1i}32i=1, each of which hasn0dimensions. Then, a matrix V1, where Vij ∈ {0,1}encodes the 2D topography of the pooling layer [13], down-samples each of these feature map to 48 ×48 in order toreduce the number of parameters need to be learned andobtain more robust features. Each x1ican be computed as3x1i=V1σ(W1ix0),(1)where σ(x) = max(0, x)is the rectiﬁed linear function[19] that is feature-intensity-invariant. So it is robust toshape and illumination variations. x1can be obtained byconcatenating all the x1i∈R48×48 together, obtaining alarge feature map in n1= 48 ×48 ×32 dimensions.In the second layer, each x1iis transformed to x2i32 sub-matrices {W2i}32i=1,∀W2i∈R48×48,48×48,x2i=32Xj=1V2σ(W2jx1i),(2)where x2iis down-sampled using V2to 24×24 dimensions.Eq.2 means that each small feature map in the ﬁrst layer ismultiplied by 32 sub-matrices and then summed together.Here, each sub-matrix has sparse structure as discussedabove. We can reformulate Eq.2 into a matrix formx2=V2σ(W2x1),(3)where W2= [W201;. . . ;W2032],∀W20i∈R48×48,n1andx1= [x11;. . . ;x132]∈Rn1, respectively. W20iis simplyobtained by repeating W2ifor 32 times. Thus, x2hasn2= 24 ×24 ×32 dimensions.In the third layer,x2is transformed to x3,i.e. the FIPfeatures, similar to the second layer, but without pooling.1In our notation, X∈Ra,b means Xis a two dimensional matrixwith arows and bcolumns. x∈Ra×bmeans xis a vector with a×bdimensions. Also, [x;y]means that we concatenate vectors or matricesxand ycolumn-wisely, while [xy]means that we concatenate xand yrow-wisely.2For the convolutional neural network such as [14], the non-zero valuesare the same for each row.3Note that in the conventional deep model [9], there is a bias term b, sothat the output is σ(W x +b). Since W x +bcan be written asfWex, wedrop the bias term bfor simpliﬁcation.Thus, x3is the same size as x2.x3=σ(W3x2),(4)where W3= [W31;. . . ;W332],∀W3i∈R24×24,n2and x2=[x21;. . . ;x232]∈Rn2, respectively.Finally, the reconstruction layer transforms the FIPfeatures x3to the frontal face image y, through a weightmatrix W4∈Rn0,n2,y=σ(W4x3).(5)4. TrainingTraining our deep network requires estimating all theweight matrices {Wi}as introduced above, which is chal-lenging because of the millions of parameters. Therefore,we ﬁrst initialize the weights and then update them all. V1and V2are manually deﬁned [13] and ﬁxed.4.1. Parameter InitializationWe cannot employ RBMs [9] to unsupervised pre-trainthe weight matrices, because our input/output data are indifferent spaces. Therefore, we devise a supervised methodbased on the least square dictionary learning. As shown inFig.3, X3={x3i}mi=1 are a set of FIP features and Y={yi}mi=1 are a set of target images, where mdenotes thenumber of training examples. Our objective is to minimizethe reconstruction errorarg minW1,W 2,W 3,W 4kY−σ(W4X3)k2F,(6)where k · kFis the Frobenius norm. Optimizing Eq.6 isnot trivial because of its nonlinearity. However, we caninitialize the weight matrices layer-wisely asarg minW1kY−OW 1X0k2F,(7)arg minW2kY−P W 2X1k2F,(8)arg minW3kY−QW 3X2k2F,(9)arg minW4kY−W4X3k2F.(10)In Eq.7, X0={x0i}mi=1 is a set of input images. W1has been introduced in Sec.3, so that W1X0results in 32feature maps for each input. Ois a ﬁxed binary matrixthat sums together the pixels in the same position of thesefeature maps, which makes OW 1X0at the same size as Y.In Eq.8, X1={x1i}mi=1 is a set of outputs of the ﬁrst locallyconnected layer before pooling and Pis also a ﬁxed binarymatrix, which sums together the corresponding pixels andrescales the results to the same size as Y.Q, X2in Eq.9 aredeﬁned in the same way.Intuitively, we ﬁrst directly use X0to approximateYwith a linear transform W1without pooling. Once
 W1has been initialized, X1=V1σ(W1X0)is usedto approximate Yagain with another linear transform,W2. We repeat this process until all the matrices havebeen initialized. A similar strategy has been adoptedby [33], which learns different levels of representationswith a convolutional architecture. All of the above equa-tions have closed-form solutions. For example, W0=(OTO)−1(OTY X0T)(X0X0T)−1. The other matrices canbe computed in the same way.4.2. Parameter UpdateWe update all the weight matrices after the initializationby minimizing the loss function of reconstruction errorE(X0;W) =kY−Yk2F,(11)where W={W1, . . . , W 4}.X0={x0i},Y={yi},and Y={yi}are a set of input images, a set of targetimages, and a set of reconstructed images, respectively. Weupdate Wusing the stochastic gradient descent, in whichthe update rule of Wi, i = 1 . . . 4, in the k-th iteration is∆k+1 = 0.9·∆k−0.004 ··Wik−·∂E∂W ik,(12)Wik+1 = ∆k+1 +Wik,(13)where ∆is the momentum variable [20], is the learningrate, and ∂E∂W i=xi−1(ei)Tis the derivative, which iscomputed as the outer product of the back-propagation erroreiand the feature of the previous layer xi−1. In our deepnetwork, there are three different expressions of ei. First,for the transformation layer, e4is computed based on thederivative of the linear rectiﬁed function [19]e4j=[y−y]j, δ4j>00, δ4j≤0,(14)where δ4j= [W4x3]j.[·]jdenotes the j-th element of avector.Similarly, back-propagation error for e3is computed ase3j=[W4Te4]j, δ3j>00, δ3j≤0,(15)where δ3j= [W3x2]j.We compute e1and e2in the same way as e3since theyboth adopt the same activation function. There is a slightdifference due to down-sampling. For these two layers, wemust up-sample the corresponding back-propagation error eso that it has the same dimensions as the input feature. Thisstrategy has been introduced in [14]. We need to enforce theweight matrices to have locally connected structures aftereach gradient step as introduced in [12]. We implement thisby setting the corresponding matrix elements to zeros, ifthere supposed to be no connections.5. ExperimentsWe conduct two sets of experiments. Sec.5.1 compareswith state-of-the-art methods and learning-based descrip-tors. Sec.5.2 demonstrates that classical face recognitionmethods can be signiﬁcantly improved when applied on ourreconstructed face images in the canonical view.Dataset. To extensively evaluate our method underdifferent poses and illuminations, we select the MultiPIEface database [7], which contains 754,204 images of 337identities. Each identity has images captured under 15poses and 20 illuminations. These images were capturedin four sessions during different periods. Like the previousmethods [3, 18, 17], we evaluate our algorithm on a subsetof the MultiPIE database, where each identity has imagesfrom all the four sections under seven poses from yawangles −45◦∼+45◦, and 20 illuminations marked as ID00-19 in MultiPIE. This subset has 128,940 images.5.1. Face RecognitionThe existing works conduct experiments on MultiPIEwith three different settings: Setting-I was introduced in[3, 18, 34]; Setting-II and Setting-III were introduced in[17]. We describe these settings below.Setting-I and Setting-II only adopt images with differ-ent poses, but with neutral illumination marked as ID 07.They evaluate robustness to pose variations. For Setting-I,the images of the ﬁrst 200 identities in all the four sessionsare chosen for training, and the images of the remaining137 identities for test. During test, one frontal image (i.e.0◦) of each identity in the test set is selected to the gallery,so there are 137 gallery images in total. The remainingimages from −45◦∼+45◦except 0◦are selected asprobes. For Setting-II, only the images in session one areused, which only has 249 identities. The images of theﬁrst 100 identities are for training, and the images of theremaining 149 identities for test. During test, one frontalimage of each identity in the test set is selected in thegallery. The remaining images from −45◦∼+45◦except0◦are selected as probes.Setting-III also adopts images in session one for trainingand test, but it utilizes the images under all the 7 posesand 20 illuminations. This is to evaluate the robustnesswhen both pose and illumination variations are present. Theselection of probes and gallery are the same as Setting-II.We evaluate both the FIP features and the reconstructedimages using the above three settings. Face images areroughly aligned according to the positions of eyes, andrescaled to 96×96. They are converted to grayscale images.The mean value over the training set is subtracted fromeach pixel. For each identity, we use the images with6 poses ranging from −45◦∼+45◦except 0◦, and 19illuminations marked as ID 00-19 except 07, as input totrain our deep network. The reconstruction target is the
 −45◦−30◦−15◦+15◦+30◦+45◦Avg PoseLGBP[34] 37.7 62.5 77 83 59.2 36.1 59.3 !VAAM[3] 74.1 91 95.7 95.7 89.5 74.8 86.9 !FA-EGFC[18] 84.7 95 99.3 99 92.9 85.2 92.7 #SA-EGFC[18] 93 98.7 99.7 99.7 98.3 93.6 97.2 !LE[4]+LDA 86.9 95.5 99.9 99.7 95.5 81.8 93.2 #CRBM[10]+LDA 80.3 90.5 94.9 96.4 88.3 75.2 87.6 #FIP+LDA 93.4 95.6 100.0 98.5 96.4 89.8 95.6 #RL+LDA 95.6 98.5 100.0 99.3 98.5 97.8 98.3 #Table 1. Recognition rates under Setting-I. The ﬁrst and the secondhighest rates are highlighted. “X” indicates the method needs to knowthe pose; “×”, otherwise.−45◦−30◦−15◦+15◦+30◦+45◦Avg PoseLE[4]+`263.0 90.0 95.0 95.0 90.0 61.5 82.4 #CRBM[10]+`259.9 68.5 94.9 83.2 88.3 66.4 74.7 #FIP+`278.6 87.9 94.9 96.1 91.8 80.8 88.3 #RL+`294.9 94.2 98.5 99.3 98.5 84.0 94.9 #Table 2. Recognition rates under Setting-I. The proposed features arecompared with LE and CRBM using only the `2distance for facerecognition. The ﬁrst and the second highest rates are highlighted. “X”indicates the method needs to know the pose; “×”, otherwise.image captured in 0◦under neutral illumination (ID 07).In the test stage, in order to better demonstrate the proposedmethods, we directly adopt the FIP and the reconstructedimages (denoted as RL) as features for face recognition.5.1.1 Results of Setting-IIn this setting, we show superior results in Table 1, wherethe FIP and RL features are compared with four methods,including LGBP [34], VAAM [3], FA-EGFC [18], and SA-EGFC [18], and two learning-based descriptors, includingLE [4] and CRBM [10]. As discussed in Sec.1, LGBPis a 2D-based method, while VAAM, FA-EGFC, and SA-EGFC used 3D face models. We apply LDA on LE, CRBM,FIP, and RL to obtain compact features. Note that LGBP,VAAM, and SA-EGFC need to know the pose of a probe,which means that they build different models to accountfor different poses speciﬁcally. We do not need to knowthe pose of the probe, since our deep network can extractFIP features and reconstruct the face image in the canonicalview given a probe under any pose and any illumination.This is one of our advantages over existing methods.Several observations can be made from Table 1. First,RL performs best on the averaged recognition rates and ﬁveposes. The improvement is larger for larger pose variations.It is interesting to note that RL even outperforms all the3D-based models, which veriﬁes that our reconstructed faceimages in the canonical view are of high quality and robustto pose changes. Fig.4 shows several reconstructed images,indicating that RL can effectively remove the variationsof poses and illuminations, while still retains the intrinsicshapes and structures of the identities.−45◦−30◦−15◦+15◦+30◦+45◦Avg PoseLi [17] 97.0 97.0 100.0 100.0 97.0 92.0 96.8 !RL+LDA 97.8 98.6 100.0 100.0 98.6 98.4 98.4 #Table 3. Recognition rates of RL+LDA compared with Li [17] underSetting-II. “X” indicates the method needs to know the pose; “×”,otherwise.Recognition Rates on Different Poses−45◦−30◦−15◦+15◦+30◦+45◦Avg PoseLi [17] 63.5 69.3 79.7 75.6 71.6 54.6 69.3 !RL+LDA 67.1 74.6 86.1 83.3 75.3 61.8 74.7 #Recognition Rates on Different Illuminations00 01 02 03 04 05 06Li [17] 51.5 49.2 55.7 62.7 79.5 88.3 97.5RL+LDA 72.8 75.8 75.8 75.7 75.7 75.7 75.708 09 10 11 12 13 14Li [17] 97.7 91.0 79.0 64.8 54.3 47.7 67.3RL+LDA 75.7 75.7 75.7 75.7 75.7 75.7 73.415 16 17 18 19 Avg.Li [17] 67.7 75.5 69.5 67.3 50.8 69.3RL+LDA 73.4 73.4 73.4 72.9 72.9 74.7Table 4. Recognition rates of RL+LDA compared with Li [17] underSetting-III. “X” indicates the method needs to know the pose; “×”,otherwise.Second, FIP features are better than the two learning-based descriptors and the other three methods except SA-EGFC, which used the 3D model and required the pose ofthe probe. We further report the results of FIP comparedwith LE and CRBM using only `2distance in Table 2 .The RL and FIP outperform the above two learning basedfeatures, especially when large pose variations are present.Third, although FIP does not exceed RL, its still avaluable representation, because it has the sparse propertyand can reconstruct RL efﬁciently and losslessly.5.1.2 Results of Setting-II and Setting-IIILi et al. [17] evaluated on these two settings and reportedthe state-of-the-art results. Setting-II covers only posevariations and Setting-III covers both pose and illuminationvariations.For Setting-II, the results of RL+LDA compared with[17] are reported in Table 3, which shows that RL obtainsthe best results on all the poses. Note that the poses ofprobes in [17] are assumed to be given, which means theytrained a different model for each pose separately. [17]did not report detailed recognition rates when the poses ofthe probes are unknown, except for describing a 20-30%decline of the overall recognition rate.For Setting-III, RL+LDA is compared with [17] onimages with both pose and illumination variations. Table4 reports that our approach achieves better results on allthe poses and illuminations. The recognition rate under apose is the averaged result over all the possible illumina-tions. Similarly, the recognition rate under one illumination
 condition is the averaged result of all the possible poses. Weobserve that the performance of RL+LDA under differentilluminations is close because RL can well remove theeffect of different types of illuminations.5.2. Improve Classical Face Recognition MethodsIn this section, we will show that the conventional featureextraction and dimension reduction methods in the facerecognition literature, such as LBP [2], Gabor [31], PCA[11], LDA [1], and Sparse Coding (SC) [32], can achievesigniﬁcant improvement when they adopt our reconstructedimages as input.We conduct three experiments using the training/testingdata of Setting-I. First, we show the advantage of ourreconstructed images in the canonical view over the originalimages. Second, we show the improvements of Gabor whenit is extracted on our reconstructed images. Third, we showthat LBP can be improved as well.In the ﬁrst experiment, `2distance, SC, PCA, LDA, andPCA+LDA are directly applied on the raw pixels of theoriginal images and our reconstructed images, respectively.The recognition rates are reported in Fig.5(a), where theresults on the original images and the reconstructed imagesare illustrated as solid bars (front) and hollow bars (back).We observe that each of the above methods can be improvedat least 30% on average. They can achieve relatively highperformance on different poses, because our reconstructionlayer can successfully recover the frontal face image. Forexample, the recognition rates of SC on different posesusing the original images are 20.9%,43.6%,65.0%,66.1%,38.3%, and 26.9%, respectively, while 92.7%,97.1%,97.8%,98.5%,97.8%, and 81.8%, respectively, using thereconstructed images.In the second experiment, we extract Gabor features onboth the original images and reconstructed images. Weobserve large improvements by using the reconstructedimages. Speciﬁcally, for each image in 96 ×96, we evenlyselect 11 ×10 keypoints and apply 40 Gabor kernels (5scales ×8 orientations) on each of these keypoints. Weagain use the `2distance, PCA, LDA, and PCA+LDA forface recognition. The results are shown in Fig.5(b).In the third experiment, we extract LBP features on bothoriginal images and reconstructed images. Speciﬁcally, wedivide each 96 ×96 image into 12 ×12 cells, and the 59uniform binary patterns are computed in each cell. Wethen adopt the χ2distance, PCA, LDA, and PCA+LDA forface recognition. Fig.5(c) shows that LBP combined withall these methods can also be signiﬁcantly improved. Forinstance, the averaged recognition rate of LBP+χ2using theoriginal images is 75.9%, and the corresponding accuracyon our reconstructed images, i.e. RL+LBP+χ2, is 96.5%,which is better than 94.9% of RL+`2in Table 2.0102030405060708090100-45 -30 -15 +15 +30 +45Recognition Rate(%)Probe Pose(°)NN Sparse Coding PCA LDA PCA+LDA(a) Pixels0102030405060708090100-45 -30 -15 +15 +30 +45Recognition Rate(%)Probe Pose(°)Gabor+NN Gabor+PCA Gabor+LDA Gabor+PCA+LDA(b) Gabor descriptors0102030405060708090100-45 -30 -15 +15 +30 +45Recognition Rate(%)Probe Pose(°)LBP+NN LBP+PCA LBP+LDA LBP+PCA+LDA(c) LBP descriptorsFigure 5. The conventional face recognition methods can be improvedwhen they are applied on our reconstructed images. The results of threedescriptors (pixel intensity, Gabor, and LBP) and four face recognitionmethods (`2or χ2distance, sparse coding (SC), PCA, and LDA) arereported in (a), (b) and (c), respectively. The hollow bars are theperformance of these methods applied on our reconstructed images, whilethe solid bars are on the original images.6. ConclusionWe have proposed identity-preserving features for facerecognition. The FIP features are not only robust topose and illumination variations, but can also be used toreconstruct face images in the canonical view. FIP islearned using a deep model that contains feature extractionlayers and a reconstruction layer. We show that FIP featuresoutperform the state-of-the-art face recognition methods.We have aslo improved classical face recognition methodsby applying them on our reconstructed face images. In thefuture work, we will extend the framework to deal withrobust face recognition in other difﬁcult conditions suchas expression change and face sketch recognition [25, 30],and will combine FIP features with more classic facerecognition approaches to further improve the performance[28, 29, 27].References[1] H. Abdi. Discriminant correspondence analysis. Encyclopedia of Measurementand Statistics, 2007.[2] T. Ahonen, A. Hadid, and M. Pietikainen. Face description with local binarypatterns: Application to face recognition. IEEE Transactions on PatternAnalysis and Machine Intelligence, 28(12):2037–2041, 2006.
 Figure 4. Examples of face reconstruction. For each identity, we select its images with 6 poses and arbitrary illuminations. The reconstructed frontal faceimages under neutral illumination are visualized below. We clearly see that our method can remove the effects of both poses and illuminations, and retainsthe intrinsic face shapes and structures of the identity.[3] A. Asthana, T. K. Marks, M. J. Jones, K. H. Tieu, and M. Rohith. Fullyautomatic pose-invariant face recognition via 3d pose normalization. In ICCV,2011.[4] Z. Cao, Q. Yin, X. Tang, and J. Sun. Face recognition with learning-baseddescriptor. In CVPR, 2010.[5] C. D. Castillo and D. W. Jacobs. Wide-baseline stereo for face recognition withlarge pose variation. In CVPR, 2011.[6] S. Chopra, R. Hadsell, and Y. LeCun. Learning a similarity metric discrimina-tively, with application to face. In CVPR, 2005.[7] R. Gross, I. Matthews, J. Cohn, T. Kanade, and S. Baker. Multi-pie. InInternational Conference on Automatic Face and Gesture Recognition, 2008.[8] Y. Guo, G. Zhao, M. Pietikainen, and Z. Xu. Descriptor learning based on ﬁsherseparation criterion for texture classiﬁcation. In ACCV, 2010.[9] G. E. Hinton, S. Osindero, and Y.-W. Teh. A fast learning algorithm for deepbelief nets. Neural Computation, 18(7):1527–1554, 2006.[10] G. B. Huang, H. Lee, and E. Learned-Miller. Learning hierarchical represen-tations for face veriﬁcation with convolutional deep belief networks. In CVPR,2012.[11] I. T. Jolliffe. Principal component analysis, volume 487. 1986.[12] A. Krizhevsky, I. Sutskever, and G. Hinton. Imagenet classiﬁcation with deepconvolutional neural networks. In NIPS, 2012.[13] Q. V. Le, J. Ngiam, Z. Chen, D. Chia, P. W. Koh, and A. Y. Ng. Tiledconvolutional neural networks. In NIPS, 2010.[14] Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner. Gradient-based learningapplied to document recognition. In Proceedings of the IEEE, 1998.[15] H. Lee, R. Grosse, R. Ranganath, and A. Y. Ng. Convolutional deep beliefnetworks for scalable unsupervised learning of hierarchical representations. InProc. 26th International Conference on Machine Learning, pages 609–616.ACM, 2009.[16] Z. Lei, D. Yi, and S. Z. Li. Discriminant image ﬁlter learning for facerecognition with local binary pattern like representation. In CVPR, 2012.[17] A. Li, S. Shan, and W. Gao. Coupled bias–variance tradeoff for cross-pose facerecognition. IEEE Transactions on Image Processing, 21(1):305–315, 2012.[18] S. Li, X. Liu, X. Chai, H. Zhang, S. Lao, and S. Shan. Morphable displacementﬁeld based image matching for face recognition across pose. In ECCV. 2012.[19] V. Nair and G. E. Hinton. Rectiﬁed linear units improve restricted boltzmannmachines. In Proc. 27th International Conference on Machine Learning, 2010.[20] N. Qian. On the momentum term in gradient descent learning algorithms.Neural Networks, 1999.[21] M. Ranzato, J. Susskind, V. Mnih, and G. Hinton. On deep generative modelswith applications to recognition. In CVPR, 2011.[22] R. Salakhutdinov and G. E. Hinton. Deep boltzmann machines. InProceedings of the International Conference on Artiﬁcial Intelligence andStatistics, volume 5, pages 448–455, 2009.[23] F. Schroff, T. Treibitz, D. Kriegman, and S. Belongie. Pose, illuminationand expression invariant pairwise face-similarity measure via doppelg¨anger listcomparison. In ICCV, 2011.[24] Y. Sun, X. Wang, and X. Tang. Hybrid deep learning for face veriﬁcation. InICCV, 2013.[25] X. Tang and X. Wang. Face sketch recognition. IEEE Transactions on Circuitsand Systems for Video Technology, 14(1):50–57, 2004.[26] A. Wagner, J. Wright, A. Ganesh, Z. Zhou, H. Mobahi, and Y. Ma. Towarda practical face recognition system: Robust alignment and illumination bysparse representation. IEEE Transactions on Pattern Analysis and MachineIntelligence, 34(2):372–386, 2012.[27] X. Wang and X. Tang. Dual-space linear discriminant analysis for facerecognition. In CVPR, 2004.[28] X. Wang and X. Tang. A uniﬁed framework for subspace face recognition.IEEE Transactions on Pattern Analysis and Machine Intelligence, 26(9):1222–1228, 2004.[29] X. Wang and X. Tang. Random sampling for subspace face recognition.International Journal of Computer Vision, 70(1):91–104, 2006.[30] X. Wang and X. Tang. Face photo-sketch synthesis and recognition. IEEETransactions on Pattern Analysis and Machine Intelligence, 31(11):1955–1967,2009.[31] L. Wiskott, J.-M. Fellous, N. Kuiger, and C. von der Malsburg. Face recognitionby elastic bunch graph matching. IEEE Transactions on Pattern Analysis andMachine Intelligence, 19(7):775–779, 1997.[32] J. Wright, A. Y. Yang, A. Ganesh, S. S. Sastry, and Y. Ma. Robust facerecognition via sparse representation. IEEE Transactions on Pattern Analysisand Machine Intelligence, 31(2):210–227, 2009.[33] M. D. Zeiler, G. W. Taylor, and R. Fergus. Adaptive deconvolutional networksfor mid and high level feature learning. In ICCV, 2011.[34] W. Zhang, S. Shan, W. Gao, X. Chen, and H. Zhang. Local gabor binarypattern histogram sequence (lgbphs): A novel non-statistical model for facerepresentation and recognition. In ICCV, 2005.[35] W. Zhang, X. Wang, and X. Tang. Coupled information-theoretic encoding forface photo-sketch recognition. In CVPR, 2011.[36] X. Zhang and Y. Gao. Face recognition across pose: A review. PatternRecognition, 42(11):2876–2896, 2009.
 CitationsCitations116ReferencesReferences41Further works consider generalization across multiple viewpoints[34]and multiview inter and intra discriminant analysis[13]. With the introduction of DNNs, prior works aim to transfer information from pose variant inputs to a frontalized appearance[41,45], which is then used for face recognition[51]. The frontal appearance reconstruction usually relies on large amount of training data and the pairing across poses is too strict to be practical. Reconstruction-Based Disentanglement for Pose-invariant Face Recognition[Show abstract] [Hide abstract] ABSTRACT: Deep neural networks (DNNs) trained on large-scale datasets have recently achieved impressive improvements in face recognition. But a persistent challenge remains to develop methods capable of handling large pose variations that are relatively under-represented in training data. This paper presents a method for learning a feature representation that is invariant to pose, without requiring extensive pose coverage in training data. We first propose to generate non-frontal views from a single frontal face, in order to increase the diversity of training data while preserving accurate facial details that are critical for identity discrimination. Our next contribution is to seek a rich embedding that encodes identity features, as well as non-identity ones such as pose and landmark locations. Finally, we propose a new feature reconstruction metric learning to explicitly disentangle identity and pose, by demanding alignment between the feature reconstructions through various combinations of identity and pose features, which is obtained from two images of the same subject. Experiments on both controlled and in-the-wild face datasets, such as MultiPIE, 300WLP and the profile view database CFP, show that our method consistently outperforms the state-of-the-art, especially on images with large head pose variations. Full-text · Conference Paper · Oct 2017 Xi PengYu ‡ XiangKihyuk Sohn+1 more author...Dimitris N. MetaxasRead full-textFeature pooling across different poses[16]is also proposed to allow a single network structure for multiple pose inputs. Pose-invariant feature disentanglement[20]or identity preservation[45,39]methods aim to factorize out the non-identity part with carefully designed networks. Some other methods focus on fusing information at the feature level[3]or distance level[18]. Towards Large-Pose Face Frontalization in the Wild[Show abstract] [Hide abstract] ABSTRACT: Despite recent advances in face recognition using deep learning, severe accuracy drops are observed for large pose variations in unconstrained environments. Learning pose-invariant features is one solution, but needs expensively labeled large-scale data and carefully designed feature learning algorithms. In this work, we focus on frontalizing faces in the wild under various head poses, including extreme profile views. We propose a novel deep 3D Morphable Model (3DMM) conditioned Face Frontalization Generative Ad-versarial Network (GAN), termed as FF-GAN, to generate neutral head pose face images. Our framework differs from both traditional GANs and 3DMM based modeling. Incorporating 3DMM into the GAN structure provides shape and appearance priors for fast convergence with less training data, while also supporting end-to-end training. The 3DMM-conditioned GAN employs not only the discriminator and generator loss but also a new masked symmetry loss to retain visual quality under occlusions, besides an identity loss to recover high frequency information. Experiments on face recognition, landmark localization and 3D reconstruction consistently show the advantage of our frontalization method on faces in the wild datasets. 1 Full-text · Conference Paper · Oct 2017 Xi YinYu ‡ XiangKihyuk Sohn+1 more author...Xiaoming LiuRead full-textAdversarial Loss vs. L2 Loss Prior work[38,40,44]on face rotation normally employ the L2 loss to learn a mapping between two views. To compare the L2 loss with our adversarial loss, we train a model where G is supervised by an L2 loss on the ground truth face with the target view. Disentangled Representation Learning GAN for Pose-Invariant Face Recognition[Show abstract] [Hide abstract] ABSTRACT: The large pose discrepancy between two face images is one of the key challenges in face recognition. Conventional approaches for pose-invariant face recognition either perform face frontalization on, or learn a pose-invariant representation from, a non-frontal face image. We argue that it is more desirable to perform both tasks jointly to allow them to leverage each other. To this end, this paper proposes Disentangled Representation learning-Generative Adversarial Network (DR-GAN) with three distinct novelties. First, the encoder-decoder structure of the generator allows DR-GAN to learn a generative and discriminative representation, in addition to image synthesis. Second, this representation is explicitly disentangled from other face variations such as pose, through the pose code provided to the decoder and pose estimation in the discriminator. Third, DR-GAN can take one or multiple images as the input, and generate one unified representation along with an arbitrary number of synthetic images. Quantitative and qualitative evaluation on both controlled and in-the-wild databases demonstrate the superiority of DR-GAN over the state of the art. Full-text · Conference Paper · Jul 2017 Luan Quoc TranXi YinXiaoming LiuRead full-textFace alignment refers to detecting facial landmarks such as pupil centers, nose tip and mouth corners. It is the preprocessor stage of many face analysis tasks like face recognition[1]and face animation[2]. There is a pressing need for a robust and accurate face alignment method with the development of social networks and mobile terminals. Learning a Multi-Center Convolutional Network for Unconstrained Face Alignment[Show abstract] [Hide abstract] ABSTRACT: In this paper, we propose a novel multi-center convolutional neural network for unconstrained face alignment. To utilize structural correlations among different facial landmarks, we determine several clusters based on their spatial position. We pre-train our network to learn generic feature representations. We further fine-tune the pre-trained model to emphasize on locating a certain cluster of landmarks respectively. Fine-tuning contributes to searching an optimal solution smoothly without deviating from the pre-trained model excessively. We obtain an excellent solution by combining multiple fine-tuned models. Extensive experiments demonstrate that our method possesses superior capability of handling extreme occlusions and complex variations of pose, expression, illumination. The code for our method is available at https://github.com/ZhiwenShao/MCNet. Full-text · Conference Paper · Jul 2017 Zhiwen ShaoHengliang ZhuYangyang Hao+1 more author...Min WangRead full-textApart from CNN and GAN, lots of other deep neural network based methods have been developed. A novel face representation called the face identity-preserving (FIP) feature is proposed by[72]. In order to better understand facial features and generate multi-view images, Zhu et al.[73]proposed multi-view perceptron (MVP). Recent Progress of Face Image Synthesis[Show abstract] [Hide abstract] ABSTRACT: Face synthesis has been a fascinating yet challenging problem in computer vision and machine learning. Its main research effort is to design algorithms to generate photo-realistic face images via given semantic domain. It has been a crucial prepossessing step of main-stream face recognition approaches and an excellent test of AI ability to use complicated probability distributions. In this paper, we provide a comprehensive review of typical face synthesis works that involve traditional methods as well as advanced deep learning approaches. Particularly, Generative Adversarial Net (GAN) is highlighted to generate photo-realistic and identity preserving results. Furthermore, the public available databases and evaluation metrics are introduced in details. We end the review with discussing unsolved difficulties and promising directions for future research.Article · Jun 2017 Zhihe LuZhihang LiJie Cao+1 more author...Ran HeReadEarly attempts to disentangle style and content manifolds used factored tensor representations[30,34,10,29], applying their results to face image synthesis. More recent work focuses on learning hierarchical feature representations using deep convolutional neural networks to separate identity and pose manifolds for faces[43,26,44,36]and products[8]. Gatys et al.[11]use features of a ConvNet, pretrained for image recognition, as a means for discovering content and style vectors. Semantically Decomposing the Latent Spaces of Generative Adversarial Networks[Show abstract] [Hide abstract] ABSTRACT: We propose a new algorithm for training generative adversarial networks to jointly learn latent codes for both identities (e.g. individual humans) and observations (e.g. specific photographs). In practice, this means that by fixing the identity portion of latent codes, we can generate diverse images of the same subject, and by fixing the observation portion we can traverse the manifold of subjects while maintaining contingent aspects such as lighting and pose. Our algorithm features a pairwise training scheme in which each sample from the generator consists of two images with a common identity code. Corresponding samples from the real dataset consist of two distinct photographs of the same subject. In order to fool the discriminator, the generator must produce images that are both photorealistic, distinct, and appear to depict the same person. We augment both the DCGAN and BEGAN approaches with Siamese discriminators to accommodate pairwise training. Experiments with human judges and an off-the-shelf face verification system demonstrate our algorithms ability to generate convincing, identity-matched photographs.Article · May 2017 Chris DonahueAkshay BalsubramaniJulian McAuleyZachary Chase LiptonZachary Chase LiptonReadShow moreRecommendationsDiscover more publications, questions and projects in Deep LearningProjectLearning Multiscale Features Directly from WaveformsZhenyao ZhuAwni HannunJesse EngelView projectDiscover moreData provided are for informational purposes only. Although carefully collected, accuracy cannot be guaranteed. Publisher conditions are provided by RoMEO. Differing provisions from the publishers actual policy or licence agreement may be applicable.This publication is from a journal that may support self archiving.Learn moreLast Updated: 24 Apr 17 
