I often reply with I dont know exactly, but I have lots of ideas.
Then I proceed to list out all of the ideas I can think of that might give a lift in performance.
Rather than write out that list again, Ive decided to put all of my ideas into this post.
The ideas wont just help you with deep learning, but really any machine learning algorithm.
How To Improve Deep Learning PerformancePhoto by Pedro Ribeiro Simões, some rights reserved.
This list of ideas is not complete but it is a great start.
My goal is to give you lots ideas of things to try, hopefully, one or two ideas that you have not thought of.
You often only need one good idea to get a lift.
If you get results from one of the ideas, let me know in the comments.
If you have one more idea or an extension of one of the ideas listed, let me know, I and all readers would benefit! It might just be the one idea that helps someone else get their breakthrough.
The gains often get smaller the further down the list. For example, a new framing of your problem or more data is often going to give you more payoff than tuning the parameters of your best performing algorithm. Not always, but in general.
I have included lots of links to tutorials from the blog, questions from related sites as well as questions on the classic Neural Net FAQ.
Some of the ideas are specific to artificial neural networks, but many are quite general. General enough that you could use them to spark ideas on improving your performance with other techniques.
You can get big wins with changes to your training data and problem definition. Perhaps even the biggest wins.
The quality of your models is generally constrained by the quality of your training data. You want the best data you can get for your problem.
Deep learning and other modern nonlinear machine learning techniques get better with more data. Deep learning especially. It is one of the main points that make deep learning so exciting.
More data does not always help, but it can. If I am given the choice, I will get more data for the optionality it provides.
If you cant reasonably get more data, you can invent more data.
If your data are vectors of numbers, create randomly modified versions of existing vectors.
If your data are images, create randomly modified versions of existing images.
You can use a generative model. You can also use simple tricks.
For example, with photograph image data, you can get big gains by randomly shifting and rotating existing images. It improves the generalization of the model to such transforms in the data if they are to be expected in new data.
This is also related to adding noise, what we used to call adding jitter. It can act like a regularization method to curb overfitting the training dataset.
A traditional rule of thumb when working with neural networks is:
If you are using sigmoid activation functions, rescale your data to values between 0-and-1. If youre using the Hyperbolic Tangent (tanh), rescale to values between -1 and 1.
This applies to inputs (x) and outputs (y). For example, if you have a sigmoid on the output layer to predict binary values, normalize your y values to be binary. If you are using softmax, you can still get benefit from normalizing your y values.
This is still a good rule of thumb, but I would go further.
I would suggest that you create a few different versions of your training dataset as follows:
Then evaluate the performance of your model on each. Pick one, then double down.
Big values accumulating in your network are not good. In addition, there are other methods for keeping numbers small in your network such as normalizing activation and weights, but well look at these techniques later.
How To Prepare Your Data For Machine Learning in Python with Scikit-Learn
You must really get to know your data. Visualize it. Look for outliers.
Does a column look like a skewed Gaussian, consider adjusting the skew with a Box-Cox transform.
Does a column look like an exponential distribution, consider a log transform.
Does a column look like it has some features, but they are being clobbered by something obvious, try squaring, or square-rooting.
Can you make a feature discrete or binned in some way to better emphasize some feature.
Can you expose some interesting aspect of the problem with a new boolean flag?
Can you explore temporal or other structure in some other way?
But they will also learn a problem much faster if you can better expose the structure of the problem to the network for learning.
Spot-check lots of different transforms of your data or of specific attributes and see what works and what doesnt.
Discover Feature Engineering, How to Engineer Features and How to Get Good at It
How To Prepare Your Data For Machine Learning in Python with Scikit-Learn
Theyll use a near-zero weight and sideline the contribution of non-predictive attributes.
Still, thats data, weights, training cycles used on data not needed to make good predictions.
There are lots of feature selection methods and feature importance methods that can give you ideas of features to keep and features to boot.
Try some. Try them all. The idea is to get ideas.
Again, if you have time, I would suggest evaluating a few different selected Views of your problem with the same network and see how they perform.
Maybe you can do as well or better with fewer features. Yay, faster!
Maybe all the feature selection methods boot the same specific subset of features. Yay, consensus on useless features.
Maybe a selected subset gives you some ideas on further feature engineering you can perform. Yay, more ideas.
Are the observations that youve collected the only way to frame your problem?
Maybe there are other ways. Maybe other framings of the problem are able to better expose the structure of your problem to learning.
I really like this exercise because it forces you to open your mind. Its hard. Especially if youre invested (ego!!!, time, money) in the current approach.
Even if you just list off 3-to-5 alternate framings and discount them, at least you are building your confidence in the chosen approach.
Maybe you can incorporate temporal elements in a window or in a method that permits timesteps.
Maybe your classification problem can become a regression problem, or the reverse.
It is a good idea to think through the problem and its possible framings before you pick up the tool, because youre less invested in solutions.
Nevertheless, if youre stuck, this one simple exercise can deliver a spring of ideas.
Also, you dont have to throw away any of your prior work. See the ensembles section later on.
All the theory and math describes different approaches to learn a decision process from data (if we constrain ourselves to predictive modeling).
Youve chosen deep learning for your problem. Is it really the best technique you could have chosen?
In this section, well touch on just a few ideas around algorithm selection before next diving into the specifics of getting the most from your chosen deep learning method.
You cannot know which algorithm will perform best on your problem beforehand.
What evidence have you collected that your chosen method was a good choice?
No single algorithm can perform better than any other, when performance is averaged across all possible problems. All algorithms are equal. This is a summary of the finding from the no free lunch theorem.
Now, we are not trying to solve all possible problems, but the new hotness in algorithm land may not be the best choice on your specific dataset.
My advice is to collect evidence. Entertain the idea that there are other good algorithms and given them a fair shot on your problem.
Spot-check a suite of top methods and see which fair well and which do not.
Evaluate some linear methods like logistic regression and linear discriminate analysis.
Evaluate some tree methods like CART, Random Forest and Gradient Boosting.
Evaluate some other neural network methods like LVQ, MLP, CNN, LSTM, hybrids, etc.
Double down on the top performers and improve their chance with some further tuning or data preparation.
Rank the results against your chosen deep learning method, how do they compare?
Maybe you can drop the deep learning model and use something a lot simpler, a lot faster to train, even something that is easy to understand.
Why you should be Spot-Checking Algorithms on your Machine Learning Problems
A great shortcut to picking a good method, is to steal ideas from literature.
Who else has worked on a problem like yours and what methods did they use.
Check papers, books, blog posts, Q&A sites, tutorials, everything Google throws at you.
Write down all the ideas and work your way through them.
This is not about replicating research, it is about new ideas that you have not thought of that may give you a lift in performance.
There are a lot of smart people writing lots of interesting things. Mine this great library for the nuggets you need.
This often means we cannot use gold standard methods to estimate the performance of the model such as k-fold cross validation.
Maybe you are using a simple train/test split, this is very common. If so, you need to ensure that the split is representative of the problem. Univariate stats and visualization are a good start.
Maybe you can exploit hardware to improve the estimates. For example, if you have a cluster or an Amazon Web Services account, we can train n-models in parallel then take the mean and standard deviation of the results to get a more robust estimate.
Maybe you can use a validation hold out set to get an idea of the performance of the model as it trains (useful for early stopping, see later).
Maybe you can hold back a completely blind validation set that you use only after you have performed model selection.
Going the other way, maybe you can make the dataset smaller and use stronger resampling methods.
Maybe you see a strong correlation with the performance of the model trained on a sample of the training dataset as to one trained on the whole dataset. Perhaps you can perform model selection and tuning using the smaller dataset, then scale the final technique up to the full dataset at the end.
Maybe you can constrain the dataset anyway, take a sample and use that for all model development.
You must have complete confidence in the performance estimates of your models.
Evaluate the Performance of Machine Learning Algorithms in Python using Resampling
You can often unearth one or two well-performing algorithms quickly from spot-checking. Getting the most from those algorithms can take, days, weeks or months.
Here are some ideas on tuning your neural network algorithms in order to get more out of them.
You may need to train a given configuration of your network many times (3-10 or more) to get a good estimate of the performance of the configuration. This probably applies to all the aspects that you can tune in this section.
How to Grid Search Hyperparameters for Deep Learning Models in Python With Keras
You will get better performance if you know why performance is no longer improving.
It will be doing one or the other, just by varying degrees.
A quick way to get insight into the learning behavior of your model is to evaluate it on the training and a validation dataset each epoch, and plot the results.
If training is much better than the validation set, you are probably overfitting and you can use techniques like regularization.
If training and validation are both low, you are probably underfitting and you can probably increase the capacity of your network and train more or longer.
If there is an inflection point when training goes above the validation, you might be able to use early stopping.
Create these plots often and study them for insight into the different techniques you can use to improve performance.
These plots might be the most valuable diagnostics you can create.
Another useful diagnostic is to study the observations that the network gets right and wrong.
On some problems, this can give you ideas of things to try.
Perhaps you need more or augmented examples of the difficult-to-train on examples.
Perhaps you can remove large samples of the training dataset that are easy to model.
Perhaps you can use specialized models that focus on different clear regions of the input space.
In practice, that is still probably good enough. But is it the best for your network?
There are also heuristics for different activation functions, but I dont remember seeing much difference in practice.
Remember, the weights are the actual parameters of your model that you are trying to find. There are many sets of weights that give good performance, but you want better performance.
Try all the different initialization methods offered and see if one is better with all else held constant.
Try taking an existing model and retraining a new input and output layer for your problem (transfer learning)
Remember, changing the weight initialization method is closely tied with the activation function and even the optimization function.
Grid search common learning rate values from the literature and see how far you can push the network.
Try a learning rate that drops every fixed number of epochs by a percentage.
Try adding a momentum term then grid search learning rate and momentum together.
Larger networks need more training, and the reverse. If you add more neurons or more layers, increase your learning rate.
Learning rate is coupled with the number of training epochs, batch size and optimization method.
Using Learning Rate Schedules for Deep Learning Models in Python with Keras
Before that it was sigmoid and tanh, then a softmax, linear or sigmoid on the output layer. I dont recommend trying more than that unless you know what youre doing.
Try all three though and rescale your data to meet the bounds of the functions.
Obviously, you want to choose the right transfer function for the form of your output, but consider exploring different representations.
For example, switch your sigmoid for binary classification to linear for a regression problem, then post-process your outputs. This may also require changing the loss function to something more appropriate. See the section on Data Transforms for more ideas along these lines.
Try topology patterns (fan out then in) and rules of thumb from books and papers (see links below).
Its hard. Larger networks have a greater representational capability, and maybe you need it.
More layers offer more opportunity for hierarchical re-composition of abstract features learned from the data. Maybe you need that.
Later networks need more training, both in epochs and in learning rate. Adjust accordingly.
These links will give you lots of ideas of things to try, well they do for me.
The batch size defines the gradient and how often to update weights. An epoch is the entire training data exposed to the network, batch-by-batch.
Have you experimented with different batch sizes and number of epochs?
Above, we have commented on the relationship between learning rate, network size and epochs.
Small batch sizes with large epoch size and a large number of training epochs are common in modern deep learning implementations.
This may or may not hold with your problem. Gather evidence and see.
Try batch size equal to training data size, memory depending (batch learning).
Try a grid search of different mini-batch sizes (8, 16, 32, ).
Try training for a few epochs and for a heck of a lot of epochs.
Consider a near infinite number of epochs and setup check-pointing to capture the best performing model seen so far, see more on this further down.
Some network architectures are more sensitive than others to batch size. I see Multilayer Perceptrons as often robust to batch size, whereas LSTM and CNNs quite sensitive, but that is just anecdotal.
Intuitively, how does mini-batch size affect the performance of (stochastic) gradient descent?
Regularization is a great approach to curb overfitting the training data.
The hot new regularization technique is dropout, have you tried it?
Dropout randomly skips neurons during training, forcing others in the layer to pick up the slack. Simple and effective. Start with dropout.
There are extensions on the dropout idea that you can also play with like drop connect.
Also consider other more traditional neural network regularization techniques , such as:
Experiment with the different aspects that can be penalized and with the different types of penalties that can be applied (L1, L2, both).
It used to be stochastic gradient descent, but now there are a ton of optimizers.
Stochastic Gradient Descent is the default. Get the most out of it first, with different learning rates, momentum and learning rate schedules.
Many of the more advanced optimization methods offer more parameters, more complexity and faster convergence. This is good and bad, depending on your problem.
To get the most out of a given method, you really need to dive into the meaning of each parameter, then grid search different values for your problem. Hard. Time Consuming. It might payoff.
I have found that newer/popular methods can converge a lot faster and give a quick idea of the capability of a given network topology, for example:
You can also explore other optimization algorithms such as the more traditional (Levenberg-Marquardt) and the less so (genetic algorithms). Other methods can offer good starting places for SGD and friends to refine.
The loss function to be optimized might be tightly related to the problem you are trying to solve.
Nevertheless, you often have some leeway (MSE and MAE for regression, etc.) and you might get a small bump by swapping out the loss function on your problem. This too may be related to the scale of your input data and activation functions that are being used.
This can save a lot of time, and may even allow you to use more elaborate resampling methods to evaluate the performance of your model.
Early stopping is a type of regularization to curb overfitting of the training data and requires that you monitor the performance of the model on training and a held validation datasets, each epoch.
Once performance on the validation dataset starts to degrade, training can stop.
You can also setup checkpoints to save the model if this condition is met (measuring loss of accuracy), and allow the model to keep learning.
Checkpointing allows you to do early stopping without the stopping, giving you a few models to choose from at the end of a run.
After algorithm tuning, this is the next big area for improvement.
In fact, you can often get good performance from combining the predictions from multiple good enough models rather than from multiple highly tuned (and fragile) models.
Well take a look at three general areas of ensembles you may want to consider:
If you have multiple different deep learning models, each that performs well on the problem, combine their predictions by taking the mean.
The more different the models, the better. For example, you could use very different network topologies or different techniques.
The ensemble prediction will be more robust if each model is skillful but in different ways.
Each time you train the network, you initialize it with different weights and it converges to a different set of final weights. Repeat this process many times to create many networks, then combine the predictions of these networks.
Their predictions will be highly correlated, but it might give you a small bump on those patterns that are harder to predict.
As above, but train each network on a different view or framing of your problem.
Again, the objective is to have models that are skillful, but in different ways (e.g. uncorrelated predictions).
You can lean on the very different scaling and transform techniques listed above in the Data section for ideas.
The more different the transforms and framing of the problem used to train the different models, the more likely your results will improve.
Using a simple mean of predictions would be a good start.
You can also learn how to best combine the predictions from multiple models.
Often you can get better results over that of a mean of the predictions using simple linear methods like regularized regression that learns how to weight the predictions from different models.
Baseline reuslts using the mean of the predictions from the submodels, but lift performance with learned weightings of the models.
Theres a lot of good resources, but few tie all the ideas together.
Ill list some resources and related posts that you may find interesting if you want to dive deeper.
How to Grid Search Hyperparameters for Deep Learning Models in Python With Keras
This is a big post and weve covered a lot of ground.
You do not need to do everything. You just need one good idea to get a lift in performance.
Did you get that one idea or method that made a difference?
Dr. Jason Brownlee is a husband, proud father, academic researcher, author, professional developer and a machine learning practitioner. He is dedicated to helping developers get started and get good at applied machine learning.
Generally, deep learning is empirical. We dont have good theory for why large networks work or even what the heck is going on inside.
How many layers? What learning rate? These are problems that can only be solved empirically, not analytically. For now.
A strong math theory could push back the empirical side/voodoo and improve understanding.
I think it is very helpful, so Id like to share the idea with my Japanese followers.
Could I ask for permission to post my summary in http://qiita.com/daisukelab ?
Maybe you can constrain the dataset anyway, take a sample and use that for all model development.
If we use smaller subset of dataset, we could use the subset for completing model development to the end?
My that comment I meant that working with a sample of your data, rather than all of the data has benefits  like increasing the speed of turning around models.
Im currently working on implementing some nlp for regressions and was wondering if I could improve my results. Ill try ensembles, as I have many models already trained. 
This is really good information what I found. Actually, I am working in Semantic Segmentation using Deep learning. It can view as an extension task of recognition task. As I read, I felt that all segmentation techniques have come from recognition (We can think that the recognition as encoding phase provides probability map, the segmentation task maps the probability maps to the image by using decode phase). Hence my opinion, I think that if any state-of-the-art recognition network architecture applies for segmentation task which can achieve more accuracy than segmentation using older recognition network architecture. Do you think so? What is good direction to improve segmentation accuracy? Thank you in advance
I am a newbie in deep learning and experimenting with existing examples, using the digits interface. Thank you for all the effort to simplify the topic, a technical documentation still well understandable for newcomers.
My interest is on detecting (and counting) particles via deep learning. After many many experiments with various samples, I realised particles too close to each (almost touching) other are counted as one, while there is a clear seperation, to my eye.
I am looking for an approach in how to handle this.
I tried some thresholding techniques on individual images in an image processing software and best results obtained with color thresholding. Although I have no idea whether thresholding is a part of particle detection in object detection processes by deep learning, but wondering if is it possible to integrate an equivalent process into object detection models ?(currently using the detectnet model)
-If you can’t reasonably get more data, you can invent more data.
-If your data are vectors of numbers, create randomly modified versions of existing vectors.
-> Have you an example how to create randomly modified versions of existing vectors.?
I dont but you could experiment with different perturbation methods to see what works best.
 randomly replace a subset of values with randomly selected values in the data population
 add a small random value (select distribution to meet the data distribution for a column)
It is tricky, because you need the new data to be reasonable for the assigned class. 
First of all thank you for the thorough explanation and rich material, its been helping me quite a lot.
One thing that still troubles me is applying Levenberg-Marquardt in Python, more specifically in Keras. Im dealing with non-ideal input variables to infer target and would like to go through a range of optimizers to test the network performance.
Since one of the best available in Matlab is Levenberg-Marquardt, it would very good (and provide comparison value between languages) if I could accurately apply it in keras to train my network.
I am currently using Keras with Theano backend. Any help at all would be appreciated.
Sorry, I do not have an example of the Levenberg-Marquardt algorithm in Python for Keras.
I have got a question- after improving deep learning performance in my project i achieve accuracy of 75% in a binary classification problem. Using other methods gives me in the best shot 77%. My question is when do i know that my model is the best possbile? Is there any measure that can explain to which extend my data has explanatory power?
Great question. We can never know for sure, but only when we run out of time or ideas.
Generally, no, Im not aware of methods to estimate explanatory power, Im not even clear what that might mean.
What I meant by explanatory power, was the ability of data to distinguish that one record belongs to class 1, second to class 2, third again to class 1 and so on.
It is some kind of limitation of the dataset, that it can achieve max. accuracy for example at 80% and my question was is it a way to measure, that level.
 Thank you for your answer, now i am ready to accept my model 🙂
Hi, Im working on my final year project which is detecting nsfw content in images and further for videos (if possible). My problem is that i cannot find any dataset for working so if you could please help me out with this problem by giving me some suggestions will really help me in this project.I need data of about (150-200 gb) to make my algorithm more precise. I have reached out to yahoo open nsfw team but there is no response from them.
Hey Jason, Do you know of any empirical evidence for the Why Deep Learning? slide by Andrew Ng. Specifically I am working on a text classification problem, I am finding BoW + (Linear SVMs or Logistic Regression) giving me the best performance (which is what I find in the literature at least pre 2015). Im fairly new to Deep learning, I have been testing it out on my problem having seen stuff like https://arxiv.org/abs/1408.5882 perform well in the Rotten Tomatoes Kaggle text classification problem. Although there was some other fancy tricks that I think gave the CNN extra juice. Do you have any recommendations or any benchmarking studies in this area that demonstrate what Andrew Ng is claiming?
(Hyperbolic Tangent (tanh), rescale to values between -1 and 1 )
Finalllllllllly! someone who has explain this wonderfully with structure, and not just said its a black box!
I have a question! do you have any pointers for unbalanced data?
is it better to sacrifice other data to balance every class out?
A long try-and-test process led me to the same conclusions. So thank you very much! This post will serve for a lot of new comers to the keras/ deep learning area.
In a classic case, you normalize your data, you train the model and then you de-normalize (inverse using the scaler). Now, imagine that the model you are training is fed with its own output and the predicted outpùt is out of the scaler range, what would you do to improve the models performance.
One thing I am still wondering about, I am interested to apply deep learning in data stream classification (real time prediction), but my concern is the execution time that the deep learning needs. Any idea how to speed it up or how to handle it for real time prediction.
For modestly sized data, the feed-forward part of the neural network (to make predictions) is very fast.
As for training the network in real-time, I would suggest that it is perhaps a bad fit for the problem.
I figure the pictures would lighten the mood, be something interesting to look at as we get deep into technical topics. I often choose pictures based on where I am going or want to go for a holiday, e.g. the beach, the forest, etc. Sometimes they are a pun (e.g. a pic of a gas pipeline for a pipeline method). Sometimes they are just random.
Youre the first person in 4 years to ask about them 🙂
Under Rescale Your Data, you have pointed about scaling and activation function. So if we scale the data between [-1,1], then we have to implicitly mention about activation function (i.e tanh function) in LSTM using Keras. Am i correct in assumption, or Keras will pass tanh activation function default in LSTM.
In https://machinelearningmastery.com/time-series-forecasting-long-short-term-memory-network-python/ artcle , you have mentioned to scale data between -1 to1. In the same article you have not used any activation function. 
In keras documentation, https://keras.io/layers/recurrent/#lstm, default activation is actually linear or no activation (i.e a(x) =x).
So my question is whether i need to add implicit activation function as tanh in LSTM layer.
The default for the LSTM is sigmoid outputs with tanh used for internal gates. I would recommend scaling input data for LSTMs to between [0,1].
