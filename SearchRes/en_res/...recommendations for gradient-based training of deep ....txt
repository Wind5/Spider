Abstract: Learning algorithms related to artificial neural networks and in particular
for Deep Learning may seem to involve many bells and whistles, called
gradient and gradient-based optimization. It also discusses how to deal with
the fact that more interesting results can be obtained when allowing one to
used to successfully and efficiently train and debug large-scale and often deep
multi-layer neural networks. It closes with open questions about the training
