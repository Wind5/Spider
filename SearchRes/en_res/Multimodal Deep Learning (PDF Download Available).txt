 Multimodal Deep LearningConference Paper · January 2011 with 1,091 ReadsSource: DBLPConference: Proceedings of the 28th International Conference on Machine Learning, ICML 2011, Bellevue, Washington, USA, June 28 - July 2, 2011Cite this publication1st Jiquan Ngiam2nd Aditya Khosla+ 23rd Mingyu KimLast Andrew Y. NgShow more authorsAbstractDeep networks have been successfully applied to unsupervised feature learning for single modalities (e.g., text, images or audio). In this work, we propose a novel application of deep networks to learn features over multiple modalities. We present a series of tasks for multimodal learning and show how to train deep networks that learn features to address these tasks. In particular, we demonstrate cross modality feature learning, where better features for one modality (e.g., video) can be learned if multiple modalities (e.g., audio and video) are present at feature learning time. Furthermore, we show how to learn a shared representation between modalities and evaluate it on a unique task, where the classifier is trained with audio-only data but tested with video-only data and vice-versa. Our models are validated on the CUAVE and AVLetters datasets on audio-visual speech classification, demonstrating best published visual speech classification on AVLetters and effective shared representation learning. 1.Discover the worlds research13+ million members100+ million publications700k+ research projectsJoin for free
 Multimodal Deep LearningJiquan Ngiam1, Aditya Khosla1, Mingyu Kim1, Juhan Nam2, Honglak Lee3, Andrew Y. Ng11Computer Science Department, Stanford University{jngiam,aditya86,minkyu89,ang}@cs.stanford.edu2Department of Music, Stanford Universityjuhan@ccrma.stanford.edu3Computer Science & Engineering Division, University of Michigan, Ann Arborhonglak@eecs.umich.eduAbstractDeep networks have been successfully applied to unsupervised feature learning forsingle modalities (e.g., text, images or audio). In this work, we propose a novel ap-plication of deep networks to learn features over multiple modalities. We present aseries of tasks for multimodal learning and show how to train a deep network thatlearns features to address these tasks. In particular, we demonstrate cross modal-ity feature learning, where better features for one modality (e.g., video) can belearned if multiple modalities (e.g., audio and video) are present at feature learn-ing time. Furthermore, we show how to learn a shared representation betweenmodalities and evaluate it on a unique task, where the classiﬁer is trained withaudio-only data but tested with video-only data and vice-versa. We validate ourmethods on the CUAVE and AVLetters datasets with an audio-visual speech clas-siﬁcation task, demonstrating superior visual speech classiﬁcation on AVLettersand effective multimodal fusion.1 IntroductionIn speech recognition, people are known to integrate audio-visual information in order to understandspeech. This was ﬁrst exempliﬁed in the McGurk effect [1] where a visual /ga/ with a voiced /ba/is perceived as /da/ by most subjects. In particular, the visual modality provides information onthe place of articulation [2] and muscle movements which can often help to disambiguate betweenspeech with similar acoustics (e.g., the unvoiced consonants /p/ and /k/). In this paper, we examinemultimodal learning and how to employ deep architectures to learn multimodal representations.Multimodal learning involves relating information from multiple sources. For example, images and3-d depth scans are correlated at ﬁrst-order as depth discontinuities often manifest as strong edgesin images. Conversely, audio and visual data for speech recognition have non-linear correlationsat a “mid-level”, as phonemes or visemes; it is difﬁcult to relate raw pixels to audio waveforms orspectrograms.In this paper, we are interested in modeling “mid-level” relationships, thus we choose to use audio-visual speech classiﬁcation to validate our methods. In particular, we focus on learning representa-tions for speech audio which are coupled with videos of the lips.We will consider the learning settings shown in Figure 1. The overall task can be divided intothree phases – feature learning, supervised training, and testing. We keep the supervised trainingand testing phases ﬁxed and examine different feature learning models with multimodal data. Indetail, we consider three learning settings – multimodal fusion, cross modality learning, and sharedrepresentation learning.1
 Feature LearningSupervised TrainingTestingClassic Deep Learning Audio Audio AudioVideo Video VideoMultimodal Fusion Audio + Video Audio + VideoAudio + VideoCross Modality Learning Audio + Video Audio AudioAudio + Video Video VideoShared Representation LearningAudio + Video Audio VideoAudio + Video Video AudioFigure 1: Multimodal Learning Settings.For the multimodal fusion setting, data from all modalities is available at all phases; this representsthe typical setting considered in most prior work in audio-visual speech recognition [3]. In crossmodality learning, one has access to data from multiple modalities only during feature learning.During the supervised training and testing phase, only data from a single modality is provided. Inthis setting, the aim is to learn better single modality representations given unlabeled data from mul-tiple modalities. Last, we consider a shared representation learning setting, which is unique in thatdifferent modalities are presented for supervised training and testing. This setting allows us to eval-uate if the feature representations can capture correlations across different modalities. Speciﬁcally,studying this setting allows us to assess whether the learned representations are modality-invariant.In the following sections, we ﬁrst describe the building blocks of our model. We then presentdifferent multimodal learning models leading to a deep network that is able to perform the variousmultimodal learning tasks. Finally, we report experimental results and conclude.2 BackgroundThe multimodal learning settings we consider can be viewed as a special case of self-taught learning[4]. The self-taught learning paradigm uses unlabeled data (not necessarily from the same distri-bution as the labeled data) to learn representations that improve performance on some task. Whileself-taught learning was ﬁrst motivated with sparse coding, recent work on deep learning [5, 6, 7]have examined how deep sigmoidal networks can be trained to produce useful representations forhandwritten digits and text. The key idea is to use greedy layer-wise training with Restricted Boltz-mann Machines (RBMs) followed by ﬁne-tuning. We use an extension of RBMs with sparsity [8],which have been shown to be able to learn meaningful features for digits and natural images. Inthe next section, we review the sparse RBM, which we use as a layer-wise building block for ourmodels.2.1 Sparse restricted Boltzmann machinesWe ﬁrst describe the restricted Boltzmann machine (RBM) [5, 6] followed by the sparsity regular-ization method [8]. The RBM is an undirected graphical model with hidden variables (h) and visiblevariables (v). There are symmetric connections between the hidden and visible variables (wi,j ), butno connections between hidden variables or visible variables. This particular conﬁguration makes iteasy to compute the conditional probability distributions, when vor his ﬁxed (Equation 2).−log P(v,h)∝E(v,h) = 12σ2Xiv2i−1σ2Xicivi+Xjbjhj+Xi,jvihjwi,j (1)p(hj|v) = sigmoid(1σ2(bj+wTjv)) (2)Equation 1 gives the negative log-probability of a RBM while Equation 2 gives the posteriorsof the hidden variables given the visible variables. This formulation models the visible vari-ables as real-valued units and the hidden variables as binary units.1As it is intractable to com-pute the gradient of the log-likelihood term, we learn the parameters of the model (wi,j , bj, ci)1We use Gaussian visible units for the RBM that is connected to the input data. When training the deeperlayers, we use binary visible units.2
 using contrastive divergence [9]. To regularize the model for sparsity, we encourage each hid-den unit to have a pre-determined expected activation using a regularization penalty of the formλPj(ρ−1m(Pmk=1 E[hj|vk]))2, where {v1, ..., vm}is the training set and ρdetermines the sparse-ness of the hidden units.3 Learning architectures......Hidden UnitsVisible Units(a) Standard RBM…......Audio InputShared Representation...Video Input(b) Shallow RBM......... ......Audio Input Video InputDeep Hidden Layer(c) Deep RBMFigure 2: RBM Pretraining Models. We train (a) for audio and video separately as abaseline. The shallow model (b) is limited and we ﬁnd that this model is unable tocapture correlations across the modalities. The deep model (c) is trained in a greedylayer-wise fashion by ﬁrst training two separate (a) models. We later “unroll” the deepmodel (c) to train the deep autoencoder models presented in Figure 3.In this section, we describe our models for the task of audio-visual bimodal feature learning, wherethe audio and visual input to the model are windows of audio (spectrogram) and video frames.To motivate our deep autoencoder [5] model, we ﬁrst describe several simple models and theirdrawbacks.One of the most straightforward approaches to feature learning is to train a RBM model separatelyfor audio and video (Figure 2a). After learning the RBM, the posteriors of the hidden variablesgiven the visible variables (Equation 2) can then be used as a new representation for the data. Weuse this model as a baseline to compare the results of our multimodal learning models, as well as forpre-training the deep networks.To train a multimodal model, an direct approach is to train a RBM over the concatenated audioand video data (Figure 2b). While this approach jointly models the distribution of the audio andvideo data, it is limited as a shallow model. In particular, since the correlations between the audioand video data are highly non-linear, it is hard for a RBM to learn these correlations and formmultimodal representations.Therefore, we consider greedily training a RBM over the pre-trained layers for each modality, asmotivated by deep learning methods (Figure 2c). In particular, the posteriors (Equation 2) of the ﬁrstlayer hidden variables are used as the training data for the new layer. By essentially representing thedata through learned ﬁrst layer representations, it can be easier for the model to learn the higher-ordercorrelations across the modalities. Intuitively, the ﬁrst layer representations correspond to phonemesand visemes (lip pose and motions) and the second layer models the relationships between them.However, there are still two issues with the above multimodal models. First, there is no explicitobjective for the models to discover correlations across the modalities. It is possible for the model toﬁnd representations such that some hidden units are tuned only for audio while others are tuned onlyfor video. Second, the models are clumsy to use in a cross modality learning setting where only onemodality is present during supervised training and testing time. To use the RBM models presentedabove with only a single modality present, one would need to integrate out the other unobservedvisible variables to perform inference.Thus, we propose an autoencoder-based model that resolves both issues for the cross modality learn-ing setting. The deep autoencoder (Figure 3a) is trained to reconstruct both modalities when givenonly video data. We initialize the deep autoencoder with the deep RBM weights (Figure 2c) basedon Equation 2, discarding any weights that are no longer present due to the network’s conﬁguration.The middle layer is used as the new feature representation. This model can be viewed as an instanceof multitask learning [10].We use the deep autoencoder (Figure 3a) models in settings where only a single modality is presentat supervised training and testing. On the other hand, when multiple modalities are available at3
 ............... ......Video InputHiddenUnitsAudio Reconstruction Video Reconstruction(a) Video-Only Deep Autoencoder......... ............ ......Audio Input Video InputSharedRepresentationAudio Reconstruction Video Reconstruction(b) Bimodal Deep AutoencoderFigure 3: Deep Autoencoder Models. A “video-only” model is shown in (a) where themodel learns to reconstruct both modalities given only video as the input. A similarmodel can be drawn for the “audio-only” setting. We train the (b) bimodal deepautoencoder in a denoising fashion, using an augmented dataset with examples thatrequire the network to reconstruct both modalities given only one. Both models arepre-trained using sparse RBMs (Figure 2c). Since we use a sigmoid transfer functionin the deep network, we can initialize the network using the conditional probabilitydistributions p(h|v)and p(v|h)of the learned RBM.task time, it is less clear how to use the model as one would need to train a deep autoencoder foreach modality. One straightforward solution is to train the networks such that the decoding weightsare tied. However, such an approach does not scale well – if we were to allow any combinationof modalities to be present or absent at test time, we will need to train an exponential number ofmodels. Instead, we propose a training method inspired by denoising autoencoders [11].We propose training the deep autoencoder network (Figure 3b) using an augmented dataset withadditional examples that have only a single-modality as input. In practice, we add examples thatzero out one of the input modalities (e.g., video) and only have the other input modality (e.g., audio)available, yet still requiring the network to reconstruct both modalities (audio and video). Thus,one-third of the training data has only video for input, while another one-third of the data has onlyaudio for input, and the last one-third of the data has both audio and video for input.Due to initialization using sparse RBMs, we ﬁnd that the hidden units have low expected activationeven after the deep autoencoder training. Therefore, when one of the modalities is set to zero, theﬁrst layer representations are close to zero. In this case, we are essentially training a modality-speciﬁc deep autoencoder network (Figure 3a). Effectively, the method learns a model which isrobust to missing modalities.4 ExperimentsWe evaluate our methods on audio-visual speech classiﬁcation of isolated letters and digits. Thesparseness parameter ρwas chosen using cross-validation, while all other parameters (includinghidden layer size and weight regularization) were kept ﬁxed.24.1 Data PreprocessingWe represent the audio signal using its spectrogram3with temporal derivatives, resulting in a 483dimension vector which was reduced to 100 dimensions with PCA whitening. A window of 10contiguous audio frames was used as the input to our models.2We cross-validated ρover {0.01,0.03,0.05,0.07}. The ﬁrst layer features was 4x overcomplete for video(1536 units) and 1.5x overcomplete for audio (1500 units). The second layer had 4554 units.3Each spectrogram frame (161 frequency bins) had a 20ms window with 10ms overlaps.4
 For the video, we preprocessed the frames so as to extract only the region-of-interest (ROI) en-compassing the mouth.4Each mouth ROI was rescaled to 60x80 pixels and further reduced to 32dimensions,5using PCA whitening. Temporal derivatives were computed over the reduced vector.We use windows of 4 contiguous video frames for input since this had approximately the sameduration as 10 audio frames.For both modalities, we also performed feature mean normalization over time [3], akin to removingthe DC component from each example. We also note that adding temporal derivatives to the repre-sentations has been widely used in the literature as it helps to model dynamic speech information[3, 14]. The temporal derivatives were computed using a normalized linear slope so that the dynamicrange of the derivative features are comparable to the original signal.4.2 Datasets and TaskSince only unlabeled data was required for unsupervised feature learning, we combined diversedatasets to learn features. We used all the datasets for feature learning. AVLetters and CUAVE werefurther used for supervised classiﬁcation. We ensured that no test data was used for unsupervisedfeature learning.CUAVE [15]. 36 individuals saying the digits 0to 9. We used the normal portion of the datasetwhere each speaker was frontal facing and spoke each digit 5 times. We evaluated digit classiﬁcationon the CUAVE dataset in a speaker independent setting. As there has not been a ﬁxed protocolfor evaluation on this dataset, we chose to use odd-numbered speakers for the test set and even-numbered ones for the training set.AVLetters [16]. 10 speakers saying the letters Ato Z, three times each. The dataset provided pre-extracted lip regions at 60x80 pixels. As we were not able to obtain the raw audio information forthis dataset, we used it for evaluation on a visual-only lipreading task. We report results on thethird-test settings used by [14, 16] for comparisons.AVLetters2 [17]. 5 speakers saying the letters Ato Z, seven times each. This is a new high deﬁnitionversion of the AVLetters dataset. We used this dataset for unsupervised training only.Stanford Dataset. 23 volunteers spoke the digits 0to 9, letters Ato Zand selected sentences fromthe TIMIT dataset. We collected this data in a similar fashion to the CUAVE dataset and used forunsupervised training only.TIMIT. We used the TIMIT [18] dataset for unsupervised audio feature pre-training.We note that in all datasets there is variability in the lips in terms of appearance, orientation and size.Our features were evaluated on speech classiﬁcation of isolated letters and digits. We extracted fea-tures from overlapping windows. Since examples had varying durations, we divided each exampleinto Sequal slices and performed average-pooling over each slice. The features from all slices weresubsequently concatenated together. We combined features using S= 1 and S= 3 to form our ﬁnalfeature representation for classiﬁcation using a linear SVM.4.3 Cross Modality LearningWe ﬁrst evaluate the learned features in a setting where unlabeled data for both modalities are avail-able during feature learning, while during supervised training and testing phases only a single modal-ity is presented. In these experiments, we evaluate cross modality learning where one learns betterrepresentations for one modality (e.g., video) when given multiple modalities (e.g., audio and video)during feature learning. For the bimodal deep autoencoder, we set the value of the other modality tozero when computing the shared representation which is consistent with the feature learning phase.All deep autoencoder models are trained with all available unlabeled audio and video data.On the AVLetters dataset (Table 1a), there is an improvement over hand-engineered features fromprior work. The deep autoencoder models performed the best on the dataset, obtaining a classiﬁca-tion score of 65.8%, outperforming the best previous published results.4We used an off-the-shelf object detector [12] with median ﬁltering over time to extract the mouth regions.5Similar to [13] we found that 32 dimensions were sufﬁcient and performed well.5
 Feature Representation AccuracyBaseline Preprocessed Video 46.2%RBM Video 53.1%Bimodal Deep Autoencoder 59.2%Video-Only Deep Autoencoder 65.8%Multiscale Spatial Analysis [16] 44.6%Local Binary Pattern [14] 58.9%(a) AVLettersFeature Representation AccuracyBaseline Video 58.5%RBM Video 65.5%Bimodal Deep Autoencoder 66.7%Video-Only Deep Autoencoder 69.7%Discrete Cosine Transform [19] 64% †§Active Appearence Model [20] 75.7% †Active Appearence Model [21] 68.7% †Fused Holistic+Patch [22] 77.1% †Visemic AAM[23] 83% †§(b) CUAVE VideoTable 1: Classiﬁcation performance for visual speech classiﬁcation on (a) AVLetters and (b)CUAVE. Learning sparse RBM features improve performance. The deep autoencoders performthe best and show effective cross modality learning. §These results consider continuous speechrecognition, although the normal portion of CUAVE consists of speakers saying isolated digits.†These models use a visual front-end system that is signiﬁcantly more complicated than oursand a different train/test split.On the CUAVE dataset (Table 1b), there is an improvement by learning video features with bothvideo audio compared to learning features with only video data. The deep autoencoder modelsultimately performs the best, obtaining a classiﬁcation score of 69.7%. In our model, we chose touse a very simple front-end that only extracts bounding boxes (without any correction for orientationor perspective changes). A more sophisticated visual front-end in conjunction with our models hasthe potential to do even better.The video classiﬁcation results show that the deep autoencoder model achieves cross modality learn-ing by discovering better video representations when given additional audio data. In particular, eventhough the AVLetters dataset did not have any audio data, we were able to obtain better performanceby learning better video features using other unlabeled data sources which had both audio and videodata.However, we also note that cross modality learning did not help to learn better audio features; sinceour feature learning mechanism is unsupervised, we ﬁnd that our model learns features that adapt tothe video modality but are not useful for speech classiﬁcation.4.4 Multimodal Fusion ResultsAlthough using audio information alone performs reasonably well for speech recognition, fusingaudio and visual information can substantially improve performance, especially when the audio isdegraded with noise [19, 20, 21, 23]. Hence, we evaluate our models in both clean and noisy audiosettings.Accuracy AccuracyFeature Representation (Clean Audio) (Noisy Audio)(a) Best Audio-Only 95.8% 79.6%(b) Best Video-Only 69.7% 69.7%(c) Bimodal Deep Autoencoder 90.0% 77.6%(d) Best-Video + Best-Audio 87.0% 75.5%(e) Bimodal + Best-Audio 94.4% 81.6%Table 2: Digit classiﬁcation performance for bimodal speech classiﬁcation on CUAVE, underclean and noisy conditions. We added white Gaussian noise to the original audio signal at 0dbSNR. Best audio refers to the best audio features we learned (single layer RBM for audio). Bestvideo refers to the video-only deep autoencoder features (Table 1b).The video modality complements the audio modality by providing information such as place ofarticulation that can help distinguish between similar sounding speech. However, when one simplyconcatenates audio and visual features (Table 2(d)), it is often the case that performance is worseas compared to using only audio features. Since our models are able to learn multimodal features6
 that go beyond simply concatenating the audio and visual features, we propose combining the audiofeatures with our multimodal features. When the best audio features are concatenated together withthe bimodal features (Table 2(e)), we achieve an increase in accuracy in the noisy setting. Thisshows that the learned multimodal features are better able to complement the audio features.4.5 Shared Representation LearningSupervisedTestingAudioSharedRepresentationVideo AudioSharedRepresentationVideoLinear ClassifierTraining TestingTrain/Test Setting AccuracyAudio/Video “Hearing to see” 29.4%Video/Audio “Seeing to hear” 27.5%Table 3: Shared Representation Learning onCUAVE. The diagram (above) depicts the Au-dio/Video “Hearing to see” setting.While the above results show that we havelearned useful features for video and audio, itdoes not yet show that the model captures cor-relations across the modalities. In this experi-ment, we assess if multimodal features indeedform a shared representation that has some in-variance to audio or video inputs. During super-vised training, we provide the algorithm datasolely from one modality (e.g., audio) and latertested only on the other modality (e.g., video).In essence, we are telling the supervised learnerhow the digits “1”, “2”, etc. sound like and ask-ing it to ﬁgure out how to visually recognizedigits – “hearing to see” (Table 3). If our modelindeed learns a shared representation that hassome invariance to the presented modality, itwill be able to perform this task well.On the “hearing to see” task, the deep autoencoder obtains an accuracy of 29.4%, while simplebaselines perform at chance (10%). Similarly, on the “seeing to hear” task, the model obtains 27.5%.This shows that our learned shared representation is partially invariant to the input modality.4.6 Visualization of learned featuresBy visualizing our features, we found that the visual bases captured lip motions and articulations.In particular, the learned features include different mouth articulations, opening and closing of themouth, exposing teeth, among others. We present some visualizations of the learned features inFigure 4.Figure 4: Visualization of Learned Representations. These ﬁgures correspond to two deephidden units, where we visualize the most strongly connected ﬁrst layer features. The units arepresented in audio-visual pairs (we have found it generally difﬁcult to interpret the connectionbetween the pair).4.7 McGurk effectThe McGurk effect [1] refers to an audio-visual perception phenomenon where a visual /ga/ with aaudio /ba/ is perceived as /da/ by most subjects. Since our model learns a multimodal representation,it would be interesting to see if the model was able to replicate a similar effect. We obtained datafrom 23 volunteers speaking 5 repetitions of /ga/,/ba/ and /da/.Audio / Visual Model predictionSetting /ga/ /ba/ /da/Visual /ga/, Audio /ga/ 82.6% 2.2% 15.2%Visual /ba/, Audio /ba/ 4.4% 89.1% 6.5%Visual /ga/, Audio /ba/ 28.3% 13.0% 58.7%Table 4: McGurk EffectUsing the learned bimodal deep autoen-coder features, we trained a linear SVMon a 3-way classiﬁcation task. The modelwas tested on three conditions that simu-late the McGurk effect. When the visualand audio data matched at test time, themodel was able to predict the correct class7
 /ba/ and /ga/ with an accuracy of 82.6% and 89.1% respectively. On the other hand, when a vi-sual /ga/ with a voiced /ba/ was mixed at test time, the model was most likely to predict /da/, eventhough /da/ neither appears in the visual or audio inputs. This is consistent with the McGurk effecton people.4.8 Additional Control ExperimentsRecall that we trained the bimodal deep autoencoder with two-thirds of data having one modal-ity missing. To evaluate the role of such a training scheme, we performed a control experimentwhere we trained the bimodal deep autoencoder without removing any of the modalities. In thisexperiment, we found that training without any missing data resulted in inferior performance.6Byinspecting the models, we found that training without missing data led to more modality speciﬁcunits in the shared representation layer. Conversely, the model trained with the data with missingmodalities had more connections to both modalities in the shared representation layer. This supportsthe hypothesis that having training data with missing modalities is required for the model to learn ashared representation and show cross modality learning.To evaluate whether a deep architecture is needed or a shallow one would sufﬁce, we trained abimodal shallow model by training a sparse RBM over the concatenated audio and video data (Figure2b). However, the correlations between the audio and video modality are highly non-linear and noteasily captured by a shallow model. As a result, we ﬁnd that the model learns largely separate audioand video features. In particular, we ﬁnd hidden units that have strong connections to variablesfrom either modality but few units that connect across the modalities. Thus, the shallow model iseffectively learning two separate representations.5 Related WorkWhile we present special cases of neural networks here for multimodal learning, we note that priorwork on audio-visual speech recognition [13, 24, 25] has also explored the use of neural networks.Yuhas et al. [24] trained a neural network to predict the auditory signal given the visual input. Theyshowed improved performance in a noisy setting when they combined the predicted auditory signal(from the network using visual input) with a noisy auditory signal. Duchnowski et al. [13, 25] trainedseparate networks to model phonemes and visemes and combined the predictions at a phonetic layerto predict the spoken phoneme. They also attempted combining the representations using the hiddenlayer from each modality.In contrast to these approaches, we explicitly use the hidden units to build a new representation ofour data. Furthermore, we do not explicitly model phonemes or visemes, which require expensivelabeling efforts. Finally, we build deep bimodal representations by modeling the correlations acrossthe learned shallow representations.6 ConclusionHand-engineering task-speciﬁc features is often difﬁcult and time consuming. For example, it is notimmediately clear what the appropriate features should be for lipreading with visual only data. Thisdifﬁculty is more pronounced with multimodal data as the features have to relate multiple disparatedata sources. In this paper, we employed deep learning architectures to learn multimodal featuresfrom unlabeled data and also to improve single modality features through cross modality learning.AcknowledgmentsWe thank Clemson University for providing the CUAVE dataset and University of Surrey for pro-viding the AVLetters2 dataset. We also thank Quoc Le, Andrew Saxe, Andrew Maas, and AdamCoates for insightful discussions, and the anonymous reviewers for helpful comments. This work issupported by the DARPA Deep Learning program under contract number FA8650-10-C-7020.6Performance of bimodal deep autoencoder without augmented dataset: Video-only tasks (Table 1) - 50.4%on AVLetters1 and 62.1% on CUAVE. “Hearing to see” and “Seeing to hear” tasks - at chance.8
 References[1] H. McGurk and J. MacDonald. Hearing lips and seeing voices. Nature, 264(5588):746–748, 1976.[2] Q. Summerﬁeld. Lipreading and audio-visual speech perception. Trans. R. Soc. Lond., pages 71–78,1992.[3] G. Potamianos, C. Neti, J. Luettin, and I. Matthews. Audio-visual automatic speech recognition: Anoverview. In Issues in Visual and Audio-Visual Speech Processing. MIT Press, 2004.[4] R. Raina, A. Battle, H. Lee, and B. Packer. Self-taught learning: Transfer learning from unlabeled data.In ICML, pages 759–766, 2007.[5] G. Hinton and R. Salakhutdinov. Reducing the dimensionality of data with neural networks. Science,313(5786):504, 2006.[6] G. Hinton, S. Osindero, and Y.W. Teh. A fast learning algorithm for deep belief nets. Neural Computation,18(7):1527–1554, 2006.[7] R. Salakhutdinov and G. Hinton. Semantic hashing. IJAR, 50(7):969–978, 2009.[8] H. Lee, C. Ekanadham, and A. Ng. Sparse deep belief net model for visual area V2. In NIPS, 2007.[9] G. Hinton. Training products of experts by minimizing contrastive divergence. Neural Computation,2002.[10] R. Caruana. Multitask learning. Machine Learning, 28(1):41–75, 1997.[11] P. Vincent, H. Larochelle, Y. Bengio, and P.A. Manzagol. Extracting and composing robust features withdenoising autoencoders. In ICML, pages 1096–1103. ACM, 2008.[12] N. Dalal and B. Triggs. Histograms of Oriented Gradients for Human Detection. In CVPR, pages 886–893. IEEE, 2005.[13] P. Duchnowski, U. Meier, and A. Waibel. See me, hear me: Integrating automatic speech recognition andlip-reading. In ICSLP, pages 547–550, 1994.[14] G. Zhao and M. Barnard. Lipreading with local spatiotemporal descriptors. IEEE Transactions on Multi-media, 11(7):1254–1265, 2009.[15] E. Patterson, S. Gurbuz, Z. Tufekci, and J. Gowdy. CUAVE: A new audio-visual database for multimodalhuman-computer interface research. In ICASSP, volume 2. Citeseer, 2002.[16] I. Matthews, T.F. Cootes, J.A. Bangham, and S. Cox. Extraction of visual features for lipreading. PAMI,24, 2002.[17] S. Cox, R. Harvey, Y. Lan, and J. Newman. The challenge of multispeaker lip-reading. In InternationalConference on Auditory-Visual Speech Processing, pages 179–184, 2008.[18] W. Fisher, G. Doddington, and Goudie Marshall. The DARPA speech recognition research database:Speciﬁcation and status. In DARPA Speech Recognition Workshop, 1986.[19] M. Gurban and J.P. Thiran. Information theoretic feature extraction for audio-visual speech recognition.IEEE Transactions on Signal Processing, 57(12):4765–4776, 2009.[20] G. Papandreou, A. Katsamanis, V. Pitsikalis, and P. Maragos. Multimodal fusion and learning withuncertain features applied to audiovisual speech recognition. In MMSP, 2007.[21] V. Pitsikalis, A. Katsamanis, G. Papandreou, and P. Maragos. Adaptive multimodal fusion by uncertaintycompensation. In ICSLP, pages 2458–2461, 2006.[22] P. Lucey and S. Sridharan. Patch-based representation of visual speech. In HCSNet Workshop on the Useof Vision in Human-Computer Interaction, 2006.[23] G. Papandreou, A. Katsamanis, V. Pitsikalis, and P. Maragos. Adaptive multimodal fusion by uncertaintycompensation with application to audiovisual speech recognition. IEEE Transactions on Audio, Speech,and Language Processing, 17(3):423–435, 2009.[24] B. P. Yuhas, M. H. Goldstein, and T. J. Sejnowski. Integration of acoustic and visual speech signals usingneural networks. IEEE Communications Magazine, 1989.[25] U. Meier, W. H¨urst, and P. Duchnowski. Adaptive Bimodal Sensor Fusion For Automatic Speechreading.In ICASSP, pages 833–836, 1996.9
 CitationsCitations371ReferencesReferences25For instance, Ngiam et. al.[2]proposed a deep learning framework using Restricted Boltzmann Machines[3]and deep belief networks[4]to learn efficient features of audio and video modalities. They further illustrated that multi-modal learning results in better performance as compared to the unimodal case. Text to Image Generative Model Using Constrained Embedding Space Mapping[Show abstract] [Hide abstract] ABSTRACT: We present a conditional generative method that maps low- dimensional embeddings of image and natural language to a common latent space hence extracting semantic relationships between them. The embedding specific to a modality is first extracted and subsequently a constrained optimization procedure is performed to project the two embedding spaces to a common manifold. Based on this, we present a method to learn the conditional probability distribution of the two em- bedding spaces; first, by mapping them to a shared latent space and generating back the individual embeddings from this common space. However, in order to enable independent conditional inference for separately extracting the cor- responding embeddings from the common latent space representation, we deploy a proxy variable trick - wherein, the single shared latent space is replaced by two separate latent spaces. We design an objective function, such that, during training we can force these separate spaces to lie close to each other, by minimizing the Euclidean distance between their distribution functions. Experimental results demonstrate that the learned joint model can generalize to learning concepts of double MNIST digits with additional attributes of colors, thereby enabling the generation of specific colored images from the respective text data. Full-text · Conference Paper · Sep 2017 · IEEE Transactions on Computational Intelligence and AI in GamesSubhajit ChaudhurySakyasingha DasguptaAsim Munawar+1 more author...Md. A. Salam KhanRead full-textTherefore, HDA and HTL can strongly benefit from multi-view learning techniques such as canonical correlation analyses [110], co-training [34], spectral embedding [203], multiple kernel learning [71]. Multi-view intermediate domains are also often exploited to define for example cross-modal similarities [3, 121], semantic [246, 174, 87, 256] or multi-view embedding [110, 153, 196, 104, 25, 238] especially used to perform cross-modal image retrieval. In the same spirit, webly supervised approaches [82, 242, 189, 14, 62, 37, 93] are also related to DA and HDA as is these approaches rely on collected Web data (source) data used to refine the target model. Domain Adaptation for Visual Applications: A Comprehensive Survey[Show abstract] [Hide abstract] ABSTRACT: The aim of this paper is to give an overview of domain adaptation and transfer learning with a specific view on visual applications. After a general motivation, we first position domain adaptation in the larger transfer learning problem. Second, we try to address and analyze briefly the state-of-the-art methods for different types of scenarios, first describing the historical shallow methods, addressing both the homogeneous and the heterogeneous domain adaptation methods. Third, we discuss the effect of the success of deep convolutional architectures which led to new type of domain adaptation methods that integrate the adaptation within the deep architecture. Fourth, we overview the methods that go beyond image categorization, such as object detection or image segmentation, video analyses or learning visual attributes. Finally, we conclude the paper with a section where we relate domain adaptation to other machine learning solutions. Full-text · Chapter · Sep 2017 · IEEE Transactions on Computational Intelligence and AI in GamesGabriela CsurkaRead full-textThe serious problem of expert knowledge is that it can take a long time to deliver results and it cannot be completely automated. In the field of feature engineering, expert knowledge can be replaced by using deep learning[16,12]or random forests[8,2]. To overcome the need of expert knowledge in method selection, a more robust forecasting method compared to the classical forecaster is needed. Telescope: A Hybrid Forecast Method for Univariate Time Series Full-text · Conference Paper · Sep 2017 · IEEE Transactions on Computational Intelligence and AI in GamesMarwin ZüfleAndré BauerNikolas Herbst+1 more author...Valentin CurtefRead full-textThe emotion label refers to what expression was requested rather than what may actually have been performed. Inspired by the work in[63]we propose the training of the fusion model using an augmented noisy dataset with additional samples that have only a single-modality as input. In practice, we added samples that have neutral state for one of the input modalities (e.g., face) and original values for the other input modality (e.g., body). Multimodal Student Engagement Recognition in Prosocial Games Full-text · Article · Aug 2017 Athanasios PsaltisKonstantinos C. ApostolakisKosmas DimitropoulosPetros DarasPetros DarasRead full-textDuring either validation or testing, we set one view to be missing and imputed it using the trained VIGAN and data from the other view. Reconstruction quality As can be seen in Tables III and IV , we compared with matrix completion[42], multimodal AE[17], pix2pix[22]and CycleGAN[12]. Our method is able to recover missing data fairly well while under the whole missing view condition. VIGAN: Missing View Imputation with Generative Adversarial Networks[Show abstract] [Hide abstract] ABSTRACT: In an era where big data is becoming the norm, we are becoming less concerned with the quantity of the data for our models, but rather the quality. With such large amounts of data collected from multiple heterogeneous sources comes the associated problems, often missing views. As most models could not handle whole view missing problem, it brings up a significant challenge when conducting any multi-view analysis, especially when used in the context of very large and heterogeneous datasets. However if dealt with properly, joint learning from these complementary sources can be advantageous. In this work, we present a method for imputing missing views based on generative adversarial networks called VIGAN which combines cross-domain relations given unpaired data with multi-view relations given paired data. In our model, VIGAN first learns bidirectional mapping between view X and view Y using a cycle-consistent adversarial network. Moreover, we incorporate a denoising multimodal autoencoder to refine the initial approximation by making use of the joint representation. Empirical results give evidence indicating VIGAN offers competitive results compared to other methods on both numeric and image data.Article · Aug 2017 · IEEE Transactions on Computational Intelligence and AI in GamesChao ShangAaron PalmerJiangwen Sun+1 more author...Ko-Shin ChenReadIn addition, the weights between layers are adjusted by error propagation, which is totally data driven. This feature is appealing for audiovisual processing, since it provides dynamic weighting capability to model the relationship between modalities[27]. The flexibility of deep learning structures motivates our team to propose a principled framework for AV-VAD. We propose a bimodal RNN implemented with BLSTM. Bimodal Recurrent Neural Network for Audiovisual Voice Activity DetectionConference Paper · Aug 2017 · IEEE Transactions on Computational Intelligence and AI in GamesFei TaoCarlos BussoReadShow morePeople who read this publication also readDeep learningChapter · Oct 2017 · American Journal of PhysicsXing HaoGuigang ZhangReadDeep learning in physicsArticle · Sep 2017 R. A. LewisReadDeep Learning Based Disease Mention RecognitionConference Paper · Sep 2017 · American Journal of PhysicsYu-Xin ZhouYi-nan LuZhi-li Pei+1 more author...Yang LuReadData provided are for informational purposes only. Although carefully collected, accuracy cannot be guaranteed. Publisher conditions are provided by RoMEO. Differing provisions from the publishers actual policy or licence agreement may be applicable.This publication is from a journal that may support self archiving.Learn more 
