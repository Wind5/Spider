are given, for pairs of images. Under the unrestricted setting, the
identity information of the person appearing in each image is also
An algorithm designed for LFW can also choose to abstain from using
decisions, results on LFW will fall into one of the six categories
The use of training data outside of LFW can have a significant impact
on recognition performance. For instance, it was shown in Wolf et
funneled images to 0.7912, despite the fact that this method was
To enable the fair comparison of different algorithms on LFW, we ask
that researchers be specific about what type of outside training data
was used in the experiments. The list of methods at the bottom of
this page will also provide rough details on outside training data
indicate methods accepted but not yet published (e.g. accepted to an
that researchers should not be compelled to compare against either of
gives the Area Under the ROC Curve (AUC). The reason for this is that
there is no legitimate way to choose a threshold for the unsupervised
prior exposure to the people pictured in the test sets. It is probable
that the human subjects were familiar with the appearance of many of
the identities in the test sets, since many of them are famous
were in the test sets, which is disallowed under all LFW protocols.
That is, the human performance experiments do not conform to any of
accuracy of humans on this task when the identities of test images are
The scripts take in one text file for each method, containing on each
line a point on the ROC curve, i.e. average true positive rate,
followed by average false positive rate, separated by a single space.
Additional methods can be added to the script by adding on to the plot
Note that each point on the curve represents the average over the 10
folds of (false positive rate, true positive rate) for a fixed
The following Matlab function is used to compute area under the ROC
 Results were obtained using the binary available from the paper's webpage. 
 View 1 of the database was used to compute the cut-off threshold used in 
 computing mean classification accuracy on View 2. For each of the 10 folds 
 of View 2 of the database, 9 of the sets were used as training, the similarity
 measures were computed for the held out test set, and the threshold value was
 used to classify pairs as matched or mismatched. This procedure was performed
 both on the original images as well as the set of aligned images from the funneled
 We used the same parameters given on the paper's webpage, with C=1 for the SVM, specifically:
pRazSimiERCF -verbose 2 -ntrees 5 -maxleavesnb 25000 -nppL 100000 -ncondtrial 1000 -nppT 1000 -wmin 15 -wmax 100 -neirelsize 1 -svmc 1
 Face images were aligned using publicly available source code from project webpage.
 Faces in Real-Life Images Workshop in European Conference on Computer Vision (ECCV), 2008.
 using a commercial system that attempts to identify nine facial
 Faces in Real-Life Images Workshop in European Conference on Computer Vision (ECCV), 2008.
 How far can you get with a modern face recognition test set using only simple features?
 extracted at nine facial feature points using the detector of
 Everingham, Sivic, and Zisserman, 'Hello! My name is... Buffy' -
 a version of LFW aligned using a commercial, fiducial-points based
 Neeraj Kumar, Alexander C. Berg, Peter N. Belhumeur, and Shree K. Nayar.
Neeraj Kumar, Alexander C. Berg, Peter N. Belhumeur, and Shree K. Nayar.
IEEE Transactions on Pattern Analysis and Machine Intelligence (PAMI), October 2011.
 vision - was used to detect fiducial point locations. These
 locations were used to align the images and extract features from
 (e.g. Brown Hair) were trained using outside data and Amazon
 similar to Angelina Jolie) were trained using images from PubFig.
 The outputs of these classifiers on LFW images were used as features
 The computed attributes for all images in LFW can be obtained in
 page, and further information on the attributes can be found on
 using the fiducial point detector of Liang, Xiao, Wen, Sun, Face
 which are then used to extract face component images for feature
 frontal, left facing, or right facing, using three images selected
 Beyond Simple Features: A Large-Scale Feature Search Approach to Unconstrained Face Recognition.
 International Conference on Automatic Face and Gesture Recognition (FG), 2011.
 Peng Li, Yun Fu, Umar Mohammed, James H. Elder, and Simon J.D. Prince.
 IEEE Transactions on Pattern Analysis and Machine Intelligence (PAMI), vol. 34, no. 1, pp. 144-157, Jan. 2012.
 using a standard facial point detector and used to determine twelve
 makes use of 200 identities from the Multi-PIE data set, covering 7
 Leveraging Billions of Faces to Overcome Performance Barriers in Unconstrained Face Recognition.
 Journal of Machine Learning Research (Special Topics on Kernel and Metric Learning), 2012.
 Used LFW-a, and features extracted from facial feature points of Guillaumin et al., 2009.
 Large Scale Strongly Supervised Ensemble Metric Learning, with Applications to Face Verification and Retrieval.
 Haoxiang Li, Gang Hua, Zhe Lin, Jonathan Brandt, and Jianchao Yang.
 Dong Chen, Xudong Cao, Liwei Wang, Fang Wen, and Jian Sun.
 Blessing of Dimensionality: High-dimensional Feature and Its Efficient Compression for Face Verification.
 Extracted features at landmarks detected using Cao et al., 
 Zhen Cui, Wen Li, Dong Xu, Shiguang Shan, and Xilin Chen.
 Fusing Robust Face Region Descriptors via Multiple Metric Learning for Face Recognition in the Wild.
 Local Higher-Order Statistics (LHS) for Texture Categorization and Facial Analysis.
 Karen Simonyan, Omkar M. Parkhi, Andrea Vedaldi, and Andrew Zisserman.
 detector, trained using Everingham et al., "Taking the bite out of
 automatic naming of characters in TV video", Image and Vision
 IEEE Transactions on Pattern Analysis and Machine Intelligence (PAMI), 24 July 2013.
 Used outside training data (FERET images including identity information) 
 Learning Hierarchical Representations for Face Verification with Convolutional Deep Belief Networks.
 losses and a training dataset with approximately 6M images from
 no intersection with LFW). At test time we use original LFW images
 The method makes use of metric learning and dense local image
 protocol, applied to the view 2 ten-fold cross validation test,
 using images provided by the LFW website, including the 
 and funnelled sets and some external data used solely for
 representation from 4 face regions and applied a simple L2 norm to
 predict whether the pair of faces have the same identity.
POOF: Part-Based One-vs-One Features for Fine-Grained Categorization, Face Verification, and Attribute Estimation.
Class-Specific Kernel Fusion of Multiple Descriptors for Face Verification Using Multiscale Binarised Statistical Image Features.
 TCIT calculates the average position of the facial area and judges
 the identical person or other person by face recognition using the
 facial area. Face Feature Positioning is applied to get the face
Abdelmalik Ouamane, Bengherabi Messaoud, Abderrezak Guessoum, Abdenour Hadid, and Mohamed Cheriet.
Multi-scale Multi-descriptor Local Binary Features and Exponential Discriminant Analysis for Robust Face Authentication.
We used original LFW images to run the test procedure. Our system choose the right images according to the requirement auto and output the facial recognition results. And then, using the standard method to get the ROC curve. We have not do training process using LFW images.
In test, we have used original LFW images, converted to greyscale, auto-aligned with our face detector and alignment system and followed unrestricted protocol with labeled outside data results, LFW was not used for training or fine-tuning.
Xiangyu Zhu, Zhen Lei, Junjie Yan, Dong Yi, and Stan Z. Li.
High-Fidelity Pose and Expression Normalization for Face Recognition in the Wild.
Celebrities Face (YCF) dataset is used as training set which contains
to accomplish face verification tasks. With final fc layer output as
The system consists of a workflow of face detection, face alignment,
deep network with a training set of 500,000 face images of 10,000
individuals (no LFW subjects are included in the training set), each
on a different face patch. The face matching (similarity of two faces)
module was trained using a deep metric learning network, where a face
is represented as the concatenation of the 25 feature vectors. The
Side-Information based Exponential Discriminant Analysis for Face Verification in the Wild.
Biometrics in the Wild, Automatic Face and Gesture Recognition Workshop, 2015.
Our system has a complete pipeline for face recognition and each
component is implemented by ourselves. In test, we used original LFW
images. It automatic detect the right face, make face alignment and
warp face to fixed size 128*128. During feature extraction, 6 DCNN
trained 3 models for face feature extraction on our own dataset with
4260000 images of more than 60000 identities (no LFW subjects were
used for training or fine-tuning). In the test procedure, we used
original LFW images, which are detected with our face detection system
vectors from the 3 trained models, we concatenate the vectors directly
We collected 1.5 million photos, and eliminate the face photos in
database of LFW. Based on deep convolution networks, we have trained 4
Lilei Zheng, Khalid Idrissi, Christophe Garcia, Stefan Duffner, and Atilla Baskurt.
collected more than 2 million face photos, and eliminate the photos in
database of LFW. We assemble features extracted from the common deep
CNNs and other more efficient deep CNNs which we devised and named as
Iacopo Masi, Anh Tuan Tran, Jatuporn Toy Leksut, Tal Hassner and Gerard Medioni.
Do We Really Need to Collect Millions of Faces for Effective Face Recognition?
We use Chinese Face data set to train our algorithm by Deep
Convolutional neural network, and crop the model to ensure it can run
on mobile phone real-time. In test, we have used original LFW images,
infrared images and has been optimized for low execution time. We use
the LFW data for evaluation, but not for training or tuning.
alignment all the faces to the canonical shape, we use residual
extract the face representation. Each of them are trained on our own
data set, which contains 1.2 million images more than 30000 persons (no
overlapping with LFW subjects and images). The feature vector from 4
Model. All the parameters are tuned on our own validation set and we
didnâ€™t fine tune the model on LFW dataset. In test phase, we directly
use original LFW images and process all the images with our end to end
Local Binary Pattern Network: A Deep Learning Approach for Face Recognition.
our system which has a complete pipeline for face recognition by
ourselves. We collected a dataset with 2 million images of more than
20,000 persons, which has no intersection with LFW. 30 deep CNN model
were trained on our own dataset and features of each model were
with LFW dataset. We trained two different embedding models based on
8 patchs of face region, which achieved 99.77% and 99.75% pair-wise
by Euclidean distance. In test, we used original LFW images, and the
final similarity of the pair is computed by linear combination of
build our system on four face alignment algorithms to meet the needs
accuracy, which are all based on deep CNNs. The whole system is
individuals. we divide it into two parts:training data set and test
dataset. Our own dataset has no intersection with LFW. The proportion
of the training data set and test data set of about 1:10.
Our system is built on database consisting a major proportion of
use 200,000 identities with 10 million images for training 5 models of
deep convolution networks. We do a fusion of the 5 feature vectors
accuracy on LFW 10-folds, for every set we calculate the accuracy
extraction on our own datasets which contain two datasets. One of two
individuals, the other is clean 300,000 images of 6000 individuals (no
feature vectors from 6 DCNNs model with PCA to train Joint Bayesian on
our datasets. In test, we have used original LFW images, detected
our system with our own face detector and alignment. There are two
dataset used in our training step, none of them has intersection with
LFW. A dataset with 6 million images of more than 80,000 persons is
which is for feature dimension reduction instead of PCA. 11 deep CNN
models are trained. In test, we have used original LFW images,
Internet, which has no intersection with LFW dataset. We trained 3
features, we directly use original LFW images and processed all the
As of February 2017, dlib includes a face recognition model. This
model is a ResNet network with 27 conv layers. It's essentially a
version of the ResNet-34 network from the paper Deep Residual Learning
for Image Recognition by He, Zhang, Ren, and Sun with a few layers
removed and the number of filters per layer reduced by half.
The network was trained from scratch on a dataset of about 3
million faces. This dataset is derived from a number of datasets. The
dataset, and then a large number of images I scraped from the
internet. I tried as best I could to clean up the dataset by removing
labeling errors, which meant filtering out a lot of stuff from VGG. I
did this by repeatedly training a face recognition CNN and then using
graph clustering methods and a lot of manual review to clean up the
dataset. In the end about half the images are from VGG and face
scrub. Also, the total number of individual identities in the dataset
is 7485. I made sure to avoid overlap with identities in LFW.
used a structured metric loss that tries to project all the identities
into non-overlapping balls of radius 0.6. The loss is basically a
type of pair-wise hinge loss that runs over all pairs in a mini-batch
github page. From there you can find links to training code as
our system using 3 million images of 30 thousand people. Care was
taken to ensure that no training images or people were present in the
totality of the LFW dataset. The face recognition algorithm utilizes a
wide and shallow convolution network design with a novel method of
single 3.4GHz core (no GPU required). Templates are compared at a rate
Internet with 4 million images of more than 80000 people, which has no
intersection with the LFW dataset. We trained only one CNN model with
Resnet-50, and use the Euclidean distance to measure the similarity of
two images. In test, we process the original LFW images with our own
build our face recognition system. We collected about 5 million images
from internet of more than 50 thousand individuals. These images have
been cleaned, so the dataset has no intersection with the LFW . We
trained only one ResNet network. In test, we used original LFW
images. After processing these images with our own face detection and
individuals is used to train our model for feature extraction, most of
some efforts of manual cleaning up has also been made. In result
using the best threshold from the rest of 9 sets on 10-folds. Given
different models were trained on our own dataset, which contains about
10,000 individuals and 1 million face images (no overlapping with LFW
subjects and images). The face is represented as the concatenation of
these feature vectors. We used original LFW images and processed all
people. There was no intersection of LFW with training dataset. We use
We use database with 20,000 identities and 2 million images without
data for training the face recognition and verification net include 4
Internet, which has no intersection with the LFW dataset. Four DNN
collected a dataset of 5 million images with 80,000 individuals from
the Internet, which has no intersection with the LFW dataset. 3 very
matching. During test, features of the 3 DNN models are concatenated
and L2 distance is used to measure the similarity of the feature
The system consists of a workflow of face detection, face landmark,
13 face feature extraction models were trained using a deep CNN
network with a training set of 800,000 face images of 20,000
individuals (no LFW subjects are included in the training set). The
face matching module was trained using a deep metric learning network,
where a face is represented as the concatenation of the 13 feature
ratio of White to Black to Asian is 1:1:1 in the train set, which
contains 100,000 individuals and 2 million face images. We spent a
week training the networks which contain a improved resnet34 layer and
a improved triplet loss layer on a Dual Maxwell-Gtx titan x machine
collected a 2 million images of more than 80,000 individuals from the
single ResNet-like network with softmax loss. We only used one model
