One of Theanos design goals is to specify computations at an abstract
level, so that the internal function compiler has a lot of flexibility
about how to carry out those computations. One of the ways we take
advantage of this flexibility is in carrying out calculations on a
There are two ways currently to use a gpu, one that should support any OpenCL
device as well as NVIDIA cards (GpuArray Backend), and the old backend that
Using the GPU in Theano is as simple as setting the device configuration
flag to device=cuda (or device=gpu for the old backend). You can optionally target a specific gpu by specifying
the number of the gpu as in e.g. device=cuda2. You also need to set the
You can also set these options in the .theanorc files [global] section:
Theano will fall back to the CPU if there is a problem with the GPU.
You can use the flag force_device=True to instead raise an error when
If you want to use the new GpuArray backend, make sure to have the
The backend was designed to support OpenCL, however current support is
incomplete. A lot of very useful ops still do not support it because they
To see if your GPU is being used, cut and paste the following program
Use the Theano flag device=cuda to require the use of the GPU. Use the flag
vlen = 10 * 30 * 768 # 10 x #cores x # threads per core
print("Looping %d times took %f seconds" % (iters, t1 - t0))
The program just computes exp() of a bunch of random numbers. Note
that we use the theano.shared() function to make sure that the
Mapped name None to device cuda0: GeForce GTX 680 (cuDNN version 5004)
By default functions that execute on the GPU still return a standard
results are returned to ensure a consistent interface with CPU code.
This allows changing the device some code runs on by only replacing
If you dont mind a loss of flexibility, you can ask theano to return
the GPU object directly. The following code is modified to do just that.
vlen = 10 * 30 * 768 # 10 x #cores x # threads per core
print("Looping %d times took %f seconds" % (iters, t1 - t0))
For information on how to set GPU contexts, see Using multiple GPUs.
Mapped name None to device cuda0: GeForce GTX 680 (cuDNN version 5004)
While the time per call appears to be much lower than the two previous
invocations (and should indeed be lower, since we avoid a transfer)
the massive speedup we obtained is in part due to asynchronous nature
of execution on GPUs, meaning that the work isnt completed yet, just
The object returned is a GpuArray from pygpu. It mostly acts as a
numpy ndarray with some exceptions due to its data being on the GPU.
You can copy it to the host and convert it to a regular ndarray by
For even more speed, you can play with the borrow flag. See
operations can be accelerated a lot (5-50x) when arguments are large enough
Summation over rows/columns of tensors can be a little slower on the
Copying of large quantities of data to and from a device is relatively slow,
and often cancels most of the advantage of one or two accelerated functions
on that data. Getting GPU performance largely hinges on making data transfer
int, ...), however GPU support varies and some units cant deal with
double (float64) or small (less than 32 bits like int16) data types.
You will get an error at compile time or runtime if this is the case.
By default all inputs will get transferred to GPU. You can prevent an
Consider adding floatX=float32 (or the type you are using) to your
.theanorc file if you plan to do a lot of GPU work.
The GPU backend supports float64 variables, but they are still slower
to compute than float32. The more float32, the better GPU performance
the GPU by default to eliminate transfer time for GPU ops using those variables.
If you arent happy with the performance you see, try running your script with
termination. Is time being used sensibly? If an op or Apply is
taking more time than its share, then if you know something about GPU
Check the line similar to Spent Xs(X%) in cpu op, Xs(X%) in gpu op and
Xs(X%) in transfer op. This can tell you if not enough of your graph is
on the GPU or if there is too much memory transfer.
To investigate whether all the Ops in the computational graph are
running on GPU, it is possible to debug or check your code by providing
a value to assert_no_cpu_op flag, i.e. warn, for warning, raise for
raising an error or pdb for putting a breakpoint in the computational
By default, all operations on the GPU are run asynchronously. This
means that they are only scheduled to run and the function returns.
It is possible to force synchronization for a particular GpuArray by
calling its sync() method. This is useful to get accurate timings
To change the value of a shared variable, e.g. to provide new data to processes,
p_1 = 1 / (1 + T.exp(-T.dot(x, w)-b)) # Probability of having a one
prediction = p_1 > 0.5 # The prediction that is done: 0 or 1
 print(ERROR, not able to tell if theano used the cpu or the gpu)
Modify and execute this example to run on GPU with floatX=float32 and
time it using the command line time python file.py. (Of course, you may use some of your answer
What can be done to further increase the speed of the GPU version? Put your ideas to test.
CUDA backend uses config.lib.cnmem for GPU memory allocation. For the new backend(GpuArray Backend), please see config.gpuarray.preallocate
Shared variables with float32 dtype are by default moved to the GPU memory space.
Insert manual cast around the mean operator (this involves division by length, which is an int64).
Automatic error checking: All CUDA errors are automatically translated into Python exceptions.
Object cleanup tied to lifetime of objects (RAII, Resource Acquisition Is Initialization).
Makes it much easier to write correct, leak- and crash-free code.
PyCUDA knows about dependencies (e.g. it wont detach from a context before all memory
If you already enjoy a good proficiency with the C programming language, you
may easily leverage your knowledge by learning, first, to program a GPU with the
CUDA extension to C (CUDA C) and, second, to use PyCUDA to access the CUDA
(on the relationship between grids, blocks and threads; see also linked and related issues on same page)
Modify and execute to return two outputs: x + y and x - y.
only applicable to computations involving a single output. Hence, to gain
efficiency over the basic solution that is asked here, the two operations would
Modify and execute to support stride (i.e. to avoid constraining the input to be C-contiguous).
