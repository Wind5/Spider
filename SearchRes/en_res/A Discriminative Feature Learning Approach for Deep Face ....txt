 A Discriminative Feature Learning Approach for Deep Face RecognitionChapter · October 2016 with 857 ReadsDOI: 10.1007/978-3-319-46478-7_31 In book: Computer Vision – ECCV 2016, pp.499-515Cite this publication1st Yandong Wen2nd Kaipeng Zhang3rd Zhifeng Li4th Yu Qiao29.14 · Chinese Academy of SciencesAbstractConvolutional neural networks (CNNs) have been widely used in computer vision community, significantly improving the state-of-the-art. In most of the available CNNs, the softmax loss function is used as the supervision signal to train the deep model. In order to enhance the discriminative power of the deeply learned features, this paper proposes a new supervision signal, called center loss, for face recognition task. Specifically, the center loss simultaneously learns a center for deep features of each class and penalizes the distances between the deep features and their corresponding class centers. More importantly, we prove that the proposed center loss function is trainable and easy to optimize in the CNNs. With the joint supervision of softmax loss and center loss, we can train a robust CNNs to obtain the deep features with the two key learning objectives, inter-class dispension and intra-class compactness as much as possible, which are very essential to face recognition. It is encouraging to see that our CNNs (with such joint supervision) achieve the state-of-the-art accuracy on several important face recognition benchmarks, Labeled Faces in the Wild (LFW), YouTube Faces (YTF), and MegaFace Challenge. Especially, our new approach achieves the best results on MegaFace (the largest public domain face benchmark) under the protocol of small training set (contains under 500000 images and under 20000 persons), significantly improving the previous results and setting new state-of-the-art for both face recognition and face verification tasks.
 CitationsCitations39ReferencesReferences40Most existing face recognition algorithms[19,20,32,43]learn face representation using only identity supervision. An underlying assumption of their success is that deep networks can " implicitly " learn to suppress non-identity factors after seeing a large volume of images with identity labels[32,39]. Reconstruction-Based Disentanglement for Pose-invariant Face Recognition[Show abstract] [Hide abstract] ABSTRACT: Deep neural networks (DNNs) trained on large-scale datasets have recently achieved impressive improvements in face recognition. But a persistent challenge remains to develop methods capable of handling large pose variations that are relatively under-represented in training data. This paper presents a method for learning a feature representation that is invariant to pose, without requiring extensive pose coverage in training data. We first propose to generate non-frontal views from a single frontal face, in order to increase the diversity of training data while preserving accurate facial details that are critical for identity discrimination. Our next contribution is to seek a rich embedding that encodes identity features, as well as non-identity ones such as pose and landmark locations. Finally, we propose a new feature reconstruction metric learning to explicitly disentangle identity and pose, by demanding alignment between the feature reconstructions through various combinations of identity and pose features, which is obtained from two images of the same subject. Experiments on both controlled and in-the-wild face datasets, such as MultiPIE, 300WLP and the profile view database CFP, show that our method consistently outperforms the state-of-the-art, especially on images with large head pose variations. Full-text · Conference Paper · Oct 2017 Xi PengYu ‡ XiangKihyuk Sohn+1 more author...Dimitris N. MetaxasRead full-textIn the same way as previous works[6,7,18], we take the same online label and quantify the original pain intensity in the range of[0,15]to be in range[0,5]. The face verification network[16]is trained on CASIAWebface dataset[19], which contains 494,414 training images from 10,575 identities. To be consistent with face verification, we perform the same pre-processing on the images of Shoulder-Pain dataset. Regularizing Face Net For Discrete-Valued Pain Regression[Show abstract] [Hide abstract] ABSTRACT: Limited annotated data is available for the research of estimating facial expression intensities, which makes the training of deep networks for automated expression assessment very challenging. Fortunately, fine-tuning from a data-extensive pre-trained domain such as face verification can alleviate the problem. In this paper, we propose a transferred network that fine-tunes a state-of-the-art face verification network using expression-intensity labeled data with a regression layer. In this way, the expression regression task can benefit from the rich feature representations trained on a huge amount of data for face verification. The proposed transferred deep regressor is applied in estimating the intensity of facial action units (2017 EmotionNet Challenge) and in particular pain intensity estimation (UNBS-McMaster Shoulder-Pain dataset). It wins the second place in the challenge and achieves the state-of-the-art performance on Shoulder-Pain dataset. Particularly for Shoulder-Pain with the imbalance issue of different pain levels, a novel weighted evaluation metric is proposed. Full-text · Conference Paper · Sep 2017 Feng WangXiang XiangChang Liu+1 more author...Trac D. TranRead full-textFace recognition is one of the most widely studied topics in computer vision. In recent years, methods based on deep convolutional neural networks (CNNs) have shown impressive performance improvements for face recognition problems[17,22,27]. Even though these methods are able to address many challenges such as the low-resolution, poseFigure 1: Sample results of our proposed method, GANVSF. Generative Adversarial Network-based Synthesis of Visible Faces from Polarimetric Thermal Faces[Show abstract] [Hide abstract] ABSTRACT: The large domain discrepancy between faces captured in polarimetric (or conventional) thermal and visible domain makes cross-domain face recognition quite a challenging problem for both human-examiners and computer vision algorithms. Previous approaches utilize a two-step procedure (visible feature estimation and visible image reconstruction) to synthesize the visible image given the corresponding polarimetric thermal image. However, these are regarded as two disjoint steps and hence may hinder the performance of visible face reconstruction. We argue that joint optimization would be a better way to reconstruct more photo-realistic images for both computer vision algorithms and human-examiners to examine. To this end, this paper proposes a Generative Adversarial Network-based Visible Face Synthesis (GAN-VFS) method to synthesize more photo-realistic visible face images from their corresponding polarimetric images. To ensure that the encoded visible-features contain more semantically meaningful information in reconstructing the visible face image, a guidance sub-network is involved into the training procedure. To achieve photo realistic property while preserving discriminative characteristics for the reconstructed outputs, an identity loss combined with the perceptual loss are optimized in the framework. Multiple experiments evaluated on different experimental protocols demonstrate that the proposed method achieves state-of-the-art performance. Full-text · Article · Aug 2017 He ZhangVishal M. PatelBenjamin S. RigganShuowen HuShuowen HuRead full-textIn[Schroff et al., 2015], the triplet loss instead of the softmax loss is introduced to minimize the distance between an anchor and a positive data point, and maximize the distance between an anchor and a negative data point. A new loss function, named center loss, is proposed to minimize the intra-class distances based on the deep features[Wen et al., 2016]. The center loss is more convenient and can save more space and time than the contrastive loss and triplet loss. Discriminative Deep Hashing for Scalable Face Image Retrieval[Show abstract] [Hide abstract] ABSTRACT: With the explosive growth of images containing faces, scalable face image retrieval has attracted increasing attention. Due to the amazing effectiveness, deep hashing has become a popular hashing method recently. In this work, we propose a new Discriminative Deep Hashing (DDH) network to learn discriminative and compact hash codes for large-scale face image retrieval. The proposed network incorporates the end-to-end learning, the divide-and-encode module and the desired discrete code learning into a unified framework. Specifically, a network with a stack of convolution-pooling layers is proposed to extract multi-scale and robust features by merging the outputs of the third max pooling layer and the fourth convolutional layer. To reduce the redundancy among hash codes and the network parameters simultaneously, a divide-and-encode module to generate compact hash codes. Moreover, a loss function is introduced to minimize the prediction errors of the learned hash codes, which can lead to discriminative hash codes. Extensive experiments on two datasets demonstrate that the proposed method achieves superior performance compared with some state-of-the-art hashing methods.Conference Paper · Aug 2017 Jie LinZechao LiJinhui TangReadIn addition, because the op-timization is directly performed to the binary codes without any relaxation, our approach can lead to the optimal network for binary codes. Finally, to update the center t c for each concept, we can utilize a mini-batched based updating rule[Wen et al., 2016]: SitNet: Discrete Similarity Transfer Network for Zero-shot Hashing[Show abstract] [Hide abstract] ABSTRACT: Hashing has been widely utilized for fast image retrieval recently. With semantic information as supervision, hashing approaches perform much better, especially when combined with deep convolution neural network(CNN). However, in practice, new concepts emerge every day, making collecting supervised information for re-training hashing model infeasible. In this paper, we propose a novel zero-shot hashing approach, called Discrete Similarity Transfer Network (SitNet), to preserve the semantic similarity between images from both ``seen concepts and new ``unseen concepts. Motivated by zero-shot learning, the semantic vectors of concepts are adopted to capture the similarity structures among classes, making the model trained with seen concepts generalize well for unseen ones benefiting from the transferability of the semantic vector space. We adopt a multi-task architecture to exploit the supervised information for seen concepts and the semantic vectors simultaneously. Moreover, a discrete hashing layer is integrated into the network for hashcode generating to avoid the information loss caused by real-value relaxation in training phase, which is a critical problem in existing works. Experiments on three benchmarks validate the superiority of SitNet to the state-of-the-arts.Conference Paper · Aug 2017 Yuchen GuoGuiguang DingJungong HanYue GaoYue GaoReadIn other words, we need to take the entire training set into account and average the hidden codes of every class in each iteration, which is inefficient even impractical. Therefore, we follow the solution in[Wen et al., 2016]to address this problem. First, instead of updating the centers with respect to the entire training set, we perform the update based on mini-batch in each iteration. Stacked Similarity-Aware Autoencoders[Show abstract] [Hide abstract] ABSTRACT: As one of the most popular unsupervised learning approaches, the autoencoder aims at transforming the inputs to the outputs with the least discrepancy. The conventional autoencoder and most of its variants only consider the one-to-one reconstruction, which ignores the intrinsic structure of the data and may lead to overfitting. In order to preserve the latent geometric information in the data, we propose the stacked similarity-aware autoencoders. To train each single autoencoder, we first obtain the pseudo class label of each sample by clustering the input features. Then the hidden codes of those samples sharing the same category label will be required to satisfy an additional similarity constraint. Specifically, the similarity constraint is implemented based on an extension of the recently proposed center loss. With this joint supervision of the autoencoder reconstruction error and the center loss, the learned feature representations not only can reconstruct the original data, but also preserve the geometric structure of the data. Furthermore, a stacked framework is introduced to boost the representation capacity. The experimental results on several benchmark datasets show the remarkable performance improvement of the proposed algorithm compared with other autoencoder based approaches.Conference Paper · Aug 2017 Wenqing ChuDeng CaiReadShow morePeople who read this publication also readConvolutional Discriminative Feature Learning for Induction Motor Fault Diagnosis Full-text · Article · Feb 2017 Wenjun SunRui ZhaoRuqiang Yan+1 more author...Siyu ShaoRead full-textIntegrated Low-Rank-Based Discriminative Feature Learning for Recognition Full-text · Article · Jun 2015 Pan ZhouZhouchen LinChao ZhangRead full-textExtended Multi-resolution Local Patterns - A Discriminative Feature Learning Approach for Colonoscopy Image ClassificationChapter · Feb 2017 Siyamalan ManivannanEmanuele TruccoReadData provided are for informational purposes only. Although carefully collected, accuracy cannot be guaranteed. Publisher conditions are provided by RoMEO. Differing provisions from the publishers actual policy or licence agreement may be applicable.This publication is from a journal that may support self archiving.Learn moreLast Updated: 02 Jul 17 
