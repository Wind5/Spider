 DeepDriving: Learning Affordance for Direct Perception in Autonomous DrivingArticle · May 2015 with 274 ReadsSource: arXivCite this publication1st Chenyi Chen2nd Ari Seff3rd Alain L. Kornhauser21.26 · Princeton University4th Jianxiong XiaoAbstractToday, there are two major paradigms for vision-based autonomous driving
systems: mediated perception approaches that parse an entire scene to make a
driving decision, and behavior reflex approaches that directly map an input
image to a driving action by a regressor. In this paper, we propose a third
paradigm: a direct perception based approach to estimate the affordance for
driving. We propose to map an input image to a small number of key perception
indicators that directly relate to the affordance of a road/traffic state for
driving. Our representation provides a set of compact yet complete descriptions
of the scene to enable a simple controller to drive autonomously. Falling in
between the two extremes of mediated perception and behavior reflex, we argue
abstraction. To demonstrate this, we train a deep Convolutional Neural Network
(CNN) using 12 hours of human driving in a video game and show that our model
can work well to drive a car in a very diverse set of virtual environments.
Finally, we also train another CNN for car distance estimation on the KITTI
dataset, results show that the direct perception approach can generalize well
 CitationsCitations59ReferencesReferences19These results have inspired many researchers in the field of robotics to explore the applications of deep learning to solve some of the challenging problems in robotics. For example, robot localization is moving from using hand-engineered features [10] to deep learning features [26], deep reinforcement learning is being used for end-to-end training for robotic arm control [17] , multiview object recognition has achieved state-of-the-art performance by deep learning camera control [9], reinforcement learning has been used to learn dual-arm robot tasks [13], and autonomous driving has been tackled by using deep learning to estimate the affordances for driving [2]. A major challenge with deep learning is that it needs a very large volume of training data, but large datasets with manually labeled images are unavailable for most robotics applications. Robotic Grasp Detection using Deep Convolutional Neural Networks[Show abstract] [Hide abstract] ABSTRACT: Deep learning has significantly advanced computer vision and natural language processing. While there have been some successes in robotics using deep learning, it has not been widely adopted. In this paper, we present a novel robotic grasp detection system that predicts the best grasping pose of a parallel-plate robotic gripper for novel objects using the RGB-D image of the scene. The proposed model uses a deep convolutional neural network to extract features from the scene and then uses a shallow convolutional neural network to predict the grasp configuration for the object of interest. Our multi-modal model achieved an accuracy of 89.21% on the standard Cornell Grasp Dataset and runs at real-time speeds. This redefines the state-of-the-art for robotic grasp detection.Conference Paper · Sep 2017 Sulabh KumraChristopher KananReadFor learning-based obstacle avoidance, deep neural networks have been successfully applied on monocular im- ages [15] and depth images [16]. Chen et al. [17] used semantics information extracted from the image by deep neural networks to decide the behavior of the autonomous vehicle. However, their control commands are simply discrete actions like turn left and turn right which may lead to rough navigation behaviors. Virtual-to-real Deep Reinforcement Learning: Continuous Control of Mobile Robots for Mapless Navigation[Show abstract] [Hide abstract] ABSTRACT: Deep Reinforcement Learning has been successful in various virtual tasks, but it is still rarely used in real world applications especially for continuous control of mobile robots navigation. In this paper, we present a learning-based mapless motion planner by taking the 10-dimensional range findings and the target position as input and the continuous steering commands as output. Traditional motion planners for mobile ground robots with a laser range sensor mostly depend on the map of the navigation environment where both the highly precise laser sensor and the map building work of the environment are indispensable. We show that, through an asynchronous deep reinforcement learning method, a mapless motion planner can be trained end-to-end without any manually designed features and prior demonstrations. The trained planner can be directly applied in unseen virtual and real environments. We also evaluated this learning-based motion planner and compared it with the traditional motion planning method, both in virtual and real environments. The experiments show that the proposed mapless motion planner can navigate the nonholonomic mobile robot to the desired targets without colliding with any obstacles. Full-text · Conference Paper · Sep 2017 Lei TaiGiuseppe PaoloMing LiuRead full-textThe work in[21]trained a " large " , recurrent neural networks (with over 1 million weights) using a reinforcement learning method. Similar to Deep Driving[6], it also utilizedFig. 2. The architecture of our proposed deep networks for the task of vision-oriented vehicle steering. Deep Steering: Learning End-to-End Driving Model from Spatial and Temporal Visual Cues[Show abstract] [Hide abstract] ABSTRACT: In recent years, autonomous driving algorithms using low-cost vehicle-mounted cameras have attracted increasing endeavors from both academia and industry. There are multiple fronts to these endeavors, including object detection on roads, 3-D reconstruction etc., but in this work we focus on a vision-based model that directly maps raw input images to steering angles using deep networks. This represents a nascent research topic in computer vision. The technical contributions of this work are three-fold. First, the model is learned and evaluated on real human driving videos that are time-synchronized with other vehicle sensors. This differs from many prior models trained from synthetic data in racing games. Second, state-of-the-art models, such as PilotNet, mostly predict the wheel angles independently on each video frame, which contradicts common understanding of driving as a stateful process. Instead, our proposed model strikes a combination of spatial and temporal cues, jointly investigating instantaneous monocular camera observations and vehicles historical states. This is in practice accomplished by inserting carefully-designed recurrent units (e.g., LSTM and Conv-LSTM) at proper network layers. Third, to facilitate the interpretability of the learned model, we utilize a visual back-propagation scheme for discovering and visualizing image regions crucially influencing the final steering prediction. Our experimental study is based on about 6 hours of human driving data provided by Udacity. Comprehensive quantitative evaluations demonstrate the effectiveness and robustness of our model, even under scenarios like drastic lighting changes and abrupt turning. The comparison with other state-of-the-art models clearly reveals its superior performance in predicting the due wheel angle for a self-driving car.Article · Aug 2017 Lu ChiYadong MuReadIn these settings, a deep neural network is typically trained with reinforcement learning or imitation learning to represent a control policy which maps input states to control sequences. However, as already discussed in[5,21], the resulting networks and encoded policies are inherently reactive, thus unable to execute planning to decide following actions, which may explain poor generalization to new or unseen environments. Conversely, optimal control algorithms utilize specified models of system dynamics and a cost function to predict future states and future cost values. Path Integral Networks: End-to-End Differentiable Optimal Control[Show abstract] [Hide abstract] ABSTRACT: In this paper, we introduce Path Integral Networks (PI-Net), a recurrent network representation of the Path Integral optimal control algorithm. The network includes both system dynamics and cost models, used for optimal control based planning. PI-Net is fully differentiable, learning both dynamics and cost models end-to-end by back-propagation and stochastic gradient descent. Because of this, PI-Net can learn to plan. PI-Net has several advantages: it can generalize to unseen states thanks to planning, it can be applied to continuous control tasks, and it allows for a wide variety learning schemes, including imitation and reinforcement learning. Preliminary experiment results show that PI-Net, trained by imitation learning, can mimic control demonstrations for two simulated problems; a linear system and a pendulum swing-up problem. We also show that PI-Net is able to learn dynamics and cost models latent in the demonstrations.Article · Jun 2017 Masashi OkadaLuca RigazioTakenobu AoshimaReadThe first is direct affordance detection performed in this research, which involves learning affordances end-to-end directly from image pixels[6],[27]. Second, indirect affordance detection is when known objects in a scene are linked to their affordances via an affordance knowledge tree or lookup table[3],[4],[41]. The third approach is semi-direct affordance detection, which is the use of object attributes to detect object affordances[14],[28]. Multi-Modal Trip Hazard Affordance Detection On Construction Sites[Show abstract] [Hide abstract] ABSTRACT: Trip hazards are a significant contributor to accidents on construction and manufacturing sites, where over a third of Australian workplace injuries occur [1]. Current safety inspections are labour intensive and limited by human fallibility,making automation of trip hazard detection appealing from both a safety and economic perspective. Trip hazards present an interesting challenge to modern learning techniques because they are defined as much by affordance as by object type; for example wires on a table are not a trip hazard, but can be if lying on the ground. To address these challenges, we conduct a comprehensive investigation into the performance characteristics of 11 different colour and depth fusion approaches, including 4 fusion and one non fusion approach; using colour and two types of depth images. Trained and tested on over 600 labelled trip hazards over 4 floors and 2000m$\mathrm{^{2}}$ in an active construction site,this approach was able to differentiate between identical objects in different physical configurations (see Figure 1). Outperforming a colour-only detector, our multi-modal trip detector fuses colour and depth information to achieve a 4% absolute improvement in F1-score. These investigative results and the extensive publicly available dataset moves us one step closer to assistive or fully automated safety inspection systems on construction sites.Article · Jun 2017 Sean McMahonNiko SünderhaufBen UpcroftMichael MilfordMichael MilfordReadThis environment model is used by a further component to plan and control the vehicles behavior. In[4]a direct perception approach is proposed. Instead of using detections of lane markings and other objects to derive requirements for the vehicles behavior indirectly, a set of affordances for driving actions is defined. Learning How to Drive in a Real World Simulation with Deep Q-Networks[Show abstract] [Hide abstract] ABSTRACT: We present a reinforcement learning approach using Deep Q-Networks to steer a vehicle in a 3D physics simulation. Relying solely on camera image input the approach directly learns steering the vehicle in an end-to-end manner. The system is able to learn human driving behavior without the need of any labeled training data. An action-based reward function is proposed, which is motivated by a potential use in real world reinforcement learning scenarios. Compared to a na¨ıvena¨ıve distance-based reward function, it improves the overall driving behavior of the vehicle agent. The agent is even able to reach comparable to human driving performance on a previously unseen track in our simulation environment. Full-text · Conference Paper · Jun 2017 Peter WolfChristian HubschneiderMichael Weber+1 more author...André BauerRead full-textShow moreRecommended publicationsDiscover more publications, questions and projects in DrivingConference PaperDeepDriving: Learning Affordance for Direct Perception in Autonomous DrivingDecember 2015Read moreArticleLearning from Maps: Visual Common Sense for Autonomous DrivingNovember 2016Todays autonomous vehicles rely extensively on high-definition 3D maps to navigate the environment. While this approach works well when these maps are completely up-to-date, safe autonomous vehicles must be able to corroborate the maps information via a real time sensor-based system. Our goal in this work is to develop a model for road layout inference given imagery from on-board cameras,... [Show full abstract]Read moreArticleLSUN: Construction of a Large-scale Image Dataset using Deep Learning with Humans in the LoopJune 2015The state-of-the-art visual recognition algorithms are all data-hungry,
requiring a huge amount of labeled image data to optimize millions of
parameters. While there has been remarkable progress in algorithm and system
design, the labeled datasets used by these models are quickly becoming outdated
in terms of size. To overcome the bottleneck of human labeling speed during
dataset construction,... [Show full abstract]Read moreChapterR-CNN for Small Object DetectionMarch 2017 · Lecture Notes in Computer Science · Impact Factor: 0.51Existing object detection literature focuses on detecting a big object covering a large part of an image. The problem of detecting a small object covering a small part of an image is largely ignored. As a result, the state-of-the-art object detection algorithm renders unsatisfactory performance as applied to detect small objects in images. In this paper, we dedicate an effort to bridge the... [Show full abstract]Read moreDiscover moreData provided are for informational purposes only. Although carefully collected, accuracy cannot be guaranteed. Publisher conditions are provided by RoMEO. Differing provisions from the publishers actual policy or licence agreement may be applicable.This publication is from a journal that may support self archiving.Learn more 
