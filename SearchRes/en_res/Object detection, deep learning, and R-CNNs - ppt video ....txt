 We think you have liked this presentation. If you wish to download it, please recommend it to your friends in any social system. Share buttons are a little bit lower. Thank you!
 Outline Object detection Convolutional Neural Networks (CNNs)the task, evaluation, datasets Convolutional Neural Networks (CNNs) overview and history Region-based Convolutional Networks (R-CNNs)
 Sort by confidence ... ... ... ... ... ✓ ✓ ✓ true positive false0.9 0.8 0.6 0.5 0.2 0.1 ... ... ... ... ... ✓ ✓ ✓ X X X true positive (high overlap) false positive (no overlap, low overlap, or duplicate) Let’s define the problem a bit more so we’re all on the same page.
 Evaluation metric ... ... ... ... ... ✓ ✓ ✓ ✓ ✓ + X 0.9 0.8 0.6 0.50.2 0.1 ... ... ... ... ... ✓ ✓ ✓ X X X 𝑡 ✓ #𝑡𝑟𝑢𝑒 #𝑡𝑟𝑢𝑒 Let’s define the problem a bit more so we’re all on the same page. ✓ + X #𝑡𝑟𝑢𝑒 #𝑔𝑟𝑜𝑢𝑛𝑑 𝑡𝑟𝑢𝑡ℎ 𝑜𝑏𝑗𝑒𝑐𝑡𝑠
 Evaluation metric ... ... ... ... ... ✓ ✓ ✓ Average Precision (AP)0.9 0.8 0.6 0.5 0.2 0.1 ... ... ... ... ... ✓ ✓ ✓ X X X Average Precision (AP) 0% is worst 100% is best mean AP over classes (mAP) Let’s define the problem a bit more so we’re all on the same page.
 Histograms of Oriented Gradients for Human Detection, Dalal and Triggs, CVPR 2005AP ~77% More sophisticated methods: AP ~90% Pedestrians (a) average gradient image over training examples (b) each “pixel” shows max positive SVM weight in the block centered on that pixel (c) same as (b) for negative SVM weights (d) test image (e) its R-HOG descriptor (f) R-HOG descriptor weighted by positive SVM weights (g) R-HOG descriptor weighted by negative SVM weights
 Part-based models & multiple features (MKL)41% 41% rapid performance improvements 37% DPM++, MKL, Selective Search Selective Search, DPM++, MKL 28% DPM++ 23% DPM, MKL 17% DPM, HOG+ BOW DPM
 Region-based Convolutional Networks (R-CNNs)62% 53% R-CNN v2 R-CNN v1 41% 41% 37% DPM++, MKL, Selective Search Selective Search, DPM++, MKL 28% DPM++ 23% DPM, MKL 17% DPM, HOG+ BOW DPM [R-CNN. Girshick et al. CVPR 2014]
 Convolutional NNs Shared (“tied”) weights𝑤 1 𝑤 2 𝑤 3 All green units share the same parameters 𝒘 Each green unit computes the same function, but with a different input window 𝑤 1 𝑤 2 𝑤 3
 Convolutional NNs Convolution with 1-D filter: [ 𝑤 3 , 𝑤 2 , 𝑤 1 ]All green units share the same parameters 𝒘 Each green unit computes the same function, but with a different input window
 Convolutional NNs Convolution with 1-D filter: [ 𝑤 3 , 𝑤 2 , 𝑤 1 ]All green units share the same parameters 𝒘 Each green unit computes the same function, but with a different input window 𝑤 3
 Convolutional NNs Convolution with 1-D filter: [ 𝑤 3 , 𝑤 2 , 𝑤 1 ]All green units share the same parameters 𝒘 Each green unit computes the same function, but with a different input window 𝑤 1 𝑤 2 𝑤 3
 Convolutional NNs Convolution with 1-D filter: [ 𝑤 3 , 𝑤 2 , 𝑤 1 ]All green units share the same parameters 𝒘 Each green unit computes the same function, but with a different input window 𝑤 1 𝑤 2 𝑤 3
 Convolutional NNs Convolution with 1-D filter: [ 𝑤 3 , 𝑤 2 , 𝑤 1 ]All green units share the same parameters 𝒘 Each green unit computes the same function, but with a different input window 𝑤 1 𝑤 2 𝑤 3
 Convolutional NNs Multiple feature maps𝑤′ 1 𝑤′ 2 All orange units compute the same function but with a different input windows Orange and green units compute different functions 𝑤′ 3 𝑤 1 Feature map 2 (array of orange units) 𝑤 2 𝑤 3 Feature map 1 (array of green units)
 Supervised learning – 1986 Gradient descent training with error backpropagation Early demonstration that error backpropagation can be used for supervised training of neural nets (including ConvNets)
 Demo http://cs.stanford.edu/people/karpathy/convnetjs/ demo/mnist.htmlConvNetJS by Andrej Karpathy (Ph.D. student at Stanford) Software libraries Caffe (C++, python, matlab) Torch7 (C++, lua) Theano (python)
 The fall of ConvNets The rise of Support Vector Machines (SVMs)Mathematical advantages (theory, convex optimization) Competitive performance on tasks such as digit classification Neural nets became unpopular in the mid 1990s
 What’s new since the 1980s? More layersLeNet-3 and LeNet-5 had 3 and 5 learnable layers Current models have 8 – 20+ “ReLU” non-linearities (Rectified Linear Unit) 𝑔 𝑥 = max 0, 𝑥 Gradient doesn’t vanish “Dropout” regularization Fast GPU implementations More data 𝑔(𝑥) 𝑥
 " alt="Rich feature hierarchies for accurate object detection and semantic segmentation 2014 IEEE Conference on Computer Vision and Pattern Recognition Ross Girshick,">
 Rich feature hierarchies for accurate object detection and semantic segmentation 2014 IEEE Conference on Computer Vision and Pattern Recognition Ross Girshick,
 " alt="Deformable Part Models (DPM) Felzenswalb, Girshick, McAllester & Ramanan (2010) Slides drawn from a tutorial By R. Girshick AP 12% 27% 36% 45% 49% 2005.">
 Deformable Part Models (DPM) Felzenswalb, Girshick, McAllester & Ramanan (2010) Slides drawn from a tutorial By R. Girshick AP 12% 27% 36% 45% 49% 2005.
 " alt="1 Bilinear Classifiers for Visual Recognition Computational Vision Lab. University of California Irvine To be presented in NIPS 2009 Hamed Pirsiavash Deva.">
 1 Bilinear Classifiers for Visual Recognition Computational Vision Lab. University of California Irvine To be presented in NIPS 2009 Hamed Pirsiavash Deva.
 " alt="“Secret” of Object Detection Zheng Wu (Summer intern in MSRNE) Sep. 3, 2010 Joint work with Ce Liu (MSRNE) William T. Freeman (MIT) Adam Kalai (MSRNE)">
 “Secret” of Object Detection Zheng Wu (Summer intern in MSRNE) Sep. 3, 2010 Joint work with Ce Liu (MSRNE) William T. Freeman (MIT) Adam Kalai (MSRNE)
