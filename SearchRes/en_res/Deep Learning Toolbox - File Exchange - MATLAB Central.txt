There are much better tools available for deep learning than this toolbox, e.g. Theano, torch or tensorflow
I would suggest you use one of the tools mentioned above rather than use this toolbox.
Deep Learning is a new subfield of machine learning that focuses on learning deep hierarchical models of data. It is inspired by the human brains apparent deep (layered, hierarchical) architecture. A good overview of the theory of Deep Learning theory is Learning Deep Architectures for AI
For a more informal introduction, see the following videos by Geoffrey Hinton and Andrew Ng.
Prediction as a candidate for learning deep hierarchical models of data (Palm, 2012)
i would like to ask why when training dbn and fine tune the model using nn , then predict the model on test data the out_y get same index using nnff the nn.a{end} are the same values ( i used numeric data with 800 observations for binary classification)
 I m getting an error saying it cant load mnist_uint8.
i tried using the mnist data from their website and it says some columns are not available due to ASCII.
 Sorry, but the more i look the code the more im convinced that this si not as cool as it seems at first. True that there are no other libraries doing CAEs, but this is left unfinished. 
Beware that even kernels dont work. Also you have to expect that the toolbox is strictly thought for image data. 
 This is a really nice toolbox, but as some say it lacks of a document and of code commenting.
May i ask what is the difference between input and output kernels in the Convolutional autoencoders? Why such a distinction is not done in the simple convolutional networks? And if i was to pre-train a CNN using a CAE how should i preceed? 
When I select the number of batches to be equal to one the codes sum(v1 - v2) and sum(h1 - h2) sum up to a scalar when in fact they should be a vector. I dont know if this is different for your version of MATLAB, but a simple fix in R2014a is simply adding a second parameter to the sum function like this: sum(v1 - v2, 1)
I havent explored that much the library to say this issue is not present elsewhere
 Hi, i tried to run DBN.m with my own data, but when I run this code:[er, bad] = nntest(nn, test_x, test_y);
I found er is zero and bad is null. the input size of train_x is 320*200,the output is 320*1. who could tell my why this happened ? Thanks 
It seems like my Octave is too old? So how can I update my Octave? Just download new Octave and install it and then everything will be great?? Thanks
I wrote a post explaining the CNN example along with documented / commented versions of most of the CNN functions:
 I just started using this code and was puzzled by the following (plz excuse me for my newbie questions):
How come the errors on MNIST in the examples are less than state of the art? I get ~0.07 error on the 2-layer DBN-NN in the example. State-of-the-art, as far as I am aware, is higher than this. 
Also when visualizing the dbn.rbm{2}.W layer I see pretty much garbage. There is no structure to the weights like dbn.rbm{1}.W. What has to be done to enable higher level structure learning.
 Thanks for your code! But, when I execute cnnexamples in CNN folder after modifying cnn.layers kernel size from "5" to "4", 
I got an error in cnnbp.m line 37 like "Array dimensions must match for binary array op."
 Just tried to run the 2nd DBN example and it failed. First, the assert at line 6 of rbmtrain.m failed. From what I see, the assert should be ==0 not ~= 0 (numbatches should be integer).
Secondly, the example failed on nnff line 14. From what I can tell, the last size size in dbn.sizes should be 10 as thats the y (==> output layer) size.
 autoencoder backpropagation convolutional neu... deep belief network deep learning feedforward logistic regression machine learning neural net popular file 2014 restricted boltzm... toolbox
