This course consists of videos and programming exercises to teach you about
give you hands-on, practical experience for getting these algorithms to work.
To get the most out of this course, you should watch the videos and complete
the exercises in the order in which they are listed. 
Octave, which has been called a "free version of Matlab." 
If you are using Octave, be sure to install the Image package as well
(available for Windows as an option in the installer, and available
The files contain some example measurements of heights for various boys
between the ages of two and eights. The y-values are the heights measured
in meters, and the x-values are the ages of the boys corresponding to the 
our dataset. There are training examples, and you will use them to 
1. Implement gradient descent using a learning rate of . Since Matlab/Octave and Octave index vectors starting from 1 rather than 0, you'll probably use 
of gradient descent from this initial starting point. Record the value of
of and that you get after this first iteration. (To verify that 
your implementation is correct, later we'll ask you to check your values of and against ours.) 
2. Continue running gradient descent for more iterations until converges. 
(this will take a total of about 1500 iterations). After convergence, record the 
straight line fit from your algorithm on the same graph as your training data.
Note that for most machine learning problems, is very high dimensional, so we don't be
able to plot . But since in this example we have only one feature, being able
to plot this gives a nice sanity-check on our result. 
3. Finally, we'd like to make some predictions using the learned hypothesis.
Use your model to predict the height for a two boys of age 3.5 and age 7.
If you are using Matlab/Octave and seeing many errors at runtime, try inspecting your matrix
operations to check that you are multiplying and adding matrices in ways that their
dimensions would allow. Remember that Matlab/Octave by default interprets an operation as a 
matrix operation. In cases where you don't intend to use the matrix definition 
of an operator but your expression is ambiguous to Matlab/Octave, you will have to use 
the 'dot' operator to specify your command. Additionally, you can try printing x,
 y, and theta to make sure their dimensions are correct.
We'd like to understand better what gradient descent has done, and visualize the relationship between 
 and . In this problem, we'll plot as a 3D surface plot.
(When applying learning algorithms, we don't usually try to plot since usually 
very high-dimensional so that we don't have any simple way to plot or visualize . But because the
After you have completed the exercises above, please refer to the solutions below and check that your implementation and your 
answers are correct. In a case where your implementation does not result in the same parameters/phenomena as described 
below, debug your solution until you manage to replicate the same effect as our implementation. 
A complete m-file implementation of the solutions can be found 
here. Run this m-file in Matlab/Octave to produce all the solutions
parameters are approximately equal to the exact closed-form solution (which you 
If you run gradient descent in MATLAB for 1500 iterations at a learning rate of 0.07, 
you should see these exact numbers for theta. If used fewer iterations, your answer 
should not differ by more than 0.01, or you probably did not iterate enough. 
 running gradient descent in MATLAB for 500 iterations gives theta = [0.7318, 0.0672]. 
This is close to convergence, but theta can still get closer to the exact value if you
If your answer differs drastically from the solutions above, there may be a bug in your
implementation. Check that you used the correct learning rate of 0.07 and that you
defined the gradient descent update correctly. Then, check that your x and y vectors 
are indeed what you expect them to be. Remember that x needs an extra column of ones.
3. The predicted height for age 3.5 is 0.9737 meters, and for 
Plot A plot of the training data with the best fit from gradient descent
In your surface plot, you should see that the cost function approaches a minimum near the values of and that you found through
gradient descent. In general, the cost function for a linear regression problem
will be bowl-shaped with a global minimum and no local optima.
Depending on the viewing window of your surface plot, it may not be so 
apparent that the cost function is bowl-shaped. To see the approach to the
global optimum better, it may be helpful to plot a contour plot.
