This course will assume some familiarity with reinforcement learning, numerical optimization and machine learning. Students who are not familiar with the concepts below are encouraged to brush up using the references provided right below this list. We’ll review this material in class, but it will be rather cursory.
 Classification and regression problems: what loss functions are used, how to fit linear and nonlinear models
The assignments will be provided as Jupyter (formerly called IPython) notebooks (docs) and will use NumPy (docs) with Python 2.7.
You may find the following tutorial helpful (from Stanford CS231): Python/Numpy.
There will be four problem sets (the first one divided into two parts) and a final assignment in which you write a 1-page research proposal about a research idea or an application of Deep RL.
Homework 1 is released: Download it here. After installing dependencies, unzip the file, navigate into the hw1 directory, and type ipython notebook.
It will be due Monday September 7th at 11:59PM. The URL and credentials for uploading your homework will be provided on Piazza.
Homework 2 is released: Download it here. It will be due Sunday October 25th at 11:59pm. 
The URL and credentials for uploading your homework will be provided on Piazza.
Problems 4 and 5 optionally use CGT, if you choose to use the reference implementations.
The Atari wrapper doesn’t work on Windows, but all of the code should work on Mac and Linux.
Problem sets will each be worth 20% of your grade and the final proposal will be worth the last 20%.
Below you can find a tentative outline of the course. Slides, videos, and references will be posted as the course proceeds. Dates are tentative.
 See Powell textbook for more information on applications in operations research.
 See Stephane Ross’ thesis (Introduction) for more info on structured prediction as reinforcement learning and how is RL difference from supervised learning?
 For a very thorough analysis of reverse mode automatic differentiation, see Griewank and Walther’s textbook Evaluating Derivatives. Chances are, you’re computing derivatives all day, so it pays off to go into some depth on this topic!
 Peters & Schaal: Reinforcement learning of motor skills with policy gradients: solid review article on policy gradient methods.
 Sham Kakade’s thesis, chapter 4: nice historical overview and theoretical study, with an alternative perspective based on advantage functions.
 Greensmith, Baxter, & Bartlett: Variance Reduction Techniques for Gradient Estimates in Reinforcement Learning, and also see the paper Infinite-Horizon Policy-Gradient Estimation which introduces the theoretical framework.
 Kakade & Langford: Approximately optimal approximate reinforcement learning – Conservative policy iteration algorithm, providing some nice intuition about policy gradient methods, and some generally useful theoretical ideas.
 Schulman, Levine, Moritz, Jordan, Abbeel: Trust Region Policy Optimization: combines theoretical ideas from conservative policy gradient algorithm to prove that monotonic improvement can be guaranteed when one solves a series of subproblems of optimizing a bound on the policy performance. The conclusion is that one should use KL-divergence constraint.
 Schulman, Moritz, Levine, Jordan, Abbeel: High-Dimensional Continuous Control Using Generalized Advantage Estimation: Better estimation of advantage function for policy gradient algorithms, using a λ parameter.
 Q-learning convergence result is originally due to Watkins. A compact and general alternative proof is provided by Jordan, Jaakola, and Singh: On the Convergence of Stochastic Iterative Dynamic Programming Algorithm, which also applies to TD(λ)
 Deep Q Network (DQN) by Mnih et al. of DeepMind: ArXiv, Nature
 Scherrer et al., Approximate Modified Policy Iteration. A very general framework that subsumes many other ADP algorithms as special cases. Also see the related paper with more practical tips: Approximate Dynamic Programming Finally Performs Well in the Game of Tetris.
 See Bertsekas’ textbook, 2ed for a very extensive treatment of approximate value function estimation methods, and approximate policy iteration.
 DAGGER and related ideas based on querying an expert (or search algorithm) while executing agent’s policy:
 A Reduction of Imitation Learning and Structured Prediction to No-Regret Online Learning - DAGGER
 Reinforcement and Imitation Learning via Interactive No-Regret Learning AGGREVATE – same authors as DAGGER, cleaner and more general framework (in my opinion).
 Deep Learning for Real-Time Atari Game Play Using Offline Monte-Carlo Tree Search Planning Monte-Carlo Tree Search + DAGGER
 SEARN in Practice - similar to DAGGER/AGGREVATE but using a stochastic policy, and targeted at structured prediction problems. Stephane Ross’ thesis has a nice explanation of SEARN.
 Guided Policy Search: use (modification of) importance sampling to get policy gradient, where samples are obtained via trajectory optimization.
 Constrained Guided Policy Search: Formulates an objective that jointly includes a collection of trajectories and a policy, and encourages them to become consistent. End-To-End Training of Deep Visuomotor Policies uses CGPS learn mapping from image pixels to low-level control signal in robotic manipulation problems.
 Combining the Benefits of Function Approximation and Trajectory Optimization jointly optimizing trajectories and policies, with some different design choices from CGPS. (Igor has written a new NIPS paper on this topic, it will be linked when it’s ready)
 Bertsekas, Dynamic Programming and Optimal Control, Vols I and II
