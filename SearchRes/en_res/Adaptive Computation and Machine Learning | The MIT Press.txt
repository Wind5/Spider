 The goal of building systems that can adapt to their environments and learn from their experience has attracted researchers from many fields, including computer science, engineering, mathematics, physics, neuroscience, and cognitive science. Out of this research has come a wide variety of learning techniques, including methods for learning decision trees, decision rules, neural networks, statistical classifiers, and probabilistic graphical models. The researchers in these various areas have also produced several different theoretical frameworks for understanding these methods, such as computational learning theory, Bayesian learning theory, classical statistical theory, minimum description length theory, and statistical mechanics approaches. These theories provide insight into experimental results and help to guide the development of improved learning algorithms. A goal of the series is to promote the unification of the many diverse strands of machine learning research and to foster high quality research and innovative applications. This series will publish works of the highest quality that advance the understanding and practical application of machine learning and adaptive computation. Research monographs, introductory and advanced level textbooks, how-to books for practitioners will all be considered. For information on the submission of proposals and manuscripts, please contact any of the series editors above or the publisher, Marie Lee (marielee@mit.edu). The series editor is Thomas G. Dietterich, and the associate series editors are Christopher M. Bishop, David Herckerman, Michael I. Jordan, and Michael Kerns.
 A comprehensive review of an area of machine learning that deals with the use of unlabeled data in classification problems: state-of-the-art algorithms, a taxonomy of the field, applications, benchmark experiments, and directions for future research. 
 Advanced statistical modeling and knowledge representation techniques for a newly emerging area of machine learning and probabilistic reasoning; includes introductory material, tutorials for different proposed approaches, and applications. 
 A comprehensive introduction and reference guide to the minimum description length (MDL) Principle that is accessible to researchers dealing with inductive reference in diverse areas including statistics, pattern classification, machine learning, data mining, biology, econometrics, and experimental psychology, as well as philosophers interested in the foundations of statistics. 
 The authors address the assumptions and methods that allow us to turn observations into causal knowledge, and use even incomplete causal knowledge in planning and prediction to influence and control our environment. 
 Richard Sutton and Andrew Barto provide a clear and simple account of the key ideas and algorithms of reinforcement learning. Their discussion ranges from the history of the field's intellectual foundations to the most recent developments and applications.
