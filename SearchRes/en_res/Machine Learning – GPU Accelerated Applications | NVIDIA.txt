Data scientists in both industry and academia have been using GPUs for machine learning to make groundbreaking improvements across a variety of applications including image classification, video analytics, speech recognition and natural language processing. In particular, Deep Learning – the use of sophisticated, multi-level deep neural networks to create systems that can perform feature detection from massive amounts of unlabeled training data – is an area that has been seeing significant investment and research.  
 Although machine learning has been around for decades, two relatively recent trends have sparked widespread use of machine learning: the availability of massive amounts of training data, and powerful and efficient parallel computing provided by GPU computing.  GPUs are used to train these deep neural networks using far larger training sets, in an order of magnitude less time, using far less datacenter infrastructure.  GPUs are also being used to run these trained machine learning models to do classification and prediction in the cloud, supporting far more data volume and throughput with less power and infrastructure.
 Early adopters of GPU accelerators for machine learning include many of the largest web and social media companies, along with top tier research institutions in data science and machine learning. With thousands of computational cores and 10-100x application throughput compared to CPUs alone, GPUs have become the processor of choice for processing big data for data scientists.
"With GPUs, pre-recorded speech or multimedia content can be transcribed much more quickly. Compared to CPU implementation we are able to perform recognition up to 33x faster."
Accelerate your results with the NVIDIA DiGiTS DevBox – the world’s fastest deskside deep learning system. DiGiTS DevBox offers the freedom to explore multiple network architectures, and accelerated dataset manipulation—all in one powerful, energy-efficient, cool, and quiet solution that fits under your desk. Register for the DiGiTS DevBox early access program or learn how to create your own system.
Learn how other data scientists are advancing their work in the field of machine learning, and get information about tools, software frameworks, and computing configurations that will help you get started.
 Theano: Python library to define, optimize, and evaluate mathematical expressions
 cuBLAS: GPU-accelerated version of the complete standard BLAS library 
 Press Release: Researchers Deploy GPUs To Build World's Largest Artificial Neural Network
 Video: Visual Object Recognition Using Deep Convolutional Neural Networks (Rob Fergus, Facebook/NYU) 
 Video: 10 Billion Parameter Neural Networks in Your Basement (Adam Coates, Stanford University) 
 Video: Deep Neural Networks for Visual Pattern Recognition (Dan Ciresan, IDSIA) 
 Video: GPU Accelerated Model Combination for Robust Speech Recognition and Keyword Search (Wonkyum Lee, Carnegie Mellon University) 
 Video: Clarifai: Enabling Next Generation Intelligent Applications (Matthew Zeiler, Clarifai) 
 Video: Beyond Pedestrian Detection: Deep Neural Networks Level-Up Automotive Savety (Ikuro Sato, Hideki Niihara, Denso IT Laboratory) 
 Video: Using GPUs to Accelerate Learning to Rank (Alexander Shcekalev, Yandex) 
 Video: Extreme Matching Learning w/ GPUs (John Canny, UC Berkeley) 
 Video: GPU-Optimized Deep Learning Networks for Automatic Speech Recognition (Jessica Ray, MIT Lincoln Laboratory)
 Whitepaper: Deep Learning with COTS HPC Systems(Adam Coates, Stanford) 
 Whitepaper: ImageNet Classification with Deep Convolutional Neural Networks (Alex Krizhevsky, Ilya Sutskever, Geoffrey Hinton, University of Toronto) 
 Whitepaper: Multi-GPU Training of ConVets (Omry Yadan, Keith Adams Yaniv Taigman, Marc Ranzato, Facebook) 
 Whitepaper: OverFeat Recognition Localication Detection (Pierre Sermanet, David Eigen, Xiang Zhang, Michael Mathieu, Rob Fergus, Yann LeCun, New York University) 
 Whitepaper: Mitosis Detection in Breast Cancer Histology Images using Deep Neural Networks (Dan Ciresan,IDSIA)
 Whitepaper: Fast Support Vector Machine Training and Classification on GPU Processors (Bryan Catanzaro, NVIDIA) 
