Deep learning is becoming ubiquitous. With recent advancements in deep learning algorithms and GPU technology, we are able to solve problems once considered impossible in fields such as computer vision, natural language processing, and robotics.
Deep learning uses deep neural networks which have been around for a few decades; what’s changed in recent years is the availability of large labeled datasets and powerful GPUs. Neural networks are inherently parallel algorithms and GPUs with thousands of cores can take advantage of this parallelism to dramatically reduce computation time needed for training deep learning networks. In this post, I will discuss how you can use MATLAB to develop an object recognition system using deep convolutional neural networks and GPUs.
Machine learning techniques use data (images, signals, text) to train a machine (or model) to perform a task such as image classification, object detection, or language translation. Classical machine learning techniques are still being used to solve challenging image classification problems. However, they dont work well when applied directly to images, because they ignore the structure and compositional nature of images. Until recently, state-of-the-art techniques made use of feature extraction algorithms that extract interesting parts of an image as compact low-dimensional feature vectors. These were then used along with traditional machine learning algorithms.
Enter Deep learning. Deep convolutional neural networks (CNNs), a specific type of deep learning algorithm, address the gaps in traditional machine learning techniques, changing the way we solve these problems. CNNs not only perform classification, but they can also learn to extract features directly from raw images, eliminating the need for manual feature extraction. For computer vision applications you often need more than just image classification; you need state-of-the-art computer vision techniques for object detection, a bit of domain expertise, and the know-how to set up and use GPUs efficiently. Through the rest of this post, I will use an object recognition example to illustrate how easy it is to use MATLAB for deep learning, even if you don’t have extensive knowledge of computer vision or GPU programming.
The goal in this example is to detect a pet in a video and correctly label the pet as a cat or a dog. To run this example, you will need MATLAB®, Parallel Computing Toolbox™, Computer Vision System Toolbox™ and Statistics and Machine Learning Toolbox™. If you don’t have these tools, request a trial at www.mathworks.com/trial. For this problem I used an NVIDIA Tesla K40 GPU; you can run it on any MATLAB compatible CUDA-enabled NVIDIA GPU.
Object Recognition: Now that I know where it is, is it a cat or a dog?
The first step is to train a classifier that can classify images of cats and dogs. I could either:
Collect a massive amount of cropped, resized and labeled images of cats and dogs in a reasonable amount of time (good luck!), or
Use a model that has already been trained on a variety of common objects and adapt it for my problem.
Figure 2: Pretrained ImageNet model classifying the image of the dog as beagle.
For this example, Im going to go with option (2) which is common in practice. To do that Im going to first start with a pretrained CNN classifier that has been trained on the ImageNet dataset.
I will be using MatConvNet, a CNN package for MATLAB that uses the NVIDIA cuDNN library for accelerated training and prediction. [To learn more about cuDNN, see this Parallel Forall post.] Download and install instructions for MatConvNet are available on its home page. Once Ive installed MatConvNet on my computer, I can use the following MATLAB code to download and make predictions using the pretrained CNN classifier. Note: I also use the cnnPredict() helper function, which Ive made available on Github.
The pretrained CNN classifier works great out of the box at object classification. The CNN model is able to tell me that there is a beagle in the example image (Figure 2). While this is certainly a great starting point, our problem is a little different. I want to be able to (1) put a box around where the pet is (object detection) and then (2) label it accurately as a dog or a cat (classification). Let’s start by building a dog vs cat classifier from the pretrained CNN model.
The objective is simple. I want to solve a simple classification task: given an image I’d like to train a classifier that can accurately tell me if it’s an image of a dog or a cat. I can do that easily with this pretrained classifier and a few dog and cat images.
To get a small collection of labeled images for this project, I went around my office asking colleagues to send me pictures of their pets. I segregated the images and put them into separate cat and dog folders under a parent called pet_images. The advantage of using this folder structure is that the imageSet function can automatically manage image locations and labels. I loaded them all into MATLAB using the following code.
What Id like to do next is use this new dataset along with the pretrained ImageNet to extract features. As I mentioned earlier, CNNs can learn to extract generic features from images. These features can be used to train a new classifier to solve a different problem, like classifying cats and dogs in our problem.
CNN algorithms are compute-intensive and can be slow to run. Since they are inherently parallel algorithms, I can use GPUs to speed up the computation. Here is the code that performs the feature extraction using the pretrained model, and a comparison of multithreaded CPU (Intel Core i7-3770 CPU) and GPU (NVIDIA Tesla K40 GPU) implementations.
% Depending on how much memory you have on your GPU you may use a larger
% batch size. I have 400 images, so I choose 200 as my batch size
Figure 3: Comparision of execution times for feature extraction using a CPU (left) and NVIDIA Tesla K40 GPU (right).
Figure 4: The CPU and GPU time required to extract features from 1128 images.
As you can see the performance boost you get from using a GPU is significant, about 15x for this feature extraction problem.
The function cnnPredict is a wrapper around MatConvNet’s vl_simplenn predict function. The highlighted line of code in Figure 5 is the only modification you need to make to run the prediction on a GPU. Functions like gpuArray in the Parallel Computing Toolbox make it easy to prototype your algorithms using a CPU and quickly switch to GPUs with minimal code changes.
Figure 5: The `gpuArray` and `gather` functions allow you to transfer data from the MATLAB workspace to the GPU and back.
With the features I extracted in the previous step, Im now ready to train a shallow classifier. To train and compare multiple models interactively, I can use the Classification Learner app in the Statistics and Machine Learning Toolbox. Note: for an introduction to machine learning and classification workflows in MATLAB, check out this Machine Learning Made Easy webinar.
Next, I will directly train an SVM classifier using the extracted features by calling the fitcsvm function using cnnFeatures as the input or predictors and trainingLabels as the output or response values. I will also cross-validate the classifier to test its validation accuracy. The validation accuracy is an unbiased estimate of how the classifier would perform in practice on unseen data.
% Here I train a linear support vector machine (SVM) classifier.
svmmdl is my classifier that I can now use to classify an image as a cat or a dog.
Most images and videos frames have a lot going on in them. In addition to a dog, there may be a tree or a raccoon chasing the dog. Even with a great image classifier, like the one I built in the previous step, it will only work well if I can locate the object of interest in an image (dog or cat), crop the object and then feed it to a classifier. The step of locating the object is called object detection.
For object detection, I will use a technique called Optical Flow that uses the motion of pixels in a video from frame to frame. Figure 6 shows a single frame of video with the motion vectors overlaid.
Figure 6: A single frame of video with motion vectors overlaid (left) and magnitude of the motion vectors (right).
The next step in the detection process is to separate out pixels that are moving, and then use the Image Region Analyzer app to analyze the connected components in the binary image to filter out the noisy pixels caused by the camera motion. The output of the app is a MATLAB function (I’m going to call it findPet) that can locate where the pet is in the field of view.
I now have all the pieces I need to build a pet detection and recognition system.
Crop the pet from the image and extract features using a pretrained CNN;
Tying all these pieces together, the following code shows my complete MATLAB pet detection and recognition system.
Solutions to real-world computer vision problems often require tradeoffs depending on your application: performance, accuracy, and simplicity of the solution. Advances in techniques such as deep learning have significantly raised the bar in terms of the accuracy of tasks like visual recognition, but the performance costs were too significant for mainstream adoption. GPU technology has closed this gap by accelerating training and prediction speeds by orders of magnitude.
MATLAB makes computer vision with deep learning much more accessible. The combination of an easy-to-use application and programming environment, a complete library of standard computer vision and machine learning algorithms, and tightly integrated support for CUDA-enabled GPUs makes MATLAB an ideal platform for designing and prototyping computer vision solutions.
If you enjoyed reading this post, please register for our upcoming webinar to learn more:
Deep Learning for Computer Vision with MATLAB. We will be available after the webinar to answer questions. You may also be interested in checking out these previous MATLAB posts on Parallel Forall.
Related PostsDeep Speech: Accurate Speech Recognition with GPU-Accelerated Deep LearningDeep Learning for Image Understanding in Planetary ScienceEasy Multi-GPU Deep Learning with DIGITS 2DIGITS: Deep Learning GPU Training System
Shashank Prasanna is a product marketing manager at NVIDIA where he focuses on deep learning products and applications. Prior to joining NVIDIA, Shashank worked for MathWorks, makers of MATLAB, focusing on machine learning and data analytics, and for Oracle Corp. designing and developing CRM software. Shashank holds an M.S. in electrical engineering from Arizona State University.
 Nice writeup. For those interested in GPU systems with cuDNN preinstalled I found the following which readers here might be interested in: http://exxactcorp.com/index.php/solution/solu_detail/225
 Hi Wendell, glad you liked the post. The images and videos belong to my my colleagues and unfortunately I don’t have permissions to share all of them. You can find several dog and cat image datasets and videos on the internet that be readily used for this task. Please note that your mileage may vary since the solution is sensitivity to the training images. For example, if your training data is small and only includes certain pet poses, your model may not be robust to all poses in the video. You may then need to gather more images to introduce pose invariance.
 Under MatConvNet, the function vl_nnconv.m has nothing within it. If you have it, then kindly send the zip folder
 Hi Kadir, opticalFlowFarneback is part of the Computer Vision System Toolbox and was introduced in R2015b (current release or MATLAB). Make sure you upgrade to this release and you should be able to run the example. Feel free to contact us if you have further questions or need help with upgrade or usage: http://www.mathworks.com/support/contact_us/
 I have a problem.Under MatConvNet, the function vl_nnconv.m has nothing within it. If you have it, then kindly send the zip folder
 I setup Computer Vision System Toolbox, but in examples, with
mexOpencv example.cpp command, creates the example.mex and example.m files and then build example.m using this mex file. But I have now only script file so it returns Undefined function or variable opticalFlowFarneback. error. So, is there any other solution or path-lib settings I have forgotten?
 Hi Elif, see my response to Kadir below. If you have R2015b version of MATLAB and Computer Vision System Toolbox, you should be able to just run opticalFlowFarneback without the need to install anything from external packages.
 i have problems and i am using 2015a. i am using rgb image. Should i have to use gray scale image?
Reference to non-existent field normalization' means that the cnn model you provided to cnnPredict function doesnt have a field called normalization cnnPredict function needs field to do two things: (1) To resize your input image such that it is compatible with the imagenet network (2) subtract the imagenet average image.
If you downloaded a pretrained imagenet model from vlfeat webpage as suggested in the code files, the model must already have a normalization field that cnnPredict expects, in order to make a prediction.
The cnnModel.net is properly downloaded from Vlfeat. I tried imagenet-vgg-f.mat and imagenet-matconvnet-vgg-f.mat.
I had the same problem with matconvnet-1.0-beta 18, but there are only a few lines to fix in the code to get tit working.
You simply need to update the NN in order to make it compatible by: 
Those networks apparently have a slightly different structure, than in earlier versions. 
In Shashank Prasannas function cnnPredict.m simply add the meta struct field (e.g. cnnModel.net.normalization > cnnModel.net.meta.normalization ) in lines 78, 84 and 85:
 hi, i have a problem. I ran the code successfully but i didnt get the desired ouput. There was no bounding box around dog or cat in the constructed video test.avi. Whats wrong with it please explain someone.
 The blog post outlines the steps you would take to use a pretrained CNN as a feature extraction technique. Alternatively you could train a network from scratch, you should find code examples to do in the MatConvNet examples folder.
PROBLEM > Assignment has fewer non-singleton rhs dimensions than non-singleton subscripts
If I continue doing the following steps, in this one I get another problem:
X and Y do not have the same number of observations.
 Hi bfos, Can you please tell me how did you solved this error: 
PROBLEM > Assignment has fewer non-singleton rhs dimensions than non-singleton subscripts
I am also getting this error, and want to know the workaround.
In my case, this error was due to few images which are gray images, thus have 2 dimensions only. What I did is to replicate them into a 3 channel image and then copied into trainingImages. It worked for me.
 Means if you have 100 img, than you must have 100 set of features(100 img *1000 dimension feature)
in your case, you only have 1 of 100 dimension feature(total one img), not match you img number(should be more than one).
 Means if you have 100 img, than you must have 100 set of features(100 img *1000 dimension feature)
In your case, you only have 1 of 100 dimension feature(total one img), not match you img number(should be more than one).
 Thank you so much for the series on CNN. Id like to do it for Bio-Medical Image feature extraction.And later process with classifier apps. What are the steps to be needed to extract the features and to create the .mat files? Also, it could be used for predicting the test images? It would be helpful and appreciate your efforts. Thanks.
Thanks for your post, and Ive been worked on this experiment for a few days and I met a problem that I still cant solve.
I found that after I type the command(I use this because I was working without GPU support, does this matter?):
The output shows that the number of images is 10, but actually I have more than 10 images, 26 exactly, and I tried I lot to figure this out but I failed.
X and Y do not have the same number of observations.
Im new to Deep Learning and I found difficult to solve this, could you please help me?
I will also be very appreciate to anyone who can offer me any help.
 An updated version of this demo is available along with source code at the following MathWorks page:
In order to run this demo, you will need a compatible GPU and CUDA driver installed. For more information on the system requirements, check out the following page:
Thanks for your valuable post.Id like to do it in fingerprint liveness detection. How can i do feature extraction in fingerprint images?? what are the steps? Id also like to classify it into real or fake fingerprint images. after it could used to test on other images. How can i do it sir? Can you please tell me the codes for it. It would be helpful and appreciate your efforts. Thanks.
I tried to use trainFastRCNNObjectDetector with custom region proposals. so far ive got
Unable to find any region proposals to use as positive or negative training samples.
