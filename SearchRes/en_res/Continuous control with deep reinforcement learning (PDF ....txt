 Continuous control with deep reinforcement learningArticle · September 2015 with 801 ReadsSource: arXivCite this publication1st Timothy P. Lillicrap2nd Jonathan J. Hunt+ 43rd Alexander PritzelLast Daan WierstraShow more authorsAbstractWe adapt the ideas underlying the success of Deep Q-Learning to the
based on the deterministic policy gradient that can operate over continuous
manipulation, legged locomotion and car driving. Our algorithm is able to find
policies whose performance is competitive with those found by a planning
algorithm with full access to the dynamics of the domain and its derivatives.
We further demonstrate that for many of the tasks the algorithm can learn
policies end-to-end: directly from raw pixel inputs.Discover the worlds research13+ million members100+ million publications700k+ research projectsJoin for free
 Continuous control with deep reinforcement learningTimothy P. Lillicrap* Jonathan J. Hunt* Alexander Pritzel Nicolas HeessTom Erez Yuval Tassa David SilverDaan WierstraGoogle Deepmind* These authors contributed equally.{countzero, jjhunt, apritzel, heess, etom,tassa, davidsilver, wierstra}@ google.comAbstractWe adapt the ideas underlying the success of Deep Q-Learning to the continuousaction domain. We present an actor-critic, model-free algorithm based on the de-terministic policy gradient that can operate over continuous action spaces. Usingthe same learning algorithm, network architecture and hyper-parameters, our al-gorithm robustly solves more than 20 simulated physics tasks, including classicproblems such as cartpole swing-up, dexterous manipulation, legged locomotionand car driving. Our algorithm is able to ﬁnd policies whose performance is com-petitive with those found by a planning algorithm with full access to the dynamicsof the domain and its derivatives. We further demonstrate that for many of thetasks the algorithm can learn policies “end-to-end”: directly from raw pixel in-puts.1 IntroductionOne of the primary goals of the ﬁeld of artiﬁcial intelligence is to solve complex tasks from unpro-cessed, high-dimensional, sensory input. Recently, signiﬁcant progress has been made by combiningadvances in deep learning for sensory processing [1] with reinforcement learning, resulting in the“Deep Q Network” (DQN) algorithm [2] that is capable of human level performance on many Atarivideo games using unprocessed pixels for input. To do so, deep neural network function approxi-mators were used to estimate the action-value function.However, while DQN solves problems with high-dimensional observation spaces, it can only handlediscrete and low-dimensional action spaces. Many tasks of interest, most notably physical controltasks, have continuous (real valued) and high dimensional action spaces. DQN cannot be straight-forwardly applied to continuous domains since it relies on a ﬁnding the action that maximises theaction-value function, which in the continuous valued case requires an iterative optimization processat every step.One obvious approach to adapting deep reinforcement learning methods such as DQN to continu-ous domains is to to simply discretize the action space. However, this has many limitations, mostnotably the curse of dimensionality: the number of actions increases exponentially with the numberof degrees of freedom. For example, a 7 degree of freedom system (as in the human arm) with thecoarsest discretization ai∈ {−k, 0, k}for each joint leads to an action space with dimensionality:37= 2187. The situation is even worse for tasks that require ﬁne control of actions as they requirea correspondingly ﬁner grained discretization, leading to an explosion of the number of discreteactions. Such large action spaces are difﬁcult to explore efﬁciently, and thus successfully training1arXiv:1509.02971v1 [cs.LG] 9 Sep 2015
 DQN-like networks in this context is likely intractable. Additionally, naive discretization of actionspaces needlessly throws away information about the structure of the action domain, which may beessential for solving many problems.In this work we present a model-free, off-policy actor-critic algorithm using deep function approxi-mators that can learn policies in high-dimensional, continuous action spaces. Our work is based onthe deterministic policy gradient (DPG) algorithm [3] (itself similar to NFQCA [4], and similar ideascan be found in [5]). However, as we show below, a naive application of this actor-critic methodwith neural function approximators is unstable for challenging problems.Here we combine the actor-critic approach with insights from the recent success of Deep Q Network(DQN) [6, 2]. Prior to DQN, it was generally believed that learning value functions using large, non-linear function approximators was difﬁcult and unstable. DQN is able to learn value functions usingsuch function approximators in a stable and robust way due to two innovations: 1. the network istrained off-policy with samples from a replay buffer to minimize correlations between samples; 2.the network is trained with a target Q network to give consistent targets during temporal differencebackups. In this work we make use of the same ideas, along with batch normalization [7], a recentadvance in deep learning.In order to evaluate our method we constructed a variety of challenging physical control problemsthat involve complex multi-joint movements, unstable and rich contact dynamics, and gait behaviour.Among these are classic problems such as the cartpole swing-up problem, as well as many newdomains. A long-standing challenge of robotic control is to learn an action policy directly from rawsensory input such as video. Accordingly, we place a ﬁxed viewpoint camera in the simulator andattempted all tasks using both low-dimensional observations (e.g. joint angles) and directly frompixels.We ﬁnd that our model-free approach which we call Deep DPG (DDPG) can learn competitivepolicies for all of our tasks using low-dimensional observations (e.g. cartesian coordinates or jointangles) using the same hyper-parameters and network structure. In many cases, we are also ableto learn good policies directly from pixels, again keeping hyperparameters and network structureconstant 1.A key feature of the approach is its simplicity: it requires only a straightforward actor-critic ar-chitecture and learning algorithm with very few “moving parts”, making it easy to implement andscale to more difﬁcult problems and larger networks. For the physical control problems we compareour results to a baseline computed by a planner [8] that has full access to the underlying simulateddynamics and its derivatives (see supplementary information). Interestingly, DDPG can sometimesﬁnd policies that exceed the performance of the planner, in some cases even when learning frompixels (the planner always plans over the true, low-dimensional state space).2 BackgroundWe consider a standard reinforcement learning setup consisting of an agent interacting with an en-vironment Ein discrete timesteps. At each timestep tthe agent receives an observation xt, takesan action atand receives a scalar reward rt. In all the environments considered here the actionsare real-valued at∈IRN. In general, the environment may be partially observed so that the entirehistory of the observation, action pairs st= (x1, a1, ..., at−1, xt)may be required to describe thestate. Here, we assumed the environment satisﬁes the Markov property so st=xt.An agent’s behavior is deﬁned by a policy, π, which maps states to a probability distribution overthe actions π:S → P(A). The environment, E, may also be stochastic. We model it as a Markovdecision process with a state space S, action space A=IRN, an initial state distribution p(s1)andtransition dynamics, p(st+1|st, at), and reward function r(st, at). Applying a policy to the MDPdeﬁnes a Markov chain and we can write Eπto denote expectations over this chain.The return from a state is deﬁned as the sum of discounted future reward Rt=PTi=tγ(i−t)r(si, ai)with a discounting factor γ∈[0,1]. Note that the return depends on the actions chosen, and therefore1You can view a movie of some of the learned policies at https://goo.gl/J4PIAz2
 on the policy π, and may be stochastic. The goal in reinforcement learning is to learn a policy whichmaximises the expected returns from the start state Eπ[R1].The action-value function is used in many reinforcement learning algorithms. It describes the ex-pected return after taking an action atin state stand thereafter following policy π:Qπ(st, at) = Eπ[Rt|st, at](1)Many approaches in reinforcement learning make use of the recursive relationship known as theBellman equation:Qπ(st, at) = Est+1∼E ,at∼π[r(st, at) + γEπ[Qπ(st+1, at+1)]] (2)If the target policy is deterministic we can describe it as a function µ:S ← A and avoid the innerexpectation:Qµ(st, at) = Est+1∼E[r(st, at) + γQµ(st+1, µ(st+1))] (3)Notice that the outer expectation depends only on the environment. This means that it is possible tolearn Qµoff-policy, using transitions which are generated from a different behaviour policy µ0.Q-learning [9], a commonly used off-policy algorithm, uses the greedy policy µ(s) =arg maxaQ(s, a). We consider function approximators parameterized by θQ, which we optimizeby minimizing the loss:L(θQ) = Eµ0hQ(st, at|θQ)−yt2i(4)whereyt=r(st, at) + γQ(st+1 , µ(st+1)|θQ).(5)While ytis also dependent on θQ, this is typically ignored.The use of large, non-linear function approximators for learning value or action-value functionshas often been avoided in the past since theoretical performance guarantees are impossible, andpractically learning tends to be very unstable. Recently, [6, 2] adapted the Q-learning algorithm inorder to make effective use of large neural networks as function approximators. Their algorithmwas able to learn to play Atari games from pixels. In order to scale Q-learning they introducedtwo major changes: the use of a replay buffer, and a separate target network for calculating yt. Weemploy these in the context of DDPG and explain their implementation the next section.3 AlgorithmIt is not possible to straightforwardly apply Q-learning to continuous action spaces, because in con-tinous spaces ﬁnding the greedy policy requires an optimization of atat every timestep; this opti-mization is too slow to be practical with large, unconstrained function approximators and nontrivialaction spaces. Instead, here we used an actor-critic approach based on the DPG algorithm [3].The DPG algorithm maintains a parameterized actor function µ(s|θµ)which speciﬁes the currentpolicy by deterministically mapping states to a speciﬁc action. The critic Q(s, a)is learned usingthe Bellman equation as in Q-learning. The actor is updated by applying the chain rule to equation3 with respect to the actor parameters:∇θµµ≈Eµ0∇θµQ(s, a|θQ)|s=st,a=µ(st|θµ)=Eµ0∇aQ(s, a|θQ)|s=st,a=µ(st)∇θµµ(s|θµ)|s=st(6)Silver et al. [3] proved that this is the policy gradient, i.e. the gradient of the policy’s overallperformance.As with Q learning, introducing non-linear function approximators means that convergence is nolonger guaranteed. However, such approximators appear essential in order to learn and generalizeon large state spaces. NFQCA [4], which uses the same update rules as DPG but with neural networkfunction approximators, uses batch learning for stability, which is intractable for large networks. Aminibatch version of NFQCA which does not reset the policy at each update, as would be requiredto scale to large networks, is equivalent to the original DPG, which we compare to here. Ourcontribution in this paper is to provide modiﬁcations to DPG, inspired by the success of DQN,3
 which allow it to use neural network function approximators to learn effectively in large state andaction spaces online. We refer to our algorithm as Deep DPG (DDPG, Algorithm 1).One challenge when using neural networks for reinforcement learning is that most optimization al-gorithms assume that the samples are independently and identically distributed. Obviously, whenthe samples are generated from exploring sequentially in an environment this assumption no longerholds. Additionally, to make efﬁcient use of hardware optimizations, it is essential to learn in mini-batches, rather than online.As in DQN, we used a replay buffer to address these issues. The replay buffer is a ﬁnite sized cacheR. Transitions were sampled from the environment according to the exploration policy and the tuple(st, at, rt, st+1)was stored in the replay buffer. When the replay buffer was full the oldest sampleswere discarded. At each timestep the actor and critic are updated by sampling a minibatch uniformlyfrom the buffer. Because DDPG is an off-policy algorithm, the replay buffer can be large, allowingthe algorithm to beneﬁt from learning across a set of transitions uncorrelated transitions.Directly implementing Q learning (equation 4) with neural networks proved to be unstable in manyenvironments. Since the network Q(s, a|θQ)being updated is also used in calculating the targetvalue (equation 5), the Q update is prone to divergence. Our solution is similar to the target net-work used in [6] but modiﬁed for actor-critic and using “soft” target updates, rather than directlycopying the weights. We create a copy of the actor and critic networks, Q0(s, a|θQ0)and µ0(s|θµ0)respectively, that are used for calculating the target values. The weights of these target networks arethen updated by having them slowly track the learned networks: θ0←τ θ + (1 −τ)θ0with τ1.This means that the target values are constrained to change slowly, greatly improving the stabilityof learning. This simple change moves the relatively unstable problem of learning the action-valuefunction closer to the case of supervised learning, a problem for which robust solutions exist. Wefound that having both a target µ0and Q0was required to have stable targets yiin order to consis-tently train the critic without divergence. This may slow learning, since the target network delaysthe propogation of value estimations. However, in practice we found this was greatly outweighedby the stability of learning.When learning from low dimensional feature vector observations, the different components of theobservation may have different physical units (for example, positions versus velocities) and theranges may vary across environments. This can make it difﬁcult for the network to learn effec-tively and may make it difﬁcult to ﬁnd hyper-parameters which generalise across environments withdifferent scales of state values.One approach to this problem is to manually scale the features so they are in similar ranges acrossenvironments and units. We address this issue by adapting a recent technique from deep learningcalled batch normalization [7]. This technique normalizes each dimension across the samples in aminibatch to have unit mean and variance. In addition, it maintains a running average of the meanand variance to use for normalization during testing (in our case, during exploration or evaluation).In deep networks, it is used to minimize covariance shift during training, by ensuring that each layerreceives whitened input. In the low-dimensional case, we used batch normalization on the state inputand all layers of the µnetwork and all layers of the Qnetwork prior to the action input (details of thenetworks are given in the supplementary material). With batch normalization, we were able to learneffectively across many different tasks with differing types of units, without needing to manuallyensure the units were within a set range.A major challenge of learning in continuous action spaces is to explore effectively. One advantage ofoff-policies algorithms such as DDPG is that we can treat the problem of exploration independentlyfrom the learning algorithm. We constructed an exploration policy µ0by adding noise sampled froma noise process Nto our actor policyµ0(st) = µ(st|θµt) + N(7)Ncan be chosen to chosen to suit the environment. As detailed in the supplementary materials weused an Ornstein-Uhlenbeck process [10] to generate temporally correlated exploration (similar useof autocorrelated noise for efﬁcient exploration was introduced in [11]).4
 Algorithm 1 DDPG algorithmRandomly initialize critic network Q(s, a|θQ)and actor µ(s|θµ)with weights θQand θµ.Initialize target network Q0and µ0with weights θQ0←θQ,θµ0←θµInitialize replay buffer Rfor episode = 1, M doInitialize a random process Nfor action explorationReceive initial observation state s1for t=1,TdoSelect action at=µ(st|θµ) + Ntaccording to the current policy and exploration noiseExecute action atand observe reward rtand observe new state st+1Store transition (st, at, rt, st+1)in RSample a random minibatch of Ntransitions (si, ai, ri, si+1)from RSet yi=ri+γQ0(si+1 , µ0(si+1|θµ0)|θQ0)Update critic by minimizing the loss: L=1NPi(yi−Q(si, ai|θQ)2)Update the actor policy using the sampled gradient:∇θµµ|si≈1NXi∇aQ(s, a|θQ)|s=si,a=µ(si)∇θµµ(s|θµ)|siUpdate the target networks:θQ0←τθQ+ (1 −τ)θQ0θµ0←τθµ+ (1 −τ)θµ0end forend for4 ResultsWe constructed simulated physical environments of varying levels of difﬁculty to test our algorithm.This included classic reinforcement learning environments such as cartpole, as well as difﬁcult,high dimensional tasks such as gripper, tasks involving contacts such as puck striking (canada) andlocomotion tasks such as cheetah [12]. In all domains but cheetah the actions were torques appliedto the actuated joints. These environments were simulated using MuJoCo [13]. Figure 1 showsrenderings of some of the environments used in the task (the supplementary contains details of theenvironments and you can view some of the learned policies at https://goo.gl/J4PIAz).In all tasks, we ran experiments using both a low-dimensional state description (such as joint anglesand positions) and high-dimensional renderings of the environment. As in DQN [6, 2], in order tomake the problems approximately fully observable in the high dimensional environment we usedaction repeats. For each timestep of the agent, we step the simulation 3 timesteps, repeating theagent’s action and rendering each time. Thus the observation reported to the agent contains 9 featuremaps (the RGB of each of the 3 renderings) which allows the agent to infer velocities using thedifferences between frames. The frames were downsampled to 64x64 pixels and the 8-bit RGBvalues were converted to ﬂoating point scaled to [0,1]. See supplementary information for details ofour network structure and hyperparameters.We evaluated the policy periodically during training by testing it without exploration noise. Figure2 shows the performance curve for a selection of environments. We also report results with compo-nents of our algorithm (i.e. the target network or batch normalization) removed. In order to performwell across all tasks, both of these additions are necessary. In particular, learning without a targetnetwork, as in the orginal work with DPG, is very poor in many environments.Surprisingly, in some simpler tasks, learning policies from pixels is just as fast as learning using thelow-dimensional state descriptor. This may be due to the action repeats making the problem simpler.It may also be that the convolutional layers provide an easily separable representation of state space,which is straightforward for the higher layers to learn on quickly.5
 Table 1 summarizes DDPG’s performance across all of the environments (results are taken acrossn= 5 replicas). We normalized the scores using two baselines. The ﬁrst baseline is the meanreturn from a naive policy which samples actions from a uniform distribution over the valid actionspace. The second baseline is iLQG [14], a planning based solver with full access to the underlyingphysical model and its derivaties. We normalize scores so that the naive policy has a mean score of0and iLQG has a mean score of 1. DDPG is able to learn good policies on many of the tasks, andin many cases some of the replicas learn policies which are superior to those found by iLQG, evenwhen learning directly from pixels.It can be challenging to learn accurate value estimates. Q-learning, for example, is prone to over-estimating values [15]. We examined DDPG’s estimates empirically by comparing the values esti-mated by Qafter training with the true returns seen on test episodes. Figure 3 shows that in simpletasks DDPG estimates returns accurately without systematic biases. For harder tasks the Q estimatesare worse, but DDPG is still able learn good policies.To demonstrate the generality of our approach we also include Torcs, a racing game where theactions are acceleration, braking and steering. Torcs has previously been used as a testbed in otherpolicy learning approaches [16]. We used an identical network architecture and learning algorithmhyper-parameters to the physics tasks but altered the noise process for exploration because of thevery different time scales involved. On both low-dimensional and from pixels, some replicas wereable to learn reasonable policies that are able to complete a circuit around the track though otherreplicas failed to learn a sensible policy.Figure 1: Example screenshots of a sample of environments we attempt to solve with DDPG. Inorder from the left: the cartpole swing-up task, a reaching task, a gasp and move task, a puck-hittingtask, a monoped balancing task, two locomotion tasks and Torcs (driving simulator). We tackleall tasks using both low-dimensional feature vector and high-dimensional pixel inputs. Detaileddescriptions of the environments are provided in the supplementary. Movies of some of the learnedpolicies are available at https://goo.gl/J4PIAz.Cart Pendulum Swing-up Cartpole Swing-up Fixed ReacherBlockworldGripper Puck ShootingMonoped BalacingMoving GripperCheetahMillion Steps01101100110010011100Normalized Reward10 0 0 0 01 1 1 1Figure 2: Performance curves for a selection of domains using variants of DPG: original DPG al-gorithm with batch normalization (light grey), with target network (dark grey), with target networksand batch normalization (green), with target networks from pixel-only inputs (blue). Target networksare crucial.5 Related workThe original DPG paper evaluated the algorithm with toy problems using tile-coding and linearfunction approximators. It demonstrated data efﬁciency advantages for off-policy DPG over both6
 Pendulum CheetahCartpoleEstimated QReturnFigure 3: Density plot showing estimated Q values versus observed returns sampled from testepisodes on 5 replicas. In simple domains such as pendulum and cartpole the Q values are quiteaccurate. In more complex tasks, the Q estimates are less accurate, but can still be used to learncompetent policies. Dotted line indicates unity, units are arbitrary.Table 1: Performance after training across all environments for at most 2.5 million steps. We reportboth the average and best observed (across 5 runs). All scores, except Torcs, are normalized so thata random agent receives 0 and a planning algorithm 1; for Torcs we present the raw reward score.environment Rav,lowd Rbest,lowd Rav,pix Rbest,pixblockworld1 1.156 1.511 0.466 1.299blockworld3da 0.340 0.705 0.889 2.225canada 0.303 1.735 0.176 0.688canada2d 0.400 0.978 -0.285 0.119cart 0.938 1.336 1.096 1.258cartpole 0.844 1.115 0.482 1.138cartpoleBalance 0.951 1.000 0.335 0.996cartpoleParallelDouble 0.549 0.900 0.188 0.323cartpoleSerialDouble 0.272 0.719 0.195 0.642cartpoleSerialTriple 0.736 0.946 0.412 0.427cheetah 0.903 1.206 0.457 0.792ﬁxedReacher 0.849 1.021 0.693 0.981ﬁxedReacherDouble 0.924 0.996 0.872 0.943ﬁxedReacherSingle 0.954 1.000 0.827 0.995gripper 0.655 0.972 0.406 0.790gripperRandom 0.618 0.937 0.082 0.791hardCheetah 1.311 1.990 1.204 1.431hopper 0.676 0.936 0.112 0.924hyq 0.416 0.722 0.234 0.672movingGripper 0.474 0.936 0.480 0.644pendulum 0.946 1.021 0.663 1.055reacher 0.720 0.987 0.194 0.878reacher3daFixedTarget 0.585 0.943 0.453 0.922reacher3daRandomTarget 0.467 0.739 0.374 0.735reacherSingle 0.981 1.102 1.000 1.083walker2d 0.705 1.573 0.944 1.476torcs -393.385 1840.036 -401.911 1876.284on- and off-policy stochastic actor critic. It also solved one more challenging task in which a multi-jointed octopus arm had to strike a target with any part of the limb. However, that paper did notdemonstrate scaling the approach to large, high-dimensional observation spaces as we have here.It has often been assumed that standard policy search methods such as those explored in the presentwork are simply too fragile to scale to difﬁcult problems [17]. Standard policy search is thought tobe difﬁcult because it deals simultaneously with complex environmental dynamics and a complexpolicy. Indeed, most past work with actor-critic and policy optimization approaches have had difﬁ-culty scaling up to more challenging problems [18]. Typically, this is due to instability in learningwherein progress on a problem is either destroyed by subsequent learning updates, or else learningis too slow to be practical.7
 Recent work with model-free policy search has demonstrated that it may not be as fragile as previ-ously supposed. Wawrzy´nski et al. [12, 19] has trained stochastic policies in an actor-critic frame-work with a replay buffer but its unclear if this approach can scale to using large function approxima-tors with high-dimensional observation spaces. Another approach, trust region policy optimization(TRPO) [20], directly constructs stochastic neural network policies without decomposing problemsinto optimal control and supervised phases. This method produces near monotonic improvements inreturn by making carefully chosen updates to the policy parameters, constraining updates to preventthe new policy from diverging too far from the existing policy. This approach does not require learn-ing an action-value function, and (perhaps as a result) appears to be signiﬁcantly less data efﬁcient.To combat the challenges of the actor-critic approach, recent work with guided policy search (GPS)algorithms (e.g., [17]) decomposes the problem into three phases that are relatively easy to solve:ﬁrst, it uses full-state observations to create locally-linear approximations of the dynamics aroundone or more nominal trajectories, and then uses optimal control to ﬁnd the locally-linear optimalpolicy along these trajectories; ﬁnal, it uses supervised learning to train a complex, non-linear policy(e.g. a deep neural network) to reproduce the state-to-action mapping found in the ﬁrst phase.This approach has several beneﬁts, including data efﬁcency, and has been applied successfully toa variety of real-world robotic manipulation tasks using vision. In these tasks GPS uses a similarconvolutional policy network to ours with 2 notable exceptions: 1. it uses a spatial softmax to reducethe dimensionality of visual features into a single (x, y)coordinate for each feature map, and 2. thepolicy also receives direct low-dimensional state information about the conﬁguration of the robot atthe ﬁrst fully connected layer in the network. Both likely increase the power and data efﬁciency ofthe algorithm and could easily be exploited within the DDPG framework.PILCO [21] uses Gaussian processes to learn a non-parametric, probabilistic model of the dynamics.Using this learned model, PILCO calculates analytic policy gradients and achieves impressive dataefﬁciency in a number of control problems. However, due to the high computational demand, PILCOis “impractical for high-dimensional problems” [22]. It seems that deep function approximators arethe most promising approach for scaling reinforcement learning to large, high-dimensional domains.Wahlstrom et al. [22] used a deep dynamical model network along with model predictive controlto solve the pendulum swing-up task from pixel input. They trained a differentiable forward modeland encoded the goal state into the learned latent space. They use model-predictive control over thelearned model to ﬁnd a policy for reaching the target. However, this approach is only applicable todomains with goal states that can be demonstrated to the algorithm.Evolutionary approaches have also had some notable success in training policies for continuousdomains, where policy parameters are learned using evolutionary optimization. Recently, evolu-tionary approaches have been used to learn competitive policies for Torcs from pixels using com-pressed weight parametrizations [23] or unsupervised learning [16] to reduce the dimensionality ofthe evolved weights. It is unclear how well these approaches generalize to other problems.6 ConclusionThe work presented here combines insights from recent advances in deep learning and reinforcementlearning, resulting in an algorithm that robustly solves challenging problems across a variety ofdomains with continuous action spaces, even when using raw pixels for observations. As withmost reinforcement learning algorithms, the use of non-linear function approximators nulliﬁes anyconvergence guarantees; however, our experimental results demonstrate that stable learning withoutthe need for any modiﬁcations between environments.Interestingly, all of our experiments used substantially fewer steps of experience than was used byDQN learning to ﬁnd solutions in the Atari domain. Nearly all of the problems we looked at weresolved within 2.5 million steps of experience (and usually far fewer), a factor of 20 fewer steps thanDQN requires for good Atari solutions. This suggests that, given more simulation time, DDPG maysolve even more difﬁcult problems than those considered here.A few limitations to our approach remain. Most notably, as with most model-free reinforcementapproaches, DDPG requires a large number training episodes to ﬁnd solutions. However, we believe8
 that a robust model-free approach may be an important component of larger systems which mayattack these limitations [24].References[1] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. “Imagenet classiﬁcation with deepconvolutional neural networks”. In: Advances in neural information processing systems. 2012,pp. 1097–1105.[2] Volodymyr Mnih et al. “Human-level control through deep reinforcement learning”. In: Na-ture 518.7540 (2015), pp. 529–533.[3] David Silver et al. “Deterministic Policy Gradient Algorithms”. In: ICML. 2014.[4] Roland Hafner and Martin Riedmiller. “Reinforcement learning in feedback control”. In: Ma-chine learning 84.1-2 (2011), pp. 137–169.[5] Danil V Prokhorov, Donald C Wunsch, et al. “Adaptive critic designs”. In: Neural Networks,IEEE Transactions on 8.5 (1997), pp. 997–1007.[6] Volodymyr Mnih et al. “Playing atari with deep reinforcement learning”. In: arXiv preprintarXiv:1312.5602 (2013).[7] Sergey Ioffe and Christian Szegedy. “Batch normalization: Accelerating deep network train-ing by reducing internal covariate shift”. In: arXiv preprint arXiv:1502.03167 (2015).[8] Yuval Tassa, Tom Erez, and Emanuel Todorov. “Synthesis and stabilization of complex be-haviors through online trajectory optimization”. In: Intelligent Robots and Systems (IROS),2012 IEEE/RSJ International Conference on. IEEE. 2012, pp. 4906–4913.[9] Christopher JCH Watkins and Peter Dayan. “Q-learning”. In: Machine learning 8.3-4 (1992),pp. 279–292.[10] George E Uhlenbeck and Leonard S Ornstein. “On the theory of the Brownian motion”. In:Physical review 36.5 (1930), p. 823.[11] Paweł Wawrzy´nski. “Control Policy with Autocorrelated Noise in Reinforcement Learn-ing for Robotics”. In: International Journal of Machine Learning and Computing 5 (2015),pp. 91–95.[12] Paweł Wawrzy´nski. “Real-time reinforcement learning by sequential actor–critics and expe-rience replay”. In: Neural Networks 22.10 (2009), pp. 1484–1497.[13] Emanuel Todorov, Tom Erez, and Yuval Tassa. “MuJoCo: A physics engine for model-basedcontrol”. In: Intelligent Robots and Systems (IROS), 2012 IEEE/RSJ International Conferenceon. IEEE. 2012, pp. 5026–5033.[14] Emanuel Todorov and Weiwei Li. “A generalized iterative LQG method for locally-optimalfeedback control of constrained nonlinear stochastic systems”. In: American Control Confer-ence, 2005. Proceedings of the 2005. IEEE. 2005, pp. 300–306.[15] Hado V Hasselt. “Double Q-learning”. In: Advances in Neural Information Processing Sys-tems. 2010, pp. 2613–2621.[16] Jan Koutn´ık, J¨urgen Schmidhuber, and Faustino Gomez. “Online Evolution of Deep Convo-lutional Network for Vision-Based Reinforcement Learning”. In: From Animals to Animats13. Springer, 2014, pp. 260–269.[17] Sergey Levine et al. “End-to-End Training of Deep Visuomotor Policies”. In: arXiv preprintarXiv:1504.00702 (2015).[18] Marc Peter Deisenroth, Gerhard Neumann, Jan Peters, et al. “A Survey on Policy Search forRobotics.” In: Foundations and Trends in Robotics 2.1-2 (2013), pp. 1–142.[19] Paweł Wawrzy´nski and Ajay Kumar Tanwani. “Autonomous reinforcement learning with ex-perience replay”. In: Neural Networks 41 (2013), pp. 156–167.[20] John Schulman et al. “Trust Region Policy Optimization”. In: arXiv preprintarXiv:1502.05477 (2015).[21] Marc Deisenroth and Carl E Rasmussen. “PILCO: A model-based and data-efﬁcient approachto policy search”. In: Proceedings of the 28th International Conference on machine learning(ICML-11). 2011, pp. 465–472.[22] Niklas Wahlstr¨om, Thomas B Sch¨on, and Marc Peter Deisenroth. “From Pixels to Torques:Policy Learning with Deep Dynamical Models”. In: arXiv preprint arXiv:1502.02251 (2015).9
 [23] Jan Koutn´ık, J¨urgen Schmidhuber, and Faustino Gomez. “Evolving deep unsupervised con-volutional networks for vision-based reinforcement learning”. In: Proceedings of the 2014conference on Genetic and evolutionary computation. ACM. 2014, pp. 541–548.[24] Jan Gl¨ascher et al. “States versus rewards: dissociable neural prediction error signals under-lying model-based and model-free reinforcement learning”. In: Neuron 66.4 (2010), pp. 585–595.[25] Diederik Kingma and Jimmy Ba. “Adam: A Method for Stochastic Optimization”. In: arXivpreprint arXiv:1412.6980 (2014).[26] Xavier Glorot, Antoine Bordes, and Yoshua Bengio. “Deep sparse rectiﬁer networks”. In:Proceedings of the 14th International Conference on Artiﬁcial Intelligence and Statistics.JMLR W&CP Volume. Vol. 15. 2011, pp. 315–323.10
 Supplementary Information: Continuous control withdeep reinforcement learning7 Experiment DetailsWe used Adam [25] for learning the neural network parameters with a base learning rate of 10−3and 10−4for the actor and critic respectively. For Qwe included L2weight decay of 10−2andused a discount factor of γ= 0.99. For the soft target updates we used τ= 0.001. The neuralnetworks used the rectiﬁed non-linearity [26] for all hidden layers. The low-dimensional networkshad 2 hidden layers with 400 and 300 units respectively (≈130,000 parameters). Actions were notincluded until the 2nd hidden layer of Q. When learning from pixels we used 3 convolutional layers(no pooling) with 32 ﬁlters at each layer. This was followed by two fully connected layers with200 units (≈430,000 parameters). The ﬁnal layer weights and biases of both the actor and criticwere initialized from a uniform distribution [−3×10−3,3×10−3]and [3 ×10−4,3×10−4]for thelow dimensional and pixel cases respectively. This was to ensure the initial outputs for the policyand value estimates were near zero. The other layers were initialized from uniform distributions[−1√f,1√f]where fis the fan-in of the layer. The actions were not included until the fully-connectedlayers. We trained with minibatch sizes of 64 for the low dimensional problems and 16 on pixels.We used a replay buffer size of 106.For the exploration noise process we used temporally correlated noise in order to explore well inphysical environments that have momentum. We used an Ornstein-Uhlenbeck process [10] withθ= 0.15 and σ= 0.3. The Ornstein-Uhlenbeck process models the velocity of a Brownian particlewith friction, which results in temporally correlated values centered around 0.8 Planning algorithmOur planner is implemented as a model-predictive controller [8]: at every time step we run a singleiteration of trajectory optimization (using iLQG, [14]), starting from the true state of the system.Every single trajectory optimization is planned over a horizon between 250ms and 600ms, and thisplanning horizon recedes as the simulation of the world unfolds, as is the case in model-predictivecontrol.The iLQG iteration begins with an initial rollout of the previous policy, which determines the nom-inal trajectory. We use repeated samples of simulated dynamics to approximate a linear expansionof the dynamics around every step of the trajectory, as well as a quadratic expansion of the costfunction. We use this sequence of locally-linear-quadratic models to integrate the value functionbackwards in time along the nominal trajectory. This back-pass results in a putative modiﬁcation tothe action sequence that will decrease the total cost. We perform a derivative-free line-search overthis direction in the space of action sequences by integrating the dynamics forward (the forward-pass), and choose the best trajectory. We store this action sequence in order to warm-start the nextiLQG iteration, and execute the ﬁrst action in the simulator. This results in a new state, which isused as the initial state in the next iteration of trajectory optimization.9 Environment details9.1 Torcs environmentFor the Torcs environment we used a reward function which provides a positive reward at each stepfor the velocity of the car projected along the track direction and a penalty of −1for collisions.Episodes were terminated if progress was not made along the track after 500 frames.9.2 MuJoCo environmentsFor physical control tasks we used reward functions which provide feedback at every step. In alltasks, the reward contained a small action cost. For all tasks that have a static goal state (e.g.pendulum swingup and reaching) we provide a smoothly varying reward based on distance to a goal11
 state, and in some cases an additional positive reward when within a small radius of the target state.For grasping and manipulation tasks we used a reward with a term which encourages movementtowards the payload and a second component which encourages moving the payload to the target. Inlocomotion tasks we reward forward action and penalize hard impacts to encourage smooth ratherthan hopping gaits [20]. In addition, we used a negative reward and early termination for falls whichwere determined by simple threshholds on the height and torso angle (in the case of walker2d).Table 2 states the dimensionality of the problems and below is a summary of all the physics envi-ronments.task name dim(s) dim(a) dim(o)blockworld1 18 5 43blockworld3da 31 9 102canada 22 7 62canada2d 14 3 29cart 2 1 3cartpole 4 1 14cartpoleBalance 4 1 14cartpoleParallelDouble 6 1 16cartpoleParallelTriple 8 1 23cartpoleSerialDouble 6 1 14cartpoleSerialTriple 8 1 23cheetah 18 6 17ﬁxedReacher 10 3 23ﬁxedReacherDouble 8 2 18ﬁxedReacherSingle 6 1 13gripper 18 5 43gripperRandom 18 5 43hardCheetah 18 6 17hardCheetahNice 18 6 17hopper 14 4 14hyq 37 12 37hyqKick 37 12 37movingGripper 22 7 49movingGripperRandom 22 7 49pendulum 2 1 3reacher 10 3 23reacher3daFixedTarget 20 7 61reacher3daRandomTarget 20 7 61reacherDouble 6 1 13reacherObstacle 18 5 38reacherSingle 6 1 13walker2d 18 6 41Table 2: Dimensionality of the MuJoCo tasks: the dimensionality of the underlying physics modeldim(s), number of action dimensions dim(a)and observation dimensions dim(o).task name Brief Descriptionblockworld1 Agent is required to use an arm with gripper constrained to the 2D planeto grab a falling block and lift it against gravity to a ﬁxed target position.blockworld3da Agent is required to use a human-like arm with 7-DOF and a simplegripper to grab a block and lift it against gravity to a ﬁxed target posi-tion.canada Agent is required to use a 7-DOF arm with hockey-stick like appendageto hit a ball to a target.12
 canada2d Agent is required to use an arm with hockey-stick like appendage to hita ball initialzed to a random start location to a random target location.cart Agent must move a simple mass to rest at 0. The mass begins each trialin random positions and with random velocities.cartpole The classic cart-pole swing-up task. Agent must balance a pole at-tached to a cart by applying forces to the cart alone. The pole startseach episode hanging upside-down.cartpoleBalance The classic cart-pole balance task. Agent must balance a pole attachedto a cart by applying forces to the cart alone. The pole starts in theupright positions at the beginning of each episode.cartpoleParallelDouble Variant on the classic cart-pole. Two poles, both attached to the cart,should be kept upright as much as possible.cartpoleSerialDouble Variant on the classic cart-pole. Two poles, one attached to the cart andthe second attached to the end of the ﬁrst, should be kept upright asmuch as possible.cartpoleSerialTriple Variant on the classic cart-pole. Three poles, one attached to the cart,the second attached to the end of the ﬁrst, and the third attached to theend of the second, should be kept upright as much as possible.cheetah The agent should move forward as quickly as possible with a cheetah-like body that is constrained to the plane. This environment is basedvery closely on the one introduced by Wawrzy´nski [12, 19].ﬁxedReacher Agent is required to move a 3-DOF arm to a ﬁxed target position.ﬁxedReacherDouble Agent is required to move a 2-DOF arm to a ﬁxed target position.ﬁxedReacherSingle Agent is required to move a simple 1-DOF arm to a ﬁxed target position.gripper Agent must use an arm with gripper appendage to grasp an object andmanuver the object to a ﬁxed target.gripperRandom The same task as gripper except that the arm object and target posi-tion are initialized in random locations.hardCheetah The agent should move forward as quickly as possible with a cheetah-like body that is constrained to the plane. This environment is basedvery closely on the one introduced by Wawrzy´nski [12, 19], but has beenmade much more difﬁcult by removing the stabalizing joint stiffnessfrom the model.hopper Agent must balance a multiple degree of freedom monoped to keep itfrom falling.hyq Agent is required to keep a quadroped model based on the hyq robotfrom falling.movingGripper Agent must use an arm with gripper attached to a moveable platform tograsp an object and move it to a ﬁxed target.movingGripperRandom The same as the movingGripper environment except that the object po-sition, target position, and arm state are initialized randomly.13
 pendulum The classic pendulum swing-up problem. The pendulum should bebrought to the upright position and balanced. Torque limits prevent theagent from swinging the pendulum up directly.reacher3daFixedTarget Agent is required to move a 7-DOF human-like arm to a ﬁxed targetposition.reacher3daRandomTarget Agent is required to move a 7-DOF human-like arm from random start-ing locations to random target positions.reacher Agent is required to move a 3-DOF arm from random starting locationsto random target positions.reacherSingle Agent is required to move a simple 1-DOF arm from random startinglocations to random target positions.reacherObstacle Agent is required to move a 5-DOF arm around an obstacle to a ran-domized target position.walker2d Agent should move forward as quickly as possible with a bipedal walkerconstrained to the plane without falling down or pitching the torso toofar forward or backward.14
 CitationsCitations132ReferencesReferences21The proposed approach uses a discrete number of actions to perform the peg-in-hole task. As an obvious next step, we will analyze the difference between this approach and continuous space learning techniques such as A3C[14]and DDPG[15]. Deep Reinforcement Learning for High Precision Assembly Tasks[Show abstract] [Hide abstract] ABSTRACT: High precision assembly of mechanical parts requires accuracy exceeding the robot precision. Conventional part mating methods used in the current manufacturing requires tedious tuning of numerous parameters before deployment. We show how the robot can successfully perform a tight clearance peg-in-hole task through training a recurrent neural network with reinforcement learning. In addition to saving the manual effort, the proposed technique also shows robustness against position and angle errors for the peg-in-hole task. The neural network learns to take the optimal action by observing the robot sensors to estimate the system state. The advantages of our proposed method is validated experimentally on a 7-axis articulated robot arm. Full-text · Conference Paper · Sep 2017 Tadanobu InoueGiovanni De MagistrisAsim Munawar+1 more author...Tsuyoshi YokoyaRead full-textBut for a mobile robot motion planner, the continuous actions space is obviously necessary. In this paper, we use DDPG [2] to train our model and extend it to an asynchronous version, as described in Section III. As presented in Fig. 1and the definition function, the abstracted 10-dimensional laser range findings, the previous action and the relative target position are merged together as a 14-dimensional input vector. Virtual-to-real Deep Reinforcement Learning: Continuous Control of Mobile Robots for Mapless Navigation[Show abstract] [Hide abstract] ABSTRACT: Deep Reinforcement Learning has been successful in various virtual tasks, but it is still rarely used in real world applications especially for continuous control of mobile robots navigation. In this paper, we present a learning-based mapless motion planner by taking the 10-dimensional range findings and the target position as input and the continuous steering commands as output. Traditional motion planners for mobile ground robots with a laser range sensor mostly depend on the map of the navigation environment where both the highly precise laser sensor and the map building work of the environment are indispensable. We show that, through an asynchronous deep reinforcement learning method, a mapless motion planner can be trained end-to-end without any manually designed features and prior demonstrations. The trained planner can be directly applied in unseen virtual and real environments. We also evaluated this learning-based motion planner and compared it with the traditional motion planning method, both in virtual and real environments. The experiments show that the proposed mapless motion planner can navigate the nonholonomic mobile robot to the desired targets without colliding with any obstacles. Full-text · Conference Paper · Sep 2017 Lei TaiGiuseppe PaoloMing LiuRead full-textNone of these works, all from different years, refer to each other. Besides Direct Perception, the only unique work for racing is Deep DPG[64], which extends DQN for continuous controls. Of the major work on StarCraft, half of it has roots in Qlearning. Deep Learning for Video Game Playing[Show abstract] [Hide abstract] ABSTRACT: In this paper we review recent Deep Learning advances in the context of how they have been applied to play different types of video games such as first-person shooters, arcade games or real-time strategy games. We analyze the unique requirements that different game genres pose to a deep learning system and highlight important open challenges in the context of applying these machine learning methods to video games, such as general game playing, dealing with extremely large decision spaces and sparse rewards.Article · Aug 2017 Niels JustesenPhilip BontragerJulian TogeliusSebastian RisiSebastian RisiReadThe actor network and the critic network are trained via the Adam algorithm, a gradient descent algorithm presented in[11], and the learning rates of these two networks are 10 −5 and 10 −7 respectively (our experiments show that the algorithms converge to better solutions with lower learning rates-hence the choice of the small values). Following the same idea as in[12], we add Gaussian noise to the action outputted by the actor network, with the mean of the noise decaying with the number of episodes in the exploration. Reinforcement Mechanism Design for e-commerce[Show abstract] [Hide abstract] ABSTRACT: We study the problem of allocating impressions to sellers in e-commerce websites, such as Amazon, eBay or Taobao, aiming to maximize the total revenue generated by the platform. When a buyer searches for a keyword, the website presents the buyer with a list of different sellers for this item, together with the corresponding prices. This can be seen as an instance of a resource allocation problem in which the sellers choose their prices at each step and the platform decides how to allocate the impressions, based on the chosen prices and the historical transactions of each seller. Due to the complexity of the system, most e-commerce platforms employ heuristic allocation algorithms that mainly depend on the sellers transaction records and without taking the rationality of the sellers into account, which makes them susceptible to price manipulations. In this paper, we put forward a general framework of reinforcement mechanism design, which uses deep reinforcement learning to design efficient algorithms, taking the strategic behaviour of the sellers into account. We apply the framework to the problem of allocating impressions to sellers in large e-commerce websites, a problem which is modeled as a Markov decision process, where the states encode the history of impressions, prices, transactions and generated revenue and the actions are the possible impression allocations at each round. To tackle the problem of continuity and high-dimensionality of states and actions, we adopt the ideas of the DDPG algorithm to design an actor-critic gradient policy algorithm which takes advantage of the problem domain in order to achieve convergence and stability. Our algorithm is compared against natural heuristics and it outperforms all of them in terms of the total revenue generated. Finally, contrary to the DDPG algorithm, our algorithm is robust to settings with variable sellers and easy to converge.Article · Aug 2017 Qingpeng CaiAris Filos-RatsikasPingzhong TangYiwei ZhangYiwei ZhangReadAgents using deep reinforcement learning (deep RL) methods have shown tremendous success in learning complex behaviour skills and solving challenging control tasks in high-dimensional raw sensory state-space[25,18,13]. Deep RL methods make use of deep neural networks to represent control policies. Scalable trust-region method for deep reinforcement learning using Kronecker-factored approximation[Show abstract] [Hide abstract] ABSTRACT: In this work, we propose to apply trust region optimization to deep reinforcement learning using a recently proposed Kronecker-factored approximation to the curvature. We extend the framework of natural policy gradient and propose to optimize both the actor and the critic using Kronecker-factored approximate curvature (K-FAC) with trust region; hence we call our method Actor Critic using Kronecker-Factored Trust Region (ACKTR). To the best of our knowledge, this is the first scalable trust region natural gradient method for actor-critic methods. It is also a method that learns non-trivial tasks in continuous control as well as discrete control policies directly from raw pixel inputs. We tested our approach across discrete domains in Atari games as well as continuous domains in the MuJoCo environment. With the proposed methods, we are able to achieve higher rewards and a 2- to 3-fold improvement in sample efficiency on average, compared to previous state-of-the-art on-policy actor-critic methods. Code is available at https://github.com/openai/baselines.Article · Aug 2017 Yuhuai WuElman MansimovShun Liao+1 more author...Roger GrosseReadThe proposed approach uses a discrete number of actions to perform the peg-in-hole task. As an obvious next step, we will analyze the difference between this approach and continuous space learning techniques such as A3C[14]and DDPG[15]. Deep Reinforcement Learning for High Precision Assembly Tasks[Show abstract] [Hide abstract] ABSTRACT: High precision assembly of mechanical parts requires accuracy exceeding the robot precision. Conventional part mating methods used in the current manufacturing requires tedious tuning of numerous parameters before deployment. We show how the robot can successfully perform a tight clearance peg-in-hole task through training a recurrent neural network with reinforcement learning. In addition to saving the manual effort, the proposed technique also shows robustness against position and angle errors for the peg-in-hole task. The neural network learns to take the optimal action by observing the robot sensors to estimate the system state. The advantages of our proposed method is validated experimentally on a 7-axis articulated robot arm. Full-text · Article · Aug 2017 Tadanobu InoueGiovanni De MagistrisAsim Munawar+1 more author...Tsuyoshi YokoyaRead full-textShow moreRecommended publicationsDiscover more publications, questions and projects in Reinforcement LearningArticleMemory-based control with recurrent neural networksDecember 2015Partially observed control problems are a challenging aspect of reinforcement
-- deterministic policy gradient and stochastic value gradient -- to solve
We demonstrate that this approach, coupled with long-short term... [Show full abstract]Read moreArticleLearning Continuous Control Policies by Stochastic Value GradientsOctober 2015We present a unified framework for learning continuous control policies using
the Bellman equation as a deterministic function of exogenous noise. The
product is a spectrum of general policy gradient algorithms that range from
functions. We... [Show full abstract]Read moreArticleData-efficient Deep Reinforcement Learning for Dexterous ManipulationApril 2017Deep learning and reinforcement learning methods have recently been used to solve a variety of problems in continuous control domains. An obvious application of these techniques is dexterous manipulation tasks in robotics which are difficult to solve using traditional control theory or hand-engineered approaches. One example of such a task is to grasp an object and precisely stack it on... [Show full abstract]Read moreConference PaperSynthesis and stabilization of complex behaviors through online trajectory optimizationOctober 2012 · Proceedings of the ... IEEE/RSJ International Conference on Intelligent Robots and Systems. IEEE/RSJ International Conference on Intelligent Robots and SystemsWe present an online trajectory optimization method and software platform applicable to complex humanoid robots performing challenging tasks such as getting up from an arbitrary pose on the ground and recovering from large disturbances using dexterous acrobatic maneuvers. The resulting behaviors, illustrated in the attached video, are computed only 7 × slower than real time, on a standard PC.... [Show full abstract]Read moreDiscover moreData provided are for informational purposes only. Although carefully collected, accuracy cannot be guaranteed. Publisher conditions are provided by RoMEO. Differing provisions from the publishers actual policy or licence agreement may be applicable.This publication is from a journal that may support self archiving.Learn more 
