 1 Mathematics of Deep Learning CVPR Tutorial, Las Vegas, USA, June 26th 2016 Joan Bruna (UC Berkeley), Raja Giryes (Tel Aviv University), Ben Haeffele (Hopkins), Guillermo Sapiro (Duke), Amnon Shashua (Hebrew University of Jerusalem), Ren Vidal (Hopkins)2 Learning Deep Image Feature Hierarchies Deep learning gives ~ 10% improvement on ImageNet 1.2M images 1000 categories 60 million parameters [1] Krizhevsky, Sutskever and Hinton. ImageNet classification with deep convolutional neural networks, NIPS 12. [2] Sermanet, Eigen, Zhang, Mathieu, Fergus, LeCun. Overfeat: Integrated recognition, localization and detection using convolutional networks. ICLR 14. [3] Donahue, Jia, Vinyals, Hoffman, Zhang, Tzeng, Darrell. Decaf: A deep convolutional activation feature for generic visual recognition. ICML 14.3 Impact of Deep Learning in Computer Vision classification results in ImageNet CNN non-cnn 2015 results: MSR under 3.5% error using 150 layers! Slide from Yann LeCun s CVPR 15 plenary and ICCV 15 tutorial intro by Joan Bruna 4 Transfer from ImageNet to Other Datasets plane bike bird boat btl bus car cat chair cow table dog horse moto pers plant sheep sofa train tv map INRIA [32] CNNs + SMVs [1] NUS-PSL [44] PRE-1000C map Table 1: Per-class results for object classification on the VOC2007 test set (average precision %). Pascal VOC 2007 plane bike bird boat btl bus car cat chair cow table dog horse moto pers plant sheep sofa train tv map GHM[8] NUS-PSL [49] NO PRETRAIN AGS[11] 71.1 PRE-1000C NUS[39] 70.5 PRE-1000R PRE CNN-SVM 73.9 Training images Source task Source task labels horse moto pers plant CNNaug-SVM sheep sofa 77.2 train Table tv 2: Per-class plane map bike results bird for object boat classification btl bus on the car VOC2012 cat test chair set (average cow precision table %). dog horse moto pers plant sheep s African elephant Convolutional layers Fully-connected layers aero bike 36.3 bird44.7boat50.6 INRIA bottle79.2 [32] bus 53.2 car cat chair 63.6 cow 56.1 table 71.9 dog 33.1 horse 60.6 mbike 78.0 person plant 42.6 sheep 54.9 sofa train 45.8 tv 77.5mAP Action jumpphon instr read bike horse run phot compwalk 1 : Feature map Wall clock GHM[8] NUS-PSL [44] learning action recognition task, C1-C2-C3-C4-C and 55.3 inspired FC by [6], 6 FC our 64.7 last FC re were obtained by training the target 86.3 task dim CNN 71.1with- Green snake STANFORD [1] or AGS[11] sults OXFORD 58.0 [1] PRE-1000C vector NUS[39] NO73.4 PRETRAIN out76.8 freezing91.1 the FC weights More83.6 precisely, 70.6 we 70.5copy Yorkshire terrier 007 CNN-SVM test set Retrain (average precision 83.5 PRE-1512 top-layer 82.0%) [2] Table : Per-class : Feature 77.8 results for the 78.8 ImageNet-trained object90.2 classification 54.8weights 71.1on of62.6 the layers VOC Transfer C1...C5, 71.8 FC6 test 73.9 and set (average precision % transfer parameters CNNaug-SVM PRE-1512U FC7, 80.0 we append 92.0 the 56.9adaptation layers89.1 FCa 74.9 and FCb, 77.2 and Chair horse moto pers plant sheep Table 3: sofa Pascal train VOC 2012 tv action plane mapclassification bike bird results boat (AP %). btl bus we retrain car layers cat chair FC6, FCa, cowandtable FCb on dog the action horse recognition 86.8 data This strategy 75.4 C1-C2-C3-C4-C increases 75.1the FC moto pers plant sheep Table 1: Pascal VOC 2007 Image mapclassification Results compared to other methods which also use training data outside VOC. The CNN representation Background NUS-PSL 94.5 [49] : Classifier performance 6 FC on FCa90.1 all FCb learning 4096 is71.4 not tuned 82.7 INRIA for the 93.1 [32] Pascal 59.1 VOC dataset. sessment 49.3 However, NO of localization PRETRAIN 80.0GHM 76.7[8] results, 85.2 learns 70.9wefrom 75.0 compute VOC 69.4 ana joint output 66.2 representation map action of bag-of-visual-words categories (row PRE-1512U and 61.9 contextual in Table 49.8 information. 3), yielding, to dim Person dim 4096 or vector AGS [11] learns NUS-PSL a second [44] layer70.5 of representation clustering VOC data into subcategories. NUS [39] trains a codebook for the SIFT, vector HOG 6144-dim for each54.3 PRE-1000C category 88.0by78.3 averaging the78.4 scores87.7 of all the 80.9testing the best of our knowledge, the best average result published and LBP vector TV/monitor 79.0 New adaptation PRE-1000C 77.7 patches covering a given pixel of the test image. Examples on the Pascal VOC 2012 action recognition task. Training images descriptors from the VOC dataset. Oquab et al.[29] adapt the CNN classification layers and achieves Sliding patches better results layers trained PRE-1000R Target (77.7) 59.5 task indicating Target 59.8 task labels74.9 on target task of such output maps are given in Figures 1 and 5 as well To demonstrate that we can also localize the action in the the 93.4 potential 88.6 to96.1 boost 64.3 the performance PRE-1512 by 91.1further 79.8 adaptation of88.2 the representation to the 84.4 target 90.7task/dataset as on the project webpage [2]. This visualization clearly image, we train the network in a sliding window manner, as test set (average precision demonstrates %). that the system knows the size and locations described in section 3. In particular, we use the ground truth of target objects within the image. Addressing the detection Method person bounding boxes duringmean training, Accuracy but do not use the task seems within reach. ROI + Gist[36] ground truth person bounding boxes26.1 at test time. Example ognition task, and inspired Action by [6], ourjump lastphon reobtained by training the nitiontarget task consists task of CNN 4588 with- training images and 4569 test instr read bike DPM[30] horse run output phot compwalk maps shownmap in figure 6 clearly 30.4 demonstrate that the Action recognition. The Pascal VOC 2012 action recog- Deep Face STANFORD [3] [1] Object 94.2 Bank[24] 87.6network provides 75.6 an 69.1 estimate of the37.6 action location in the mean AP OXFORD [1] RBow[31] image. images featuring people performing actions among ten categories ng 1 the FC6 weights. Moresuch NO precisely, as PRETRAIN jumping, we 43.2 phoning, copy BoP[21] out 46.1 freezing the FC6 weights. playing instrument Failure modes. Top-ranked false positives in Figure 5 Net-trained 0.8 weights of or layers reading. PRE-1512 C1...C5, This fine-grained FC and task 74.8 differs from misvm[25] 94.3 the 83.4correspond to 66.8 samples 68.4 closely resembling 46.4 target object D-Parts[40] ppend the adaptation object layersclassification PRE-1512U FCa andtask FCb, 74.8 because 46.0 and it 75.6 entails 45.3recognizing classes Resolving some of these errors may require highlevel scene interpretation. Our method may also fail to IFV[21] 60.8 fine differences in human poses (e.g. running v.s. 0.4 layers FC6, FCa, and FCb on Table the3: action Pascal recog- VOC 2012 actionmlrep[9] classification results (AP %) walking) and subtle interactions with objects (phoning recognize spatially co-occurring objects (e.g., person on a [1] Razavian, Azizpour, Sullivan, Carlsson, CNN Features off-the-shelf: an Astounding Baseline for Recognition. CVPRW This strategy increases the performance on all [2] 3 Oquab, 7 11 Bottou, Laptev, 23 Sivic. orlearning taking and photo). transferring Training mid-level samplesimage with multiple representations CNN-SVM simultaneous DeepFace: in Table actionsclosing 3), are excluded yielding, the Gap from to tohuman-level our training set. Performance CNNaug-SVM in Face chair) since patches with multiple objects using convolutional neural networks CVPR are currently excluded [3] gories Taigman, (row level Yang, PRE-1512U Ranzato, Wolf. Verification. from training. CVPR 14 This issue could 69.0 be addressed by sessment of localization results, CNN(AlexConvNet)+multiscale we compute an outputpooling map To evaluate how our transfer method performs this changing the training objective [16] to allow 68.9multiple labels per our knowledge, (a) the best average result (b) published sample. Recognition of very small or very large objects Table 2: Per-class results for object classification on the VOC2012 test set (average precision for each category by averaging the scores of all the testing action recognition task, and inspired sults were obtained by training the ta More the ImageNet-trained weights of layer FC7, we append the adaptation layer we retrain layers FC6, FCa, and FCb nition data. This strategy increases th action categories (row PRE-1512U in the best of our knowledge, the best ave 5 Ross Girshick Jeff Donahue Trevor Darrell Jitendra Malik UC Berkeley Transfer from Classification to Other Tasks CNNs + SVMs for object detection [1,2] bstract sheep sofa VOCtrain 2010 test tv aero map bike bird DPM43.8 v5 [20] UVA [39]measured on 42.4 the 15.3 ormance, as Regionlets [41] last 25.9 dataset, has 54.0 plateaued in the 38.7 methods 35.0 SegDPM 52.8[18] 43.1 complex en25.6 rming are 56.5combine 38.1 R-CNN ally multiple low-level 59.4 context R-CNN 61.2 BB we 53.0 level In52.4 this71.8 paper, boat bottle bus Input 35.9 image59.7 R-CNN: Regions car cat chair cow with tablecnn dog features horse mbike person plant sh warped 28.8 region CNN region Extract 3. Compute (~2k) CNN 41.4 features proposals aeroplane? no person? yes no. tvmonitor? Classify 69.0regions average precision (AP) in % UVA andtable Regionlets 1:algorithm Detection since allthat average (%)1:onObject VOC 2010 test. R-CNN most directly able detection im- precision Figure detection system is overview. Our comparable system (1) to UV At publication methods time, SegDPM selective regiontakes proposals. Bounding-box regression (BB) 2000 is described in Section C. At cision (map) byuse more thansearch 30% an input image, semantic (2) extracts around bottom-up region CNNs for pose estimation [3] and segmentation [4] ed by thewas other themethods. top-performer on the PASCAL VOC leaderboard. DPM and SegDPM contextusing rescoring not used b proposals, (3) computes features for eachuse proposal a large t result on VOC 2012 achieving convolutional neural network (CNN), and then (4) classifies each roach combines two key insights: ILSVRC2013 detection test set class-specific map region using linear SVMs. R-CNN achieves adetection mean test s st set class AP box plots neural ILSVRC2013 pacity convolutional net100 BB up region*r CNN proposals in order to 31.4% average precision (map) of 53.7% on90pascal VOC For comparison, [39] reports 35.1% map using the same region procts and (2)*OverFeat when labeled training 24.3% (2) 80 bag-of-visual-words apposals, but with a spatial pyramid and pre-training for an auxiliary task, 22.6% UvA Euvision proach. The popular deformable part 70 models perform at 33.4%. fic fine-tuning, yields a significant 60 On the 200-class ILSVRC2013 detection dataset, R-CNN s 20.9% *NEC MU e we combine region proposals map is 31.4%, a large improvement 50 over OverFeat [34], which 19.4% *OverFeat (1) 40 ethod R-CNN: Regions with CNN had the previous best result at 24.3%. Girshick, Donahue, Darrell A and Malik. feature hierarchies for accurate object detection and semantic segmentation, CVPR % e [1] R-CNN to Toronto OverFeat, a Rich recently [2] Sermanet, Eigen, Zhang, Mathieu, Fergus, LeCun. OverFeat: Integrated Recognition, Localization and Detection using Convolutional Networks. ICLR [3] Tompson, Goroshin, Jain, LeCun, Bregler. Efficient Object Localization Using Convolutional Networks. CVPR detector based on a similar CNN SYSU_Vision archical, [4] Pinheiro, Collobert, Dollar. Learning to 10.5% Segment Object Candidates. NIPS 15 multi-stage processes for computing features that 106 Why These Improvements in Performance? Features are learned rather than hand-crafted More layers capture more invariances [1] mean AP More data to train deeper networks More computing (GPUs) Better regularization: Dropout New nonlinearities Max pooling, Rectified linear units (ReLU) level Theoretical understanding of deep networks remains shallow [1] Razavian, Azizpour, Sullivan, Carlsson, CNN Features off-the-shelf: an Astounding Baseline for Recognition. CVPRW 14.7 Early Theoretical Results on Deep Learning Approximation theory Perceptrons and multilayer feedforward networks are universal approximators: Cybenko 89, Hornik 89, Hornik 91, Barron 93 Optimization theory No spurious local optima for linear networks: Baldi & Hornik 89 Stuck in local minima: Brady 89 Stuck in local minima, but convergence guarantees for linearly separable data: Gori & Tesi 92 Manifold of spurious local optima: Frasconi 97 [1] Cybenko. Approximations by superpositions of sigmoidal functions, Mathematics of Control, Signals, and Systems, 2 (4), , [2] Hornik, Stinchcombe and White. Multilayer feedforward networks are universal approximators, Neural Networks, 2(3), , [3] Hornik. Approximation Capabilities of Multilayer Feedforward Networks, Neural Networks, 4(2), , [4] Barron. Universal approximation bounds for superpositions of a sigmoidal function. IEEE Transactions on Information Theory, 39(3): , [5] P Baldi, K Hornik, Neural networks and principal component analysis: Learning from examples without local minima, Neural networks, [6] Brady, Raghavan, Slawny. Back propagation fails to separate where perceptrons succeed. IEEE Trans Circuits & Systems, 36(5): , [7] Gori, Tesi. On the problem of local minima in backpropagation. IEEE Trans. on Pattern Analysis and Machine Intelligence, 14(1):76 86, [8] Frasconi, Gori, Tesi. Successes and failures of backpropagation: A theoretical. Progress in Neural Networks: Architecture, 5:205, 1997.8 Recent Theoretical Results on Deep Learning Invariance, stability, and learning theory Scattering networks: Bruna 11, Bruna 13, Mallat 13 Deformation stability for Lipschitz non-linearities: Wiatowski 15 Distance and margin-preserving embeddings: Giryes 15, Sokolik 16 Geometry, generalization bounds and depth efficiency: Montufar 15, Neyshabur 15, Shashua Optimization theory and algorithms Learning low-degree polynomials from random initialization: Andoni 14 Characterizing loss surface and attacking the saddle point problem: Dauphin 14, Choromanska 15, Chaudhuri 15 Global optimality in neural network training: Haeffele 15 Training NNs using tensor methods: Janzamin 15 [1] Bruna-Mallat. Classification with scattering operators, CVPR 11. Invariant scattering convolution networks, arxiv 12. Mallat-Waldspurger. Deep Learning by Scattering, arxiv 13. [2] Wiatowski, Blcskei. A mathematical theory of deep convolutional neural networks for feature extraction. arxiv [3] Giryes, Sapiro, A Bronstein. Deep Neural Networks with Random Gaussian Weights: A Universal Classification Strategy? arxiv: [4] Sokolic. Margin Preservation of Deep Neural Networks, 2015 [5] Montufar. Geometric and Combinatorial Perspectives on Deep Neural Networks, [6] Neyshabur. The Geometry of Optimization and Generalization in Neural Networks: A Path-based Approach, [7] Andoni, Panigraphy, Valiant, Zhang. Learning Polynomials with Neural Networks. ICML [8] Dauphin, Pascanu, Gulcehre, Cho, Ganguli, Bengio, Identifying and attacking the saddle point problem in high-dimensional non-convex optimization. NIPS [9] Choromanska, Henaff, Mathieu, Arous, LeCun, The Loss Surfaces of Multilayer Networks, AISTAT [10] Chaudhuri and Soatto The Effect of Gradient Noise on the Energy Landscape of Deep Networks, arxiv [11] Haeffele, Vidal. Global Optimality in Tensor Factorization, Deep Learning and Beyond, arxiv, [12] Janzamin, Sedghi, Anandkumar, Beating the Perils of Non-Convexity: Guaranteed Training of Neural Networks using Tensor Methods, arxiv 2015.9 Motivations and Goals of this Tutorial Motivation: Deep networks have led to dramatic improvements in performance for many tasks, but the mathematical reasons for this success remain unclear. Goal: Review very recent work that aims at understanding the mathematical reasons for the success of deep networks. What we will do: Study theoretical questions such as What properties of images are being captured/exploited by DNNs? Can we ensure that the learned representations are globally optimal? Can we ensure that the learned representations are stable? What we will not do: Show X% improvement in performance for a particular application.10 Tutorial Schedule 14:00-14:15: Ren Vidal - Introduction 14:15-15:00: Amnon Shashua - On Depth Efficiency of Convolutional Networks: Theory and Implications for Practical Architectures 15:00-15:45: Ren Vidal and Benjamin Haeffele - Global Optimality and Regularization in Deep Learning 15:45-16:15: Coffee Break 16:15-17:00: Raja Giryes and Guillermo Shapiro - Data Structure Based Theory of Deep Learning 17:00-17:45: Joan Bruna - Addressing Curse of Dimensionality with Convolutional Neural Networks 17:45-18:00: Discussion 
 The Mathematics of Deep Learning ICCV Tutorial, Santiago de Chile, December 12, 2015 Joan Bruna (Berkeley), Raja Giryes (Duke), Guillermo Sapiro (Duke), Rene Vidal (Johns Hopkins) Motivations and Goals 
 R-CNN for Object Detection Ross Girshick, Jeff Donahue, Trevor Darrell, Jitendra Malik (UC Berkeley) presented by Ezgi Mercan 10/3/2014 CSE590V 14Au 1 Outline 1. Problem Statement: Object Detection (and 
 Willow project-team Learning and transferring mid-level image representions using convolutional neural networks Maxime Oquab, Léon Bottou, Ivan Laptev, Josef Sivic 1 Image classification (easy) Is there 
 CS 1699: Intro to Computer Vision Deep Learning Prof. Adriana Kovashka University of Pittsburgh December 1, 2015 Today: Deep neural networks Background Architectures and basic operations Applications Visualizing 
 : Intro. to Computer Vision Deep learning University of Massachusetts, Amherst April 19/21, 2016 Instructor: Subhransu Maji Finals (everyone) Thursday, May 5, 1-3pm, Hasbrouck 113 Final exam Tuesday, May 
 Deformable Part Models with CNN Features Pierre-André Savalle 1, Stavros Tsogkas 1,2, George Papandreou 3, Iasonas Kokkinos 1,2 1 Ecole Centrale Paris, 2 INRIA, 3 TTI-Chicago Abstract. In this work we 
 Learning and Vision Group, NUS, ILSVRC 2014 NIN, Good! ( 您好 ) (Network in Network) Jian DONG, Min LIN, Yunchao WEI, Qiang CHEN*, Wei, XIA, Hanjiang LAI, Shuicheng YAN eleyans@nus.edu.sg National University 
 Object Detection based on Convolutional Neural Network Shijian Tang Department of Electrical Engineering Stanford University sjtang@stanford.edu Ye Yuan Department of Computer Science Stanford University 
 Spatial Localization and Detection Sung-eui Yoon, 2016 Slide Credits: Ric Poirson, Justin Johnson, Andrej Karpathy, Fei-Fei Li, Svetlana Lazebnik Lecture 8-1 1 Feb 2016 Localization and Detection Results 
 Convolutional Feature Maps Elements of efficient (and accurate) CNN-based object detection Kaiming He Microsoft Research Asia (MSRA) ICCV 2015 Tutorial on Tools for Efficient Object Detection Overview 
 Rich feature hierarchies for accurate object detection and semantic segmentation Tech report (v5) Ross Girshick Jeff Donahue Trevor Darrell Jitendra Malik UC Berkeley {rbg,jdonahue,trevor,malik}@eecs.berkeley.edu 
 Lecture 4: Convolutional Neural Networks for Computer Vision Deep Learning @ UvA UVA DEEP LEARNING COURSE EFSTRATIOS GAVVES DEEPER INTO DEEP LEARNING AND OPTIMIZATIONS - 1 Previous lecture o How to define 
 Global Optimality in Matrix and Tensor Factorization, Deep Learning & Beyond Ben Haeffele and René Vidal Center for Imaging Science Johns Hopkins University Impact of Deep Learning in Computer Vision 2012-2014 
 Deep learning with non-medical training used for chest pathology identification Yaniv Bar 1, Idit Diamant 2, Lior Wolf 1, Hayit Greenspan 2 1 The Blavatnik School of Computer Science, Tel-Aviv University, 
 Learning Robust Deep Face Representation Xiang Wu University of Science and Technology Beijing Beijing, China alfredxiangwu@gmail.com arxiv:1507.04844v1 [cs.cv] 17 Jul 2015 Abstract With the development 
 Part Detector Discovery in Deep Convolutional Neural Networks Marcel Simon, Erik Rodner, and Joachim Denzler Computer Vision Group, Friedrich Schiller University of Jena, Germany www.inf-cv.uni-jena.de 
 CSED703R: Deep Learning for Visual Recognition (206S) Lecture 6: CNNs for Detection, Tracking, and Segmentation Object Detection Bohyung Han Computer Vision Lab. bhhan@postech.ac.kr 2 3 Object detection 
 Recent developments in object detection 80% PASCAL VOC mean0average0precision0(map) 70% 60% 50% 40% 30% 20% 10% Before deep convnets Using deep convnets 0% 2006 2007 2008 2009 2010 2011 2012 2013 2014 
 Image Classification for Dogs and Cats Bang Liu, Yan Liu Department of Electrical and Computer Engineering {bang3,yan10}@ualberta.ca Kai Zhou Department of Computing Science kzhou3@ualberta.ca Abstract 
 RCNN, Fast RCNN, Faster RCNN Topics of the lecture: Problem statement Review of slow R-CNN Review of Fast R-CNN Review of Faster R-CNN Presented by: Roi Shikler & Gil Elbaz Advisor: Prof. Michael Lindenbaum 
 Localization and Detection in Dense Environment for Tracking Pallabi Ghosh, Shabaz Basheer Patel Stanford University pallabig@stanford.edu,shabaz@stanford.edu Abstract Given an image from a video, we learn 
 Fine-tuning for Image Style Recognition Yinghui Xia Stanford University 450 Serra Mall, Stanford, CA 94305 yinghui@stanford.edu Abstract The style of an image contains some features and information about 
 Abstract (CNN) ImageNet CNN CNN CNN fine-tuning 1 ( )[14] ( 1) ( ). (CNN)[27] CNN (pre-trained network) pre-trained CNN 2 2.1 (CNN) CNN CNN [18] 2 ( ). Neocognitron[10] 1990 LeCun [27] CNN 2000 CNN [34, 
 Compacting ConvNets for end to end Learning Jose M. Alvarez Joint work with Lars Pertersson, Hao Zhou, Fatih Porikli. Success of CNN Image Classification Alex Krizhevsky, Ilya Sutskever, Geoffrey E. Hinton, 
 Object Detection for Semantic SLAM using Convolution Neural Networks Saumitro Dasgupta saumitro@cs.stanford.edu 1 Introduction Conventional SLAM (Simultaneous Localization and Mapping) systems typically 
 Image and Video Understanding 2VO 710.095 WS Christoph Feichtenhofer, Axel Pinz Slide credits: Many thanks to all the great computer vision researchers on which this presentation relies on. Most material 
 Learning and Transferring Mid-Level Image Representations using Convolutional Neural Networks Maxime Oquab 1, Leon Bottou 2 Ivan Laptev 1, Josef Sivic 1, 1 INRIA, Paris, France 2 MSR, New York, USA Abstract 
 Module 5 Deep Convnets for Local Recognition Joost van de Weijer 4 April 2016 Previously, end-to-end.. Dog Slide credit: Jose M 2 Previously, end-to-end.. Dog Learned Representation Slide credit: Jose 
 9th Russian Summer School in Information Retrieval (RuSSIR) August 24-28, 2015 St Petersburg, Russia Visual object recognition and localization Part 1: Introduction to visual recognition Ivan Laptev ivan.laptev@inria.fr 
 Lecture 6: Classification & Localization boris. ginzburg@intel.com 1 Agenda ILSVRC 2014 Overfeat: integrated classification, localization, and detection Classification with Localization Detection. 2 ILSVRC-2014 
 Face Authentication Using Efficient Deep Convolutional Neural Network Hayder M. Albehadili, Naz Islam Dept. of Electrical and Computer Eng. University of Missouri-Columbia, MO 65203 ABSTRACT: Recently 
 Scalable Object Detection using Deep Neural Networks Dumitru Erhan, Christian Szegedy, Alexander Toshev, and Dragomir Anguelov Google, Inc. 1600 Amphitheatre Parkway, Mountain View (CA), 94043, USA {dumitru, 
 Vision Image Recognition and Understanding Almost all modern image understanding systems use ConvNets. Google, Facebook, Microsoft, IBM, Baidu, Yahoo/Flickr, Adobe, Yandex, Wechat, NEC, NVIDIA, MobilEye, 
 Pedestrian Detection using R-CNN CS676A: Computer Vision Project Report Advisor: Prof. Vinay P. Namboodiri Deepak Kumar Mohit Singh Solanki (12228) (12419) Group-17 April 15, 2016 Abstract Pedestrian detection 
 Modeling Transformations with Neural Networks Presented by Dinghuang Ji (Deep Learning Journal Club 04/12/2016) Outline Modeling transformations Transforming autoencoder Spatial transformer networks Deep 
 Understanding How Image Quality Affects Deep Neural Networks Samuel Dodge and Lina Karam Arizona State University sfdodge@asu.edu, karam@asu.edu arxiv:14.4v2 [cs.cv] 21 Apr 16 Abstract Image quality is 
 Pedestrian Detection with RCNN Matthew Chen Department of Computer Science Stanford University mcc17@stanford.edu Abstract In this paper we evaluate the effectiveness of using a Region-based Convolutional 
 The PASCAL Visual Object Classes Challenge 27 (VOC27) Part 2 Detection Task Mark Everingham Luc Van Gool Chris Williams John Winn Andrew Zisserman Detection Challenge Predict the bounding boxes of all 
 ECE 5984: Introduction to Machine Learning Topics: Neural Networks Backprop Readings: Murphy 16.5 Dhruv Batra Virginia Tech Administrativia HW3 Due: in 2 weeks You will implement primal & dual SVMs Kaggle 
 Lecture 5: Understanding Convnets and Knowledge Transfer Deep Learning @ UvA UVA DEEP LEARNING COURSE EFSTRATIOS GAVVES UNDERSTANDING CONVNETS AND KNOWLEDGE TRANSFER - 1 Previous Lecture o What are the 
 Fooling Neural Networks Linguang Zhang Feb-4-2015 Preparation Task: image classification. Datasets: MNIST, ImageNet. training and testing data. Preparation Logistic regression: Good for 0/1 classification. 
 Deep Domain Confusion: Maximizing for Domain Invariance Eric Tzeng, Judy Hoffman, Ning Zhang UC Berkeley, EECS & ICSI {etzeng,jhoffman,nzhang}@eecs.berkeley.edu Kate Saenko UMass Lowell, CS saenko@cs.uml.edu 
 Enhancing Human Action Recognition with Region Proposals Fahimeh Rezazadegan, Sareh Shirazi, Niko Sünderhauf, Michael Milford, Ben Upcroft Australian Centre for Robotic Vision(ACRV), School of Electrical 
 Using Convolutional Neural Networks to Classify Dog Breeds Hsu, David Stanford University fcdh@stanford.edu Abstract Dog breed categorization is a very specific application of convolutional neural networks. 
 BAG-OF-WORDS MODEL The slides are from several sources through James Hays (Brown); Silvio Savarese (U. of Michigan); Bill Freeman and Antonio Torralba (MIT), including their own slides. Visual Perceptual 
 Introduction to Machine Learning (67577) Lecture 10 Shai Shalev-Shwartz School of CS and Engineering, The Hebrew University of Jerusalem Neural Networks Shai Shalev-Shwartz (Hebrew U) IML Lecture 10 Neural 
 Introduction to Machine Learning CMU-10701 Deep Learning Barnabás Póczos & Aarti Singh Credits Many of the pictures, results, and other materials are taken from: Ruslan Salakhutdinov Joshua Bengio Geoffrey 
 Image Processing Technology and Applications: Object Recognition and Detection in Natural Images Katherine Bouman MIT November 26, 2012 Goal To be able to automatically understand the content of an image 
 Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks SHAOQING REN, KAIMING HE, ROSS GIRSHICK, JIAN SUN Göksu Erdoğan Object Detection Detection as Regression? DOG, (x, y, w, h) 
 CAP 6412 Advanced Computer Vision http://www.cs.ucf.edu/~bgong/cap6412.html Boqing Gong Jan 26, 2016 Today Administrivia A bigger picture and some common questions Object detection proposals, by Samer 
 massachusetts institute of technology computer science and artificial intelligence laboratory Component Based Recognition of Objects in an Office Environment Christian Morgenstern and Bernd Heisele AI 
 Brain Tumor Segmentation with Deep Neural Networks Axel Davy 1, Mohammad Havaei 2, David Warde-Farley 3, Antoine Biard 4, Lam Tran 5, Pierre-Marc Jodoin 2, Aaron Courville 3, Hugo Larochelle 2, Chris Pal 
 PANDA: Pose Aligned Networks for Deep Aribute Modeling Ning Zhang 1,2 Manohar Paluri 1 Marc Aurelio Ranzato 1 Trevor Darrell 2 Lubomir Bourdev 1 1 Facebook AI Research 2 EECS, UC Berkeley Why is aribute 
 1 Return of the Devil in the Details: Delving Deep into Convolutional Nets Ken Chatfield, Karen Simonyan, Andrea Vedaldi, and Andrew Zisserman Visual Geometry Group, Department of Engineering Science, 
 Multi Scale Recognition with DAG-CNNs by S. Yang & D. Ramanan March 22, 2016 Niladri Basu Bal (@ gmail.com) Motivation for this paper? 1. Contemporary approaches: Input for classifier is extracted feature 
 Object Detection and Counting with Low Quality Videos Hao Jiang Mechanical Engineering Stanford University Shiquan Wang Mechanical Engineering Stanford University jianghao@stanford.edu shiquan@stanford.edu 
 This paper was submitted as a final project report for CS6424/ECE6424 Probabilistic Graphical Models and Structured Prediction in the spring semester of 2016. The work presented here is done by students 
 Training R- CNNs of various velocities Slow, fast, and faster Ross Girshick Facebook AI Research (FAIR) Tools for Efficient Object Detection, ICCV 2015 Tutorial Section overview Kaiming just covered inference 
 Image Classification with Pyramid Representation and Rotated Data Augmentation on Torch 7 Keven (Kedao) Wang Stanford University kvw@stanford.edu Abstract This project classifies images in Tiny ImageNet 
 Deep Fisher Networks and Class Saliency Maps for Object Classification and Localisation Karén Simonyan, Andrea Vedaldi, Andrew Zisserman Visual Geometry Group, University of Oxford Outline Classification 
 Fast R-CNN Author: Ross Girshick Speaker: Charlie Liu Date: Oct, 13 th Girshick, R. (2015). Fast R-CNN. arxiv preprint arxiv:1504.08083. ECS 289G 001 Paper Presentation, Prof. Lee Result 1 67% Accuracy 
 Improving neural networks by preventing co-adaptation of feature detectors Geoffrey E. Hinton, Nitish Srivastava, Alex Krizhevsky, Ilya Sutskever Ruslan R. Salakhutdinov Department of Computer Science 
 Food Image Recognition by Using Convolutional Neural Networks (CNNs) 1 Yuzhen Lu Department of Biosystems and Agricultural Engineering, Michigan State University, East Lansing, MI 48824, USA; email address: 
 Learning with Recursive Perceptual Representations Oriol Vinyals UC Berkeley Berkeley, CA Yangqing Jia UC Berkeley Berkeley, CA Li Deng Microsoft Research Redmond, WA Trevor Darrell UC Berkeley Berkeley, 
 Lecture 4: Regression ctd and multiple classes C19 Machine Learning Hilary 2015 A. Zisserman Regression Lasso L1 regularization SVM regression and epsilon-insensitive loss More loss functions Multi-class 
 IMAGE AESTHETIC EVALUATION USING PARALLELED DEEP CONVOLUTION NEURAL NETWORK Guo Lihua, Li Fudi School of Electronic and Information Engineering, South China University of Technology, Guangzhou 510640, 
 Simultaneous Detection and Segmentation Bharath Hariharan 1, Pablo Arbeláez 1,2, Ross Girshick 1, and Jitendra Malik 1 {bharath2,arbelaez,rbg,malik}@eecs.berkeley.edu 1 University of California, Berkeley 
 Neural Machine Translation by Jointly Learning to Align and Translate Neural Traduction Automatique par Conjointement Apprentissage Pour Aligner et Traduire Dzmitry Bahdanau KyungHyun Cho Yoshua Bengio 
 From Unified Subspace Analysis to Joint Deep Learning for Face Recognition Xiaogang Wang Department of Electronic Engineering, The Chinese University i of Hong Kong, CUHK CUDA Research Center CUHK CUDA 
 Object Detectors Emerge in Deep Scene CNNs Bolei Zhou, Aditya Khosla, Agata Lapedriza, Aude Oliva, Antonio Torralba Massachusetts Institute of Technology CNN for Object Recognition Large-scale image classification 
 Recognizing Cats and Dogs with Shape and Appearance based Models Group Member: Chu Wang, Landu Jiang Abstract Recognizing cats and dogs from images is a challenging competition raised by Kaggle platform 
 Recognition: A machine learning approach Slides adapted from Fei-Fei Li, Rob Fergus, Antonio Torralba, Kristen Grauman, and Derek Hoiem The machine learning framework Apply a prediction function to a feature 
 DeepID3: Face Recognition with Very Deep Neural Networks Yi Sun 1 Ding Liang 2 Xiaogang Wang 3,4 Xiaoou Tang 1,4 1 Department of Information Engineering, The Chinese University of Hong Kong 2 SenseTime 
 René Donner Visual Computing Data Analysis Consulting rene@radiology-explorer.com René Donner 2 Overview The (amazing) things can do How does it work? How can you start with DL? 3 Roughly Deep learning 
 Translation-aware Fully Convolutional Instance Segmentation Jifeng Dai*, Haozhi Qi*, Yi Li** Microsoft Research Asia Visual Computing Group (*Equal contribution. This work was done when Haozhi Qi and Yi 
 Scalable Object Detection using Deep Neural Networks Dumitru Erhan Christian Szegedy Alexander Toshev Dragomir Anguelov Google {dumitru, szegedy, toshev, dragomir}@google.com arxiv:1312.2249v1 [cs.cv] 
 Lecture 13: Segmentation and Attention Lecture 13-1 Administrative Assignment 3 due tonight! We are reading your milestones Lecture 13-2 Last time: Software Packages Caffe Lasagne Torch Theano TensorFlow 
 CSC2535: 2013 Advanced Machine Learning Taking Inverse Graphics Seriously Geoffrey Hinton Department of Computer Science University of Toronto The representation used by the neural nets that work best 
 CCF ADL 2015 Nanchang Oct 11, 2015 Learning to Process Natural Language in Big Data Environment Hang Li Noah s Ark Lab Huawei Technologies Part 1: Deep Learning - Present and Future Talk Outline Overview 
 , pp.197-206 http://dx.doi.org/10.14257/ijbsbt.2016.8.2.18 Performance Analysis of Pixel based Face Recognition Methods Unsoo Jang 1 and Eui Chul Lee 2,* 1 Department of Computer Science, Graduate School, 
 HIGHLANDER, RODRIGUEZ: EFFICIENT TRAINING OF CNNS USING FFT AND OAA 1 arxiv:1601.06815v1 [cs.ne] 25 Jan 2016 Very Efficient Training of Convolutional Neural Networks using Fast Fourier Transform and Overlap-and-Add 
 A Deep Learning Based Fast Image Saliency Detection Algorithm Hengyue Pan York University 4700 Keele Street, Toronto, Ontario, CA panhy@cse.yorku.ca Hui Jiang York University 4700 Keele Street, Toronto, 
 Deep Learning Introduction Disclaimer and credits The following lecture has been taken from the following sources: Andrew Ng, Learning feature hierarchies and deep learning, ECCV 10 Yan LeCun, Marc Aurelio 
 M2CAI Surgical Tool Detection Challenge Report Ashwin Raju, Sheng Wang, and Junzhou Huang Department of Computer Science and Engineering, University of Texas at Arlington, Arlington TX 76019, USA Abstract. 
 Bag-of-features for category classification Cordelia Schmid Category recognition Image classification: assigning a class label to the image Car: present Cow: present Bike: not present Horse: not present 
 Steven C.H. Hoi School of Information Systems Singapore Management University Email: chhoi@smu.edu.sg Introduction http://stevenhoi.org/ Finance Recommender Systems Cyber Security Machine Learning Visual 
 Multiple Object Recognition with Focusing and Blurring Holly Chiang Stanford University hchiang1@stanford.edu Yifan Ge Stanford University gyifan@stanford.edu Connie Wu Stanford University wuconnie@stanford.edu 
 Dense Object Detection in Indoor Scenes Using Depth Information Jinchao Ye Stanford University Stanford CA 94305 jcye@stanford.edu Second Author Institution2 First line of institution2 address http: ://www.author.org/_second 
 Do Convnets Learn Correspondence? Jonathan Long Ning Zhang Trevor Darrell University of California Berkeley {jonlong, nzhang, trevor}@cs.berkeley.edu Abstract Convolutional neural nets (convnets) trained 
 Learning Deconvolution Network for Semantic Segmentation Hyeonwoo Noh Seunghoon Hong Bohyung Han Department of Computer Science and Engineering, POSTECH, Korea {hyeonwoonoh,maga33,bhhan}@postech.ac.kr 
 Journal of Machine Learning Research 17 (2016) 1-31 Submitted 5/15; Revised 2/16; Published 8/16 Large Scale Visual Recognition through Adaptation using Joint Representation and Multiple Instance Learning 
 Observer Localization using ConvNets (Stanford CS231n class project, Winter 2016) Alon Kipnis Department of Electrical Engineering Stanford University 350 Serra Mall, Stanford, CA kipnisal@stanford.edu 
 Transforming Auto-encoders G. E. Hinton, A. Krizhevsky & S. D. Wang Department of Computer Science, University of Toronto {geoffrey.hinton, akrizhevsky, sidawang88}@gmail.com Abstract. The artificial neural 
 A Dynamic Convolutional Layer for Short Range Weather Prediction Benjamin Klein, Lior Wolf and Yehuda Afek The Blavatnik School of Computer Science Tel Aviv University beni.klein@gmail.com, wolf@cs.tau.ac.il, 
 Semantic Recognition: Object Detection and Scene Segmentation Xuming He xuming.he@nicta.com.au Computer Vision Research Group NICTA Robotic Vision Summer School 2015 Acknowledgement: Slides from Fei-Fei 
 1 Introduction People Detection with Algorithm By Bing Han, Dingyi Li and Jia Ji People detection is an interesting computer vision topic. Locating people in images and videos have many potential applications, 
 TEXT DETECTION WITH CONVOLUTIONAL NEURAL NETWORKS Manolis Delakis and Christophe Garcia Orange Labs, 4, rue du Clos Courtel, 35512 Rennes, France Manolis.Delakis@orange-ftgroup.com, Christophe.Garcia@orange-ftgroup.com 
 Predict Foreground Object in Still Image Stanford Univeresity CS229 Fall 2013 Amer Hammudi ahammudi@stanford.edu Darren Koh dtkoh@stanford.edu Abstract In this paper, we are interested in the detection 
 arxiv:1404.1869v1 [cs.cv] 7 Apr 2014 DenseNet: Implementing Efficient ConvNet Descriptor Pyramids Technical Report Forrest Iandola, Matt Moskewicz, Sergey Karayev, Ross Girshick, Trevor Darrell and Kurt 
 Learning to Segment Object Candidates arxiv:156.624v2 [cs.cv] 1 Sep 215 Pedro O. Pinheiro Ronan Collobert Piotr Dollár pedro@opinheiro.com locronan@fb.com pdollar@fb.com Facebook AI Research Abstract Recent 
