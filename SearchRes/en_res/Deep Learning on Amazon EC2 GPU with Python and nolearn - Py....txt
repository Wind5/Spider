By Adrian Rosebrock on October 13, 2014 in Deep Learning, Tutorials 
Twitter0Facebook0Google+16LinkedIn84Last week I wrote a post detailing my experience with CUDAMat, Deep Belief Networks, and Python using my MacBook Pro.
The post is fairly long and full of screenshots to document my experience.
But the gist of it is this: Even after installing the NVIDIA Cuda SDK and configuring CUDAMat, my CPU was training my Deep Belief Network (implemented by nolearn) faster than my GPU. As you can imagine, I was left scratching my head.
However, since the post went live last week Ive gotten a ton of valuable feedback.
Ive been told that my network isnt big enough for the GPU speedup to be fully realized. Ive also been told that I should be using Theano rather than nolearn as their GPU support is more advanced. Its even been suggested that I should explore some compile time options of CUDAMat. And finally, I was told that I shouldnt be using my MacBook Pros GPU.
All of this was great feedback and its helped me a ton  but I wasnt satisfied.
After reading Markus Beissingers fantastic post on installing Theano on an Amazon EC2 GPU instance, I decided to give it a try myself.
But instead of using Theano, I wanted to use nolearn  mainly to see if I could replicate the problems I was having on my MacBook Pro on the Amazon cloud. And if I could replicate my results then I could conclude that the issues lies with the nolearn library rather than the GPU of my MacBook Pro.
So anyway, just like last post, this post is full of screenshots as I document my way through setting up an Amazon EC2 GPU instance to train a Deep Belief Network using Python and nolearn.
Deep Learning on the Amazon EC2 GPU using Python and nolearn
If you dont already know, Amazon offers an EC2 instance that provides access to the GPU for computation purposes.
The name of this instance is g2.2xlarge and costs roughly $0.65 cents per hour. However, as Markus points out, by using Spot Instances you can get this cost down to as low as roughly $0.07 per hour (provided that you can handle interrupts in your computation, of course).
Inspired by Markus posts I decided to fire up a g2.2xlarge playground of my own and have some fun.
If youre following along with this post, Ill assume that you already have an Amazon AWS account and can setup an EC2 instance:
The first thing youll need to do is select an Operating System for your instance. I went ahead and selected Ubuntu 14.04 LTS (64-bit) (ami-3d50120d).
From there, youll need to select which instance you need. Amazon provides many different tiers of instances, each geared towards the type of computation you are looking to perform. You have General Purpose instances which are great for web servers, high-memory servers which are good for manipulating lots of data, and high-CPU availability for faster throughout.
Be sure to select GPU instances to filter only the available GPU instances that Amazon provides.
Here I am going to select the g2.2xlarge instance. It is important to note that this instance is not free and if you launch it you will be charged.
The next step to getting your g2.2xlarge instance up and running is to configure your Security Groups to prevent outside access:
Hit the Launch button and wait for your instance to start up.
Youll be prompted to download your Key Pair so that you can SSH into your server. Download the Key Pair and store it in a safe location:
Once it has, youll be able to SSH into it. Your SSH command should look something like this:
If all goes well, you should now be logged into your g2.2xlarge instance. Here is an example of what my instance looks like:
So far this has been an extremely pain-free process. And luckily, it continues to be pain-free throughout the rest of the tutorial.
In order to prepare your system to utilize the GPU, youll need to install some packages and libraries. Below I am simply re-producing the steps by Markus, as well as adding in a few of my own:
I know. It looks like a lot of steps. But it honestly wasnt bad and it didnt take me more than 10 minutes.
So now lets train a Deep Belief Network. Open up a new file, name it 
# import the necessary packagesfrom sklearn.cross_validation import train_test_splitfrom sklearn.metrics import classification_reportfrom sklearn import datasetsfrom nolearn.dbn import DBNimport numpy as np # grab the MNIST dataset (if this is the first time you are running# this script, this make take a minute -- the 55mb MNIST digit dataset# will be downloaded)print "[X] downloading data..."dataset = datasets.fetch_mldata("MNIST Original") # scale the data to the range [0, 1] and then construct the training# and testing splits(trainX, testX, trainY, testY) = train_test_split(dataset.data / 255.0, dataset.target.astype("int0"), test_size = 0.33) # train the Deep Belief Network with 784 input units (the flattened,#  28x28 grayscale image), 800 hidden units in the 1st hidden layer,# 800 hidden nodes in the 2nd hidden layer, and 10 output units (one# for each possible output classification, which are the digits 1-10)dbn = DBN([trainX.shape[1], 800, 800, 10],learn_rates = 0.3,learn_rate_decays = 0.9,epochs = 10,verbose = 1)dbn.fit(trainX, trainY) # compute the predictions for the test data and show a classification# reportpreds = dbn.predict(testX)print classification_report(testY, preds)
Take note of Line 12. This line is downloading and caching the MNIST dataset for handwritten digit recognition to your EC2 instance. Subsequent calls to this function will be substantially faster (since you wont have to download the data again). I mention this because if you are monitoring your training time and you havent already cached the MNIST dataset, you will have unreliable results.
My previous post on Deep Belief Networks utilized a very tiny DBN  an input layer of 784 inputs, a hidden layer of 300 nodes, and an output layer of 10 nodes, one for each of the possible digits 1-9.
It has been brought to my attention that the speedup in GPU training vs. CPU training is not fully realized until much larger networks are trained.
So instead of training a tiny network, Im going to train a substantially larger one (but still small in comparison to the state-of-the-art networks we see today).
This time Ill use an input layer of 784 inputs, a hidden layer of 800 nodes, a second hidden layer of 800 nodes, and finally an output layer of 10 nodes. Ill allow my network to train for 10 epochs.
When I trained my Deep Belief Network on my CPU I got the following results:
(deeplearning)ubuntu@ip-xxx:~/deep-belief-network-gpu$ time python dbn.pygnumpy: failed to import cudamat. Using npmat instead. No GPU will be used.[X] downloading data...[DBN] fitting X.shape=(46900, 784)[DBN] layers [784, 800, 800, 10][DBN] Fine-tune...100%Epoch 1:  loss 0.554249416375  err  0.139429644809  (0:00:25)100%Epoch 2:  loss 0.285018297291  err  0.071956113388  (0:00:25)100%Epoch 3:  loss 0.216535961656  err  0.0562884221311  (0:00:25)100%Epoch 4:  loss 0.1733764816  err  0.0465975068306  (0:00:25)100%Epoch 5:  loss 0.145248167361  err  0.0397242144809  (0:00:25)100%Epoch 6:  loss 0.118301114211  err  0.0330430327869  (0:00:24)100%Epoch 7:  loss 0.0983925512707  err  0.0277279713115  (0:00:24)100%Epoch 8:  loss 0.0761229886239  err  0.0223702185792  (0:00:24)100%Epoch 9:  loss 0.0661320222977  err  0.0195099043716  (0:00:24)100%Epoch 10:  loss 0.0540665843727  err  0.0165215163934  (0:00:24)             precision    recall  f1-score   support           0       0.99      0.98      0.99      2274          1       0.98      0.99      0.99      2587          2       0.97      0.96      0.97      2408          3       0.96      0.95      0.96      2337          4       0.95      0.97      0.96      2220          5       0.97      0.95      0.96      2132          6       0.96      0.99      0.97      2204          7       0.97      0.98      0.98      2382          8       0.96      0.96      0.96      2271          9       0.96      0.95      0.96      2285 avg / total       0.97      0.97      0.97     23100  real4m48.487suser9m51.390ssys28m24.093s
Almost 5 minutes to train and evaluate on the CPU  thats a good starting point.
Now, to train the Deep Belief Network, I moved my compiled 
(deeplearning)ubuntu@ip-xxx:~/deep-belief-network-gpu$ time python dbn.py[X] downloading data...[DBN] fitting X.shape=(46900, 784)[DBN] layers [784, 800, 800, 10]gnumpy: failed to use gpu_lock. Using board #0 without knowing whether it is in use or not.[DBN] Fine-tune...100%Epoch 1:  loss 0.37499609756  err  0.102971311475  (0:00:08)100%Epoch 2:  loss 0.223706473163  err  0.0572703210383  (0:00:08)100%Epoch 3:  loss 0.176561286027  err  0.0470671106557  (0:00:08)100%Epoch 4:  loss 0.131326457655  err  0.036031420765  (0:00:08)100%Epoch 5:  loss 0.095808146489  err  0.0266606898907  (0:00:08)100%Epoch 6:  loss 0.075324679088  err  0.0217511953552  (0:00:08)100%Epoch 7:  loss 0.0538377553038  err  0.0162653688525  (0:00:08)100%Epoch 8:  loss 0.0431808142149  err  0.0129567964481  (0:00:08)100%Epoch 9:  loss 0.0353603169236  err  0.010843579235  (0:00:08)100%Epoch 10:  loss 0.0275717724744  err  0.00823941256831  (0:00:08)             precision    recall  f1-score   support           0       0.99      0.98      0.98      2223          1       0.99      0.99      0.99      2639          2       0.96      0.98      0.97      2285          3       0.97      0.97      0.97      2354          4       0.99      0.97      0.98      2234          5       0.98      0.97      0.97      2085          6       0.98      0.99      0.99      2248          7       0.99      0.97      0.98      2467          8       0.97      0.97      0.97      2217          9       0.96      0.97      0.96      2348 avg / total       0.98      0.98      0.98     23100  real2m20.350suser2m5.786ssys0m15.155s
Training on the g2.2xlarge GPU I was able to cut training and evaluation time from 4 minutes, 48 seconds to 2 minutes, 20 seconds.
Thats a HUGE improvement. And certainly better than the results that I was getting on my MacBook Pro.
Furthermore, the difference between the GPU and CPU training times will become even more dramatic as the size of the network increases.
Inspired by Markus Beissingers post on installing an Amazon EC2 g2.2xlarge instance for Deep Learning using Theano, I decided I would do the same for the nolearn Python package.
Furthermore, this post serves as a redemption of sorts after I tried to train a Deep Belief Network on my MacBook Pros GPU and obtained poor results.
Your GPU matters. A Lot. The GPUs included in most notebooks are optimized for power efficiency and not necessarily computational efficiency.
More importantly: The size of your network matters. If your network isnt large enough, you wont notice a significant improvement in training time between your CPU and GPU.
There is an overhead cost transferring data to the GPU. If the amount of data being transferred is too small, then the CPU will perform more efficiently (since youll be wasting all your time transferring rather than computing).
Amazons g2.2xlarge instance is a lot of fun to play around with. It does cost money (trade an afternoon of fun for less than a cup of coffee, its a no-brainer), but if you dont want to spend the money buying a new system dedicated to Deep Learning, its well worth the cost.
If you would like to download the code and images used in this post, please enter your email address in the form below. Not only will you get a .zip of the code, I’ll also send you a FREE 11-page Resource Guide on Computer Vision and Image Search Engines, including exclusive techniques that I don’t post on this blog! Sound good? If so, enter your email address and I’ll send you the code immediately!
Enter your email address below to get my free 11-page Image Search Engine Resource Guide PDF. Uncover exclusive techniques that I don't publish on this blog and start building image search engines of your own!
It was a good experience. And it only takes setting the environment up once. From there you are good to go to run any experiments. So I think the time-tradeoff for setting it all up was worth it.
It might be interesting to see if you can have multiple EC2 instances that are parallelizing the training of DBN networks to start to get very large neural networks. Im not sure if Theano gives this out of the box or its something that has to be built manually.
Its interesting to note that while GPUs given the fastest training time, many large enterprise companies and government organizations interested in data mining or machine learning are utilizing large clusters of systems. These systems are obviously CPU based. Therefore, I think in the next 2-5 years youll see more and more Deep Learning implementations developed with Hadoop in mind. These large organizations, after spending so much money creating their clusters, will not be jumping at the chance to buy even more hardware. And more than likely, these clusters already run Hadoop. Im definitely looking forward to seeing how it works out.
In this case, training a state-of-the-art CNN (in terms of time) is relative to the size of the dataset you are working with. You can get really good accuracy on CIFAR-10 in about 40 seconds on an EC2 GPU. But if you wanted to do something like ImageNet, be prepared to let your model train for weeks. At $0.65 per hour, you can do the math and interpolate from there. You could also use spot instances and potentially decrease your cost to $0.10-0.2 per hour.
I made it as far as Compile CUDAMat. After changing into the cudamat directory and running make I get the following error:
No doubt Ive done something wrong somewhere. Anyone have any ideas?
i had the same problem with make.. what should i do? i tried to run sudo python setup.py install . and had this issue ((error: command nvcc failed with exit status 1
Hi, I enjoyed your article, but I have one comment. Your comparison does not seem to be a controlled experiment. For example, your CPU was on your MAC, but the GPU was on AWS. In fact, I find that AWS runs about 10 times faster when I do a comparison in terms of the number of CPU cores on my Windows Notebook computer vs. AWS Linux. So, the speed-up seems to be due to something in AWs, which admittedly could be the operating system (my comparison Is not a controlled experiment either). But it is interesting in that AWS rates their extra large GPU instance that you tested as equal to an extra large CPU instance in terms of the ECU units. And the cost is based upon that ECU benchmark, so it is similar. It would be interesting to see a controlled experiment. Thanks.
Great feedback Dan, thanks! The next time I boot up my AWS instance Ill re-run the experiment on the AWS server. The last time I did, there was only a marginal difference between my Mac CPU and the AWS CPU. But its my fault for not reporting on that.
I have been thinking of using GPU services on EC2 so this is really useful info for me.
i also read your earlier article on Deep Learning with python using nolearn which seemed a lot more straightforward than other options such as Theano. In that article it was also noted that dbn has been deprecated from nolearn and that you had plans to write an update about its replacement at some point  do you still have plans to do that?
It depends on what type of network you are going to train and what your budget is. There is also a benefit of being hands on on the hardware and not having to foot the power bill (GPUs consume a lot of energy). In general, you just need to do the math. GPU instances on Amazon can go for $0.60-0.70 cents per her hour. Multiply that per machine. And then estimate how long it would take to train a model over a set of experiments on the cluster.
I normally recommend starting with AWS to get your feet wet and understand the environment (and costs associated with). From there, you can break it down and determine if owning the hardware is more cost effective. By using AWS, you can at least get a baseline.
Itrying to run Faster RCNN (https://github.com/rbgirshick/py-faster-rcnn) on AWS. Ive installed Caffe and have tried following your tutorial on installing OpenCV on Ubuntu 14.04 (with python 2.7) and here, but it doesnt seem to work—Ive read that python bindings with GPUs/CUDA doesnt work correctly. Is there a way youve found to install OpenCV on AWS w/ CUDA, w/ Caffe/etc. using python bindings, or would I just have to use the C++ versions of opencv? 
I havent personally tried to use Faster RCNN, so I cant comment directly to the question. I normally leave the when compiling OpenCV (especially when the NVIDIA drivers) since Ive seen that cause a lot of problems. But yes, I have installed OpenCV, CUDA, cuDNN, and Caffe (with pycaffe bindings) open an AWS instance. I mainly use the Python bindings to classify images and pass them through the network. Ive never used the bindings for anything else other than classification.
I am trying to install caffe on an AWS GPU instance. I had one doubt, while you created the GPU instance you selected 15 GB of RAM but in one of the images of yours after you do ssh to the system it shows that 9.8% of 7.74 GB usage. Actually i am trying to work on a Problem in caffe that needs 10-12 Gb of RAM so i had this doubt. 
BTW it was great blog, had a lot of takeaways from it
Your tutorials are fantastic and I learnt a lot from them!!
Actually I followed your tutorials and now, I am trying to connect Raspberry Pi 2 to the EC2 GPU instance so that the Pi can forward the image processing computations to the EC2 GPU and the GPU sends the results back to Pi. I have read on blogs that this can be done by socket-based implementation but no concrete idea is available anywhere. Can you please give a direction or any useful suggestion on how to do that?
Could you tell me what was the speed that you noticed in Amazon AWS with your tiny DBN ?
Was it similar to the speed that you noticed in your Mac with GPU drivers enabled ?
As I mentioned in this blog post, you wont get any substantial spread increases from your GPU unless your network and dataset is large enough. For toy datasets like MNIST you can see the time get cut in half. For deep networks on large datasets you can improve speed by multiple orders of magnitude.
My Experience with CUDAMat, Deep Belief Networks, and Python - PyImageSearch - 
[] I have found my redemption! To find out how I ditched my MacBook Pro and moved to the Amazon EC2 GPU, just click here. []
bat-country: an extendible, lightweight Python package for deep dreaming with Caffe and Convolutional Neural Networks - PyImageSearch - 
[] Caffe up and running. Instead of installing Caffe on your own system, I recommend spinning up an Amazon EC2 g2.2xlarge instance (so you have access to the GPU) and working from []
Deep dream: Visualizing every layer of GoogLeNet - PyImageSearch - 
[] the visualization process will kick off. I generated my results on an Amazon EC2 g2.2xlarge instance with GPU support enabled so the script finished up within 30 []
How to install CUDA Toolkit and cuDNN for deep learning - PyImageSearch - 
[] I mentioned in an earlier blog post, Amazon offers an EC2 instance that provides access to the GPU for computation []
Click the button below to get my free 11-page Image Search Engine Resource Guide PDF. Uncover exclusive techniques that I don't publish on this blog and start building image search engines of your own.
You're interested in deep learning and computer vision, but you don't know how to get started. Let me help. My new book will teach you all you need to know about deep learning.
Are you interested in detecting faces in images & video? But tired of Googling for tutorials that never work? Then let me help! I guarantee that my new book will turn you into a face detection ninja by the end of this weekend.
The PyImageSearch Gurus course is now enrolling! Inside the course you'll learn how to perform:
Click the button below to learn more about the course, take a tour, and get 10 (FREE) sample lessons.
I'm an entrepreneur and Ph.D who has launched two successful image search engines, ID My Pill and Chic Engine. I'm here to share my tips, tricks, and hacks I've learned along the way.
Want to learn computer vision & OpenCV? I can teach you in a single weekend. I know. It sounds crazy, but it’s no joke. My new book is your guaranteed, quick-start guide to becoming an OpenCV Ninja. So why not give it a try? Click here to become a computer vision ninja.
Never miss a post! Subscribe to the PyImageSearch RSS Feed and keep up to date with my image search engine tutorials, tips, and tricks
