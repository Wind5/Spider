 Update:  Dr. Le has posted tutorials on this topic: Part 1 and Part 2.
Dr. Quoc Le from the Google Brain project team (yes, the one that made headlines for creating a cat recognizer) presented a series of lectures at the Machine Learning Summer School (MLSS 14) in Pittsburgh this week. This is my favorite lecture series from the event till now and I was glad to be able to attend them.
The good news is that the organizers have made available the entire set of video lectures in 4K for you to watch. But since Dr. Le did most of them on the board and did not provide any accompanying slides, I decided to put the contents of the lectures along with the videos here.
In this post I posted Dr. Les lecture videos and added content links with short descriptions to help you navigate them better.
Dr. Le begins his lecture starting from the fundamentals on Neural Networks if youd like to brush up your knowledge about them. Otherwise feel free to quickly skim through the initial sections but I promise there are interesting things later on. You may use the links below to quickly skip the video to the relevant parts. Let me know in the comments if they dont work.
Mathematical Expression for NN: Decision function, Minimizing Loss and Gradient Descent (Correction in derivative), Making decision
If you have already covered NN in the past then the first lecture may have been a bit dry for you but the real fun begins in this lecture when Dr. Le starts talking about his experiences of using deep learning in practice.
Clarifications from Lecture 1: Data partitioning is not needed, Derivative of the loss function, Tip  Write unit tests!
Ideas for practical implementations: Breaking Symmetry, Monitoring Progress on training, Underfitting and overfitting, How to select NN architecture and hyper-parameters, Other tips for improvements
Deep Neural Networks: Review of why NN, Shallow vs. Deep, Rectified Linear Units, Definitions for deep NN, History of NN
Deep NN Architectures: Autoencoder, Intuition for using autoencoders for initialization (Continued in the next lecture)
In this lecture, Dr. Le finishes his description on NN architectures. He also talks a bit about how they are being used at Google for applications in image and speech recognition, and language modelling.
Convolutional NN (Convnets): Local receptive field, Why are Convnets useful?, Image classification, General Pipeline
Click to share on Facebook (Opens in new window)Click to share on Twitter (Opens in new window)Click to email this to a friend (Opens in new window)Click to print (Opens in new window)MoreClick to share on Reddit (Opens in new window)Click to share on LinkedIn (Opens in new window)Click to share on Pocket (Opens in new window)Click to share on Google+ (Opens in new window)Click to share on Tumblr (Opens in new window)Click to share on Pinterest (Opens in new window)
