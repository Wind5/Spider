A simple and powerful regularization technique for neural networks and deep learning models is dropout.
In this post you will discover the dropout regularization technique and how to apply it to your models in Python with Keras.
Update Oct/2016: Updated examples for Keras 1.1.0, TensorFlow 0.10.0 and scikit-learn v0.18.
Update Mar/2017: Updated example for Keras 2.0.2, TensorFlow 1.0.1 and Theano 0.9.0.
Dropout Regularization in Deep Learning Models With KerasPhoto by Trekking Rinjani, some rights reserved.
Dropout is a regularization technique for neural network models proposed by Srivastava, et al. in their 2014 paper Dropout: A Simple Way to Prevent Neural Networks from Overfitting (download the PDF).
Dropout is a technique where randomly selected neurons are ignored during training. They are dropped-out randomly. This means that their contribution to the activation of downstream neurons is temporally removed on the forward pass and any weight updates are not applied to the neuron on the backward pass.
As a neural network learns, neuron weights settle into their context within the network. Weights of neurons are tuned for specific features providing some specialization. Neighboring neurons become to rely on this specialization, which if taken too far can result in a fragile model too specialized to the training data. This reliant on context for a neuron during training is referred to complex co-adaptations.
You can imagine that if neurons are randomly dropped out of the network during training, that other neurons will have to step in and handle the representation required to make predictions for the missing neurons. This is believed to result in multiple independent internal representations being learned by the network.
The effect is that the network becomes less sensitive to the specific weights of neurons. This in turn results in a network that is capable of better generalization and is less likely to overfit the training data.
Take my free 2-week email course and discover MLPs, CNNs and LSTMs (with sample code).
Click to sign-up now and also get a free PDF Ebook version of the course.
Dropout is easily implemented by randomly selecting nodes to be dropped-out with a given probability (e.g. 20%) each weight update cycle. This is how Dropout is implemented in Keras. Dropout is only used during the training of a model and is not used when evaluating the skill of the model.
Next we will explore a few different ways of using Dropout in Keras.
The examples will use the Sonar dataset. This is a binary classification problem where the objective is to correctly identify rocks and mock-mines from sonar chirp returns. It is a good test dataset for neural networks because all of the input values are numerical and have the same scale.
The dataset can be downloaded from the UCI Machine Learning repository. You can place the sonar dataset in your current working directory with the file name sonar.csv.
We will evaluate the developed models using scikit-learn with 10-fold cross validation, in order to better tease out differences in the results.
There are 60 input values and a single output value and the input values are standardized before being used in the network. The baseline neural network model has two hidden layers, the first with 60 units and the second with 30. Stochastic gradient descent is used to train the model with a relatively low learning rate and momentum.
# Baseline Model on the Sonar Datasetimport numpyfrom pandas import read_csvfrom keras.models import Sequentialfrom keras.layers import Densefrom keras.layers import Dropoutfrom keras.wrappers.scikit_learn import KerasClassifierfrom keras.constraints import maxnormfrom keras.optimizers import SGDfrom sklearn.model_selection import cross_val_scorefrom sklearn.preprocessing import LabelEncoderfrom sklearn.model_selection import StratifiedKFoldfrom sklearn.preprocessing import StandardScalerfrom sklearn.pipeline import Pipeline# fix random seed for reproducibilityseed = 7numpy.random.seed(seed)# load datasetdataframe = read_csv("sonar.csv", header=None)dataset = dataframe.values# split into input (X) and output (Y) variablesX = dataset[:,0:60].astype(float)Y = dataset[:,60]# encode class values as integersencoder = LabelEncoder()encoder.fit(Y)encoded_Y = encoder.transform(Y) # baselinedef create_baseline():# create modelmodel = Sequential()model.add(Dense(60, input_dim=60, kernel_initializer='normal', activation='relu'))model.add(Dense(30, kernel_initializer='normal', activation='relu'))model.add(Dense(1, kernel_initializer='normal', activation='sigmoid'))# Compile modelsgd = SGD(lr=0.01, momentum=0.8, decay=0.0, nesterov=False)model.compile(loss='binary_crossentropy', optimizer=sgd, metrics=['accuracy'])return model numpy.random.seed(seed)estimators = []estimators.append(('standardize', StandardScaler()))estimators.append(('mlp', KerasClassifier(build_fn=create_baseline, epochs=300, batch_size=16, verbose=0)))pipeline = Pipeline(estimators)kfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=seed)results = cross_val_score(pipeline, X, encoded_Y, cv=kfold)print("Baseline: %.2f%% (%.2f%%)" % (results.mean()*100, results.std()*100))
Dropout can be applied to input neurons called the visible layer.
In the example below we add a new Dropout layer between the input (or visible layer) and the first hidden layer. The dropout rate is set to 20%, meaning one in 5 inputs will be randomly excluded from each update cycle.
Additionally, as recommended in the original paper on Dropout, a constraint is imposed on the weights for each hidden layer, ensuring that the maximum norm of the weights does not exceed a value of 3. This is done by setting the kernel_constraint argument on the Dense class when constructing the layers.
The learning rate was lifted by one order of magnitude and the momentum was increase to 0.9. These increases in the learning rate were also recommended in the original Dropout paper.
Continuing on from the baseline example above, the code below exercises the same network with input dropout.
# dropout in the input layer with weight constraintdef create_model():# create modelmodel = Sequential()model.add(Dropout(0.2, input_shape=(60,)))model.add(Dense(60, kernel_initializer='normal', activation='relu', kernel_constraint=maxnorm(3)))model.add(Dense(30, kernel_initializer='normal', activation='relu', kernel_constraint=maxnorm(3)))model.add(Dense(1, kernel_initializer='normal', activation='sigmoid'))# Compile modelsgd = SGD(lr=0.1, momentum=0.9, decay=0.0, nesterov=False)model.compile(loss='binary_crossentropy', optimizer=sgd, metrics=['accuracy'])return model numpy.random.seed(seed)estimators = []estimators.append(('standardize', StandardScaler()))estimators.append(('mlp', KerasClassifier(build_fn=create_model, epochs=300, batch_size=16, verbose=0)))pipeline = Pipeline(estimators)kfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=seed)results = cross_val_score(pipeline, X, encoded_Y, cv=kfold)print("Visible: %.2f%% (%.2f%%)" % (results.mean()*100, results.std()*100))
Dropout can be applied to hidden neurons in the body of your network model.
In the example below Dropout is applied between the two hidden layers and between the last hidden layer and the output layer. Again a dropout rate of 20% is used as is a weight constraint on those layers.
# dropout in hidden layers with weight constraintdef create_model():# create modelmodel = Sequential()model.add(Dense(60, input_dim=60, kernel_initializer='normal', activation='relu', kernel_constraint=maxnorm(3)))model.add(Dropout(0.2))model.add(Dense(30, kernel_initializer='normal', activation='relu', kernel_constraint=maxnorm(3)))model.add(Dropout(0.2))model.add(Dense(1, kernel_initializer='normal', activation='sigmoid'))# Compile modelsgd = SGD(lr=0.1, momentum=0.9, decay=0.0, nesterov=False)model.compile(loss='binary_crossentropy', optimizer=sgd, metrics=['accuracy'])return model numpy.random.seed(seed)estimators = []estimators.append(('standardize', StandardScaler()))estimators.append(('mlp', KerasClassifier(build_fn=create_model, epochs=300, batch_size=16, verbose=0)))pipeline = Pipeline(estimators)kfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=seed)results = cross_val_score(pipeline, X, encoded_Y, cv=kfold)print("Hidden: %.2f%% (%.2f%%)" % (results.mean()*100, results.std()*100))
We can see that for this problem and for the chosen network configuration that using dropout in the hidden layers did not lift performance. In fact, performance was worse than the baseline.
It is possible that additional training epochs are required or that further tuning is required to the learning rate.
The original paper on Dropout provides experimental results on a suite of standard machine learning problems. As a result they provide a number of useful heuristics to consider when using dropout in practice.
Generally, use a small dropout value of 20%-50% of neurons with 20% providing a good starting point. A probability too low has minimal effect and a value too high results in under-learning by the network.
Use a larger network. You are likely to get better performance when dropout is used on a larger network, giving the model more of an opportunity to learn independent representations.
Use dropout on incoming (visible) as well as hidden units. Application of dropout at each layer of the network has shown good results.
Use a large learning rate with decay and a large momentum. Increase your learning rate by a factor of 10 to 100 and use a high momentum value of 0.9 or 0.99.
Constrain the size of network weights. A large learning rate can result in very large network weights. Imposing a constraint on the size of network weights such as max-norm regularization with a size of 4 or 5 has been shown to improve results.
Below are some resources that you can use to learn more about dropout in neural network and deep learning models.
Dropout: A Simple Way to Prevent Neural Networks from Overfitting (original paper).
How does the dropout method work in deep learning? on Quora.
In this post, you discovered the dropout regularization technique for deep learning models. You learned:
How you can use dropout on your own deep learning models.
Tips for getting the best results from dropout on your own models.
Do you have any questions about dropout or about this post? Ask your questions in the comments and I will do my best to answer.
Dr. Jason Brownlee is a husband, proud father, academic researcher, author, professional developer and a machine learning practitioner. He is dedicated to helping developers get started and get good at applied machine learning.
Question: the goal of the dropouts is to reduce the risk of overfitting right?
I am wondering whether the accuracy is the best way to measure this; here you are already doing a cross validation, which is by itself a way to reduce overfitting. As you are performing cross-validation *and* dropout, isnt this somewhat overkill? Maybe the drop in accuracy is actually a drop in amount of information available?
Yes dropout is a technique to reduce the overfitting of the network to the training data.
k-fold cross-validation is a robust technique to estimate the skill of a model. It is well suited to determine whether a specific network configuration has over or under fit the problem.
You could also look at diagnostic plots of loss over epoch on the training and validation datasets to determine how overlearning has been affected by different dropout configurations.
First of all, thanks to you for making machine learning fun to learn.
Can we use drop-out even in case we have selected the optimizer as adam and not sgd?
In the examples, sgd is being used and also in the tips section, it has been mentioned 
Use a large learning rate with decay and a large momentum. As far as I see adam does not have the momentum. So what should be the parameter to adam if we use dropouts.
In addition to sampling, how does one deal with rare events when building a Deep Learning Model?
For shallow Machine Learning, I can add a utility or a cost function, but here Im to see if a more elegant approach has been developed.
You may also oversample the rare events, perhaps with data augmentation (randomness) to make them look different.
Theres some discussion of handling imbalanced datasets here that might inspire:
Why Dropout will work in these cases? It can lower down the computations but how will it impact the increase i have been experimenting with the dropout on ANN and now on RNN (LSTM), I am using the dropout in LSTM only in the input and output not between the recurrent layers.. but the accuracy remains the same for both validation and training data set
I see two options, to fix the random seed and make the code reproducible or to run the model n times (30?) and take the mean performance from all runs.
For more on the stochastic nature of machine learning algorithms like neural nets see this post:
I am a beginner in Machine Learning and trying to learn neural networks from your blog.
In this post, I understand all the concepts of dropout, but the use of:
is making code difficult for me to understand. Would you suggest any way so that I can code without using these?
I have a question, I saw that when using dropout for the hidden layers, you applied it for all of them.
My question is, if dropout is applied to the hidden layers then, should it be applied to all of them? Or better yet how do we choose where to apply the dropout?
# this is the augmentation configuration we will use for training
# this is the augmentation configuration we will use for testing:
I am struck here. I am using binary cross entropy. I want to see probabilities as the actual ones between 0 and 1. But I am getting only maximum probabilities, ie, 0 or 1. For both test and training samples.
Thanks for the great insights on how dropout works. I have 1 question: what is the difference between adding a dropout layer (like your examples here) and setting the dropout parameter of a layer, for example:
I am using an LSTM to predict values of a sin wave. Without, the NN is able to catch quite correctly the frequency and the amplitude of the signal.
In the first case, the results are also great. But in the second, the amplitude is reduce by 1/4 of its original value..
