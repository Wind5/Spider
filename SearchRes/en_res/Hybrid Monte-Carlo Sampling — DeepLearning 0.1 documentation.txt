This is an advanced tutorial, which shows how one can implemented Hybrid
Monte-Carlo (HMC) sampling using Theano. We assume the reader is already
to sample negative phase particles (see Eq.(4) of the Restricted Boltzmann Machines (RBM) tutorial).
When training RBMs with CD or PCD, this is typically done with block Gibbs
 are used as the transition operators of the Markov chain.
to sample from (i.e. requiring expensive matrix inversions, as in the case of
the mean-covariance RBM). Also, even if Gibbs sampling can be done
efficiently, it nevertheless operates via a random walk which might not be
In this context, and when sampling from continuous variables, Hybrid Monte
Carlo (HMC) can prove to be a powerful tool [Duane87]. It avoids random walk
In HMC, model samples are obtained by simulating a physical system, where
as the sum of potential energy (same energy function defined by
As shown in [Neal93], the above transformation preserves volume and is
reversible. The above dynamics can thus be used as transition operators of a
is not ergodic however, since simulating the dynamics maintains a fixed
HMC thus alternates hamiltonian dynamic steps, with Gibbs sampling of the
In practice, we cannot simulate Hamiltonian dynamics exactly because of the
problem of time discretization. There are several ways one can do this. To
maintain invariance of the Markov chain however, care must be taken to
We thus perform a half-step update of the velocity at time
Also, rounding errors due to the use of floating point numbers means that the
In this tutorial, we obtain a new HMC sample as follows:
In Theano, update dictionaries and shared variables provide a natural way to
implement a sampling algorithm. The current state of the sampler can be
represented as a Theano shared variable, with HMC updates being implemented by
: a symbolic Python function which, given an initial position and velocity, will perform leapfrog updates and return the symbolic variables for the proposed state .
: a Python function which, given the symbolic outputs of ,
generates the list of updates for a single iteration of HMC.
To perform leapfrog steps, we first need to define a function over
one last half-step update for . In loop form, this gives:
 Inside loop of Scan. Performs one step of leapfrog update, using
 in leapfrog update equations, represents pos(t), position at time t
 Symbolic theano matrices for new position pos(t + stepsize), and
 Inside loop of Scan. Performs one step of leapfrog update, using
 in leapfrog update equations, represents pos(t), position at time t
 Symbolic theano matrices for new position pos(t + stepsize), and
 # perform leapfrog updates: the scan op is used to repeatedly compute
 # vel(t + (m-1/2)*stepsize) and pos(t + m*stepsize) for m in [2,n_steps].
 # NOTE: Scan always returns an updates dictionary, in case the
 # updates must then be used when compiling the Theano function, to
 # avoid drawing the same random numbers each time the function is
 # The last velocity returned by scan is vel(t +
 # (n_steps - 1 / 2) * stepsize) We therefore perform one more half-step
 final_vel = final_vel - 0.5 * stepsize * TT.grad(energy.sum(), final_pos)
The function implements the remaining steps (steps 1 and 3) of an
 This function performs one-step of Hybrid Monte-Carlo sampling. We start by
 sampling a random velocity from a univariate Gaussian distribution, perform
 Symbolic random number generator used to draw random velocity and
 Shared variable containing the stepsize to use for `n_steps` of HMC
 Number of HMC steps to perform before proposing a new position.
We start by sampling random velocities, using the provided shared RandomStream
object. Velocities are sampled independently for each dimension and for each
Since we now have an initial position and velocity, we can now call the
We then accept/reject the proposed state based on the Metropolis algorithm.
 # accept/reject the proposed move based on the joint distribution
 Symbolic theano tensor which contains the energy associated with the
 Symbolic theano tensor which contains the energy associated with the
 Theano shared random stream object used to generate the random number
 Returns the Hamiltonian (sum of potential and kinetic energy) for the given
 Vector whose i-th entry is the Hamiltonian at position pos[i] and
 Vector whose i-th entry is the kinetic entry associated with vel[i].
symbolic boolean variable indicating whether or not the new state 
The purpose of is to generate the list of updates to
receives as parameters, a series of shared variables to update (, and
 the `simulate` function. It takes care of updating: the position
 (if the move is accepted), the stepsize (to track a given target
 acceptance rate) and the average acceptance rate (computed as a
 Boolean-type variable representing whether or not the proposed HMC move
 The stepsize is modified in order to track this target acceptance rate.
 Amount by which to increment stepsize when acceptance rate is too high.
 Amount by which to decrement stepsize when acceptance rate is too low.
 Average acceptance rate is computed as an exponential moving average.
 A dictionary of updates to be used by the `HMC_Sampler.simulate`
 # broadcast `accept` scalar to tensor with the same dimensions as
 # if accept is True, update to `final_pos` else stay put
to update the state of the sampler with either (1) the new state 
if is True, or (2) the old state if is False. This
 expects as its first argument, a boolean mask with the same
scalar-valued, we must first use dimshuffle to transform it to a tensor with
implemented in the accompanying code to [Ranzato10]. We start by tracking the
average acceptance rate of the HMC move proposals (across many simulations),
If the average acceptance rate is larger than the , we
increase the by a factor of in order to increase the
mixing rate of our chain. If the average acceptance rate is too low however,
 is decreased by a factor of , yielding a more
conservative mixing rate. The clip op allows us to maintain the 
 # if acceptance rate is too low, our sampler is too "noisy" and we reduce
 # the stepsize. If it is too high, our sampler is too conservative, we can
 # get away with a larger stepsize (resulting in better mixing).
 Convenience wrapper for performing Hybrid Monte Carlo (HMC). It creates the
 symbolic graph for performing an HMC simulation (using `hmc_move` and
 `hmc_updates`). The graph is then compiled into the `simulate` function, a
 theano function which runs the simulation and updates the required shared
 Users should interface with the sampler thorugh the `draw` function which
 advances the markov chain and returns the current sample by calling
 The hyper-parameters are the same as those used by MarcAurelios
 # used in geometric avg. 1.0 would be not moving at all
 The sum of this energy vector must be differentiable (with
 # define the dictionary of updates, to apply on every `simulate` call
 Returns a new position obtained after `n_steps` of HMC simulation.
We test our implementation of HMC by sampling from a multi-variate Gaussian
distribution. We start by generating a random mean vector and covariance
matrix , which allows us to define the energy function of the
We then initialize the state of the sampler by allocating a shared
variable. It is passed to the constructor of along with our
Following a burn-in period, we then generate a large number of samples and
compare the empirical mean and covariance matrix to their true values.
 garbage = [sampler.draw() for r in range(burnin)] # burn-in Draw
 # `n_samples`: result is a 3D tensor of dim [n_samples, batchsize,
The above code can be run using the command: nosetests -s code/hmc/test_hmc.py. The output is as follows:
As can be seen above, the samples generated by our HMC sampler yield an
empirical mean and covariance matrix, which are very close to the true
underlying parameters. The adaptive algorithm also seemed to work well as the
[Neal93](1, 2) Neal, R. M. (1993) Probabilistic Inference Using Markov Chain Monte Carlo Methods, Technical Report CRG-TR-93-1, Dept. of Computer Science, University of Toronto, 144 pages
