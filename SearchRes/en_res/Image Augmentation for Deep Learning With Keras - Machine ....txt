Data preparation is required when working with neural network and deep learning models. Increasingly data augmentation is also required on more complex object recognition tasks.
In this post you will discover how to use data preparation and data augmentation with your image datasets when developing and evaluating deep learning models in Python with Keras.
About the image augmentation API provide by Keras and how to use it with your models.
Update: The examples in this post were updated for the latest Keras API. The datagen.next() function was removed.
Update Oct/2016: Updated examples for Keras 1.1.0, TensorFlow 0.10.0 and scikit-learn v0.18.
Update Mar/2017: Updated example for Keras 2.0.2, TensorFlow 1.0.1 and Theano 0.9.0.
Like the rest of Keras, the image augmentation API is simple and powerful.
Keras provides the ImageDataGenerator class that defines the configuration for image data preparation and augmentation. This includes capabilities such as:
Rather than performing the operations on your entire image dataset in memory, the API is designed to be iterated by the deep learning model fitting process, creating augmented image data for you just-in-time. This reduces your memory overhead, but adds some additional time cost during model training.
After you have created and configured your ImageDataGenerator, you must fit it on your data. This will calculate any statistics required to actually perform the transforms to your image data. You can do this by calling the fit() function on the data generator and pass it your training dataset.
The data generator itself is in fact an iterator, returning batches of image samples when requested. We can configure the batch size and prepare the data generator and get batches of images by calling the flow() function.
Finally we can make use of the data generator. Instead of calling the fit() function on our model, we must call the fit_generator() function and pass in the data generator and the desired length of an epoch as well as the total number of epochs on which to train.
Take my free 2-week email course and discover MLPs, CNNs and LSTMs (with sample code).
Click to sign-up now and also get a free PDF Ebook version of the course.
Now that you know how the image augmentation API in Keras works, lets look at some examples.
We will use the MNIST handwritten digit recognition task in these examples. To begin with, lets take a look at the first 9 images in the training dataset.
# Plot imagesfrom keras.datasets import mnistfrom matplotlib import pyplot# load data(X_train, y_train), (X_test, y_test) = mnist.load_data()# create a grid of 3x3 imagesfor i in range(0, 9):pyplot.subplot(330 + 1 + i)pyplot.imshow(X_train[i], cmap=pyplot.get_cmap('gray'))# show the plotpyplot.show()
Running this example provides the following image that we can use as a point of comparison with the image preparation and augmentation in the examples below.
It is also possible to standardize pixel values across the entire dataset. This is called feature standardization and mirrors the type of standardization often performed for each column in a tabular dataset.
You can perform feature standardization by setting the featurewise_center and featurewise_std_normalization arguments on the ImageDataGenerator class. These are in fact set to True by default and creating an instance of ImageDataGenerator with no arguments will have the same effect.
# Standardize images across the dataset, mean=0, stdev=1from keras.datasets import mnistfrom keras.preprocessing.image import ImageDataGeneratorfrom matplotlib import pyplotfrom keras import backend as KK.set_image_dim_ordering('th')# load data(X_train, y_train), (X_test, y_test) = mnist.load_data()# reshape to be [samples][pixels][width][height]X_train = X_train.reshape(X_train.shape[0], 1, 28, 28)X_test = X_test.reshape(X_test.shape[0], 1, 28, 28)# convert from int to floatX_train = X_train.astype('float32')X_test = X_test.astype('float32')# define data preparationdatagen = ImageDataGenerator(featurewise_center=True, featurewise_std_normalization=True)# fit parameters from datadatagen.fit(X_train)# configure batch size and retrieve one batch of imagesfor X_batch, y_batch in datagen.flow(X_train, y_train, batch_size=9):# create a grid of 3x3 imagesfor i in range(0, 9):pyplot.subplot(330 + 1 + i)pyplot.imshow(X_batch[i].reshape(28, 28), cmap=pyplot.get_cmap('gray'))# show the plotpyplot.show()break
A whitening transform of an image is a linear algebra operation that reduces the redundancy in the matrix of pixel images.
Less redundancy in the image is intended to better highlight the structures and features in the image to the learning algorithm.
Typically, image whitening is performed using the Principal Component Analysis (PCA) technique. More recently, an alternative called ZCA (learn more in Appendix A of this tech report) shows better results and results in transformed images that keeps all of the original dimensions and unlike PCA, resulting transformed images still look like their originals.
You can perform a ZCA whitening transform by setting the zca_whitening argument to True.
# ZCA whiteningfrom keras.datasets import mnistfrom keras.preprocessing.image import ImageDataGeneratorfrom matplotlib import pyplotfrom keras import backend as KK.set_image_dim_ordering('th')# load data(X_train, y_train), (X_test, y_test) = mnist.load_data()# reshape to be [samples][pixels][width][height]X_train = X_train.reshape(X_train.shape[0], 1, 28, 28)X_test = X_test.reshape(X_test.shape[0], 1, 28, 28)# convert from int to floatX_train = X_train.astype('float32')X_test = X_test.astype('float32')# define data preparationdatagen = ImageDataGenerator(zca_whitening=True)# fit parameters from datadatagen.fit(X_train)# configure batch size and retrieve one batch of imagesfor X_batch, y_batch in datagen.flow(X_train, y_train, batch_size=9):# create a grid of 3x3 imagesfor i in range(0, 9):pyplot.subplot(330 + 1 + i)pyplot.imshow(X_batch[i].reshape(28, 28), cmap=pyplot.get_cmap('gray'))# show the plotpyplot.show()break
Sometimes images in your sample data may have varying and different rotations in the scene.
You can train your model to better handle rotations of images by artificially and randomly rotating images from your dataset during training.
The example below creates random rotations of the MNIST digits up to 90 degrees by setting the rotation_range argument.
# Random Rotationsfrom keras.datasets import mnistfrom keras.preprocessing.image import ImageDataGeneratorfrom matplotlib import pyplotfrom keras import backend as KK.set_image_dim_ordering('th')# load data(X_train, y_train), (X_test, y_test) = mnist.load_data()# reshape to be [samples][pixels][width][height]X_train = X_train.reshape(X_train.shape[0], 1, 28, 28)X_test = X_test.reshape(X_test.shape[0], 1, 28, 28)# convert from int to floatX_train = X_train.astype('float32')X_test = X_test.astype('float32')# define data preparationdatagen = ImageDataGenerator(rotation_range=90)# fit parameters from datadatagen.fit(X_train)# configure batch size and retrieve one batch of imagesfor X_batch, y_batch in datagen.flow(X_train, y_train, batch_size=9):# create a grid of 3x3 imagesfor i in range(0, 9):pyplot.subplot(330 + 1 + i)pyplot.imshow(X_batch[i].reshape(28, 28), cmap=pyplot.get_cmap('gray'))# show the plotpyplot.show()break
Running the example, you can see that images have been rotated left and right up to a limit of 90 degrees. This is not helpful on this problem because the MNIST digits have a normalized orientation, but this transform might be of help when learning from photographs where the objects may have different orientations.
Objects in your images may not be centered in the frame. They may be off-center in a variety of different ways.
You can train your deep learning network to expect and currently handle off-center objects by artificially creating shifted versions of your training data. Keras supports separate horizontal and vertical random shifting of training data by the width_shift_range and height_shift_range arguments.
# Random Shiftsfrom keras.datasets import mnistfrom keras.preprocessing.image import ImageDataGeneratorfrom matplotlib import pyplotfrom keras import backend as KK.set_image_dim_ordering('th')# load data(X_train, y_train), (X_test, y_test) = mnist.load_data()# reshape to be [samples][pixels][width][height]X_train = X_train.reshape(X_train.shape[0], 1, 28, 28)X_test = X_test.reshape(X_test.shape[0], 1, 28, 28)# convert from int to floatX_train = X_train.astype('float32')X_test = X_test.astype('float32')# define data preparationshift = 0.2datagen = ImageDataGenerator(width_shift_range=shift, height_shift_range=shift)# fit parameters from datadatagen.fit(X_train)# configure batch size and retrieve one batch of imagesfor X_batch, y_batch in datagen.flow(X_train, y_train, batch_size=9):# create a grid of 3x3 imagesfor i in range(0, 9):pyplot.subplot(330 + 1 + i)pyplot.imshow(X_batch[i].reshape(28, 28), cmap=pyplot.get_cmap('gray'))# show the plotpyplot.show()break
Running this example creates shifted versions of the digits. Again, this is not required for MNIST as the handwritten digits are already centered, but you can see how this might be useful on more complex problem domains.
Another augmentation to your image data that can improve performance on large and complex problems is to create random flips of images in your training data.
Keras supports random flipping along both the vertical and horizontal axes using the vertical_flip and horizontal_flip arguments.
# Random Flipsfrom keras.datasets import mnistfrom keras.preprocessing.image import ImageDataGeneratorfrom matplotlib import pyplotfrom keras import backend as KK.set_image_dim_ordering('th')# load data(X_train, y_train), (X_test, y_test) = mnist.load_data()# reshape to be [samples][pixels][width][height]X_train = X_train.reshape(X_train.shape[0], 1, 28, 28)X_test = X_test.reshape(X_test.shape[0], 1, 28, 28)# convert from int to floatX_train = X_train.astype('float32')X_test = X_test.astype('float32')# define data preparationdatagen = ImageDataGenerator(horizontal_flip=True, vertical_flip=True)# fit parameters from datadatagen.fit(X_train)# configure batch size and retrieve one batch of imagesfor X_batch, y_batch in datagen.flow(X_train, y_train, batch_size=9):# create a grid of 3x3 imagesfor i in range(0, 9):pyplot.subplot(330 + 1 + i)pyplot.imshow(X_batch[i].reshape(28, 28), cmap=pyplot.get_cmap('gray'))# show the plotpyplot.show()break
Running this example you can see flipped digits. Flipping digits is not useful as they will always have the correct left and right orientation, but this may be useful for problems with photographs of objects in a scene that can have a varied orientation.
The data preparation and augmentation is performed just in time by Keras.
This is efficient in terms of memory, but you may require the exact images used during training. For example, perhaps you would like to use them with a different software package later or only generate them once and use them on multiple different deep learning models or configurations.
Keras allows you to save the images generated during training. The directory, filename prefix and image file type can be specified to the flow() function before training. Then, during training, the generated images will be written to file.
The example below demonstrates this and writes 9 images to a images subdirectory with the prefix aug and the file type of PNG.
# Save augmented images to filefrom keras.datasets import mnistfrom keras.preprocessing.image import ImageDataGeneratorfrom matplotlib import pyplotimport osfrom keras import backend as KK.set_image_dim_ordering('th')# load data(X_train, y_train), (X_test, y_test) = mnist.load_data()# reshape to be [samples][pixels][width][height]X_train = X_train.reshape(X_train.shape[0], 1, 28, 28)X_test = X_test.reshape(X_test.shape[0], 1, 28, 28)# convert from int to floatX_train = X_train.astype('float32')X_test = X_test.astype('float32')# define data preparationdatagen = ImageDataGenerator()# fit parameters from datadatagen.fit(X_train)# configure batch size and retrieve one batch of imagesos.makedirs('images')for X_batch, y_batch in datagen.flow(X_train, y_train, batch_size=9, save_to_dir='images', save_prefix='aug', save_format='png'):# create a grid of 3x3 imagesfor i in range(0, 9):pyplot.subplot(330 + 1 + i)pyplot.imshow(X_batch[i].reshape(28, 28), cmap=pyplot.get_cmap('gray'))# show the plotpyplot.show()break
Image data is unique in that you can review the data and transformed copies of the data and quickly get an idea of how the model may be perceive it by your model.
Below are some times for getting the most from image data preparation and augmentation for deep learning.
Review Dataset. Take some time to review your dataset in great detail. Look at the images. Take note of image preparation and augmentations that might benefit the training process of your model, such as the need to handle different shifts, rotations or flips of objects in the scene.
Review Augmentations. Review sample images after the augmentation has been performed. It is one thing to intellectually know what image transforms you are using, it is a very different thing to look at examples. Review images both with individual augmentations you are using as well as the full set of augmentations you plan to use. You may see ways to simplify or further enhance your model training process.
Evaluate a Suite of Transforms. Try more than one image data preparation and augmentation scheme. Often you can be surprised by results of a data preparation scheme you did not think would be beneficial.
You discovered a range of techniques that you can use easily in Python with Keras for deep learning models. You learned about:
The ImageDataGenerator API in Keras for generating transformed images just in time.
Do you have any questions about image data augmentation or this post? Ask your questions in the comments and I will do my best to answer.
Dr. Jason Brownlee is a husband, proud father, academic researcher, author, professional developer and a machine learning practitioner. He is dedicated to helping developers get started and get good at applied machine learning.
UPDATE: I have updated all examples in this post to use the new API. Let me know if you have any problems at all.
Now i have question that how to decide value for this parameter So that i can get good testing accuracy ..i have training dataset with 110 category with 32000 images ..
all worked fine except the last code to save images to file, I got the following exception
 for X_batch, y_batch in datagen.flow(X_train, y_train, batch_size=9, save_to_dir=images, save_prefix=aug, save_format=png):
Thanks a lot for your tutorial. It is helping me in many ways.
I had question on mask image or target Y for training image X
Can i also transform Y along with X. Helps in the case of training for segmentation
This is great stuff but I wonder if you could provide an example like this with an RGB image with three channels? I am getting some really buggy results personally with this ImageGenerator.
I wonder what channel_shift_range is about. The doc says shift range for each channels, but what does this actually mean? Is it adding a random value to each channel or doing something else?
Yes Indra, any transforms like standardization performed on the data prior to modeling will also need to be performed on new data when testing or making predictions.
In the case of standardization, we need to keep track of means and standard deviations.
The 33 creates a grid of 33 images. The number after that (1-9) indicates the position in that grid to place the next image (left to right, top to bottom ordering).
I have a question: Does this apply to image data with RGBXYZ for each pixel?
Each of my input image is of six channels including RGB and XYZ (world coordinate), which was acquired from an organized point cloud by PCL(Point Cloud Library). I am wondering whether there is a correct way to do data augmentation for my images.
I think ImageDataGenerator might be correct only for RGB images? Because when you shift/rotate/flip the RGB image, it means camera movement indeed, and the XYZ coordinates should be changed as well.
When I use zoom_range of 0.2 and inspect the output images, it seems to zoom h and v axes independently. However I want to have a small amount of zoom variation while preserving the aspect ratio of the images.
Also, when I specify a rotation_range, the rotated images have aliasing artefacts. Is there any way to specify rotations with antialiasing?
Aspect ratio of the image is important in a facial recognition setting. Antialiasing of rotated images Im not so sure about, but as they are small images (244 x 244) it doesnt make sense to degrade them further.
I can modify my own copy of the Keras code to maintain the aspect ratio of the zoom and should be able to substitute PILs rotation function, which does antialiasing, for the one used in Keras.
Keep up the good work, your writing has really helped me get up to speed with Keras quickly
The transformations in ImageGenerator are applied using [scipy.ndimage.interpolation.affine_transform](https://docs.scipy.org/doc/scipy-0.14.0/reference/generated/scipy.ndimage.interpolation.affine_transform.html), with order (the order of spline used for interpolation) set to zero.
Change this to one for linear interpolation or higher for higher orders.
I am trying to use ImageDataGenerator now. But if I want to apply feature standardization to unseen data in the future, I need to save the ImageDataGenerator to disk, right? Any suggestion to do it? Thanks a lot.
I have training data of the shape (2000,4,100,100) which means 2000 samples of the size 100100 with 4 channels and dtype= uint8, stored as .npy file. Can I use Image Augmentation technique on such data?
Since I used the fit_generator method instead of fit(), I need to use evaluate_generator in order to correctly evaluate the model or not? Is the same for predict_generator? Im a little confused.
I have a quick question about the image augmentation. I am attempting to greatly increase the size of my training data set using data augmentation in order to increase my segmentation accuracy. Does the image generator feed multiple augmentations of the same image to the model or does it just return a single augmented version instead of the original? There seems to be no way to modify the number of augmented images the Image Data Generator actually returns.
I made the exercice with your book which I find just great!!!
The problem is: it applies on randomly choosen images instead of doing it on the same ones from the Point of comparison sub-chapter. And always different samples.
I must say I dont understand how it comes the i applies on the pyplot.subplot and on the X-batch[].
Think of the augmented images as randomly modified versions of your training dataset. You have a new dataset with lots of variations of the data you provided. You do not need to tie them back to the original examples.
I think the problem of Alice is the same as mine, the data that are plotted after each modification are never the same, which is difficult to make a comparison because they change everytime.
-the first plot gives me : 5 6 3, 0 1 9, 2 3 1
 after the ZCA whitening i have : 2 3 8, 3 2 5, 0 1 7
Yes, by design, the augmentation will create different augmented versions of the images each time it is called.
This is what we want, so the model does a better job of generalizing.
What is the problem exactly, could you help me to understand please?
I have the same problem as Alice. I think that what she was saying was that the pictures that she plot after random modifications are never the same
It looks like the 9 pictures that are plotted are chosen randomly everytime.
It would be nice if you could answer me on this problem, 
C:\Usersacheu\AppData\Local\Programs\Python\Python35\libite-packages\keras\preprocessing\image.py:653: UserWarning: Expected input to be images (as Numpy array) following the data format convention channels_first (channels on axis 1), i.e. expected either 1, 3 or 4 channels on axis 1. However, it was passed an array with shape (60000, 1, 28, 28) (1 channels).
i think i have the latest version of the libraries. And I am using python 3.5.
Hi, thanks for your share. When I try to use zca-whitening and feature-wised centering on bigger data, I found its very very hard to get enough memory to do the fit() function. As the data-set has about 10000 pictures and 224*224 pixels, even generate a flow iterator will use full of my 16GB memory. When try to use fit() for zca-whitening,centering,normalization which the documents said have to use the fit() function, I never success. Will you give some advice for data preparation for bigger data? Thank you very much!
Thanks for this nice post. I have a quick question. I have large-dataset which I am loading to model using custom data-generator. I am using it in model.fit_generator(). Now I want to use data-augmentation. So my question is, how/where can I use keras ImageDataGenerator? Thank you very much.
