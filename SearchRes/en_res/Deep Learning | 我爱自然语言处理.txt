2011年秋季，Andrew Ng 推出了面向入门者的MOOC雏形课程机器学习: Machine Learning，随后在2012年4月，Andrew Ng 在Coursera上推出了改进版的Machine Learning(机器学习)公开课: Andrew Ng' Machine Learning: Master the Fundamentals，这也同时宣告了Coursera平台的诞生。当时我也是第一时间加入了这门课程，并为这门课程写了一些笔记：Coursera公开课笔记: 斯坦福大学机器学习 。同时也是受这股MOOC浪潮的驱使，建立了“课程图谱”，因此结识了不少公开课爱好者和MOOC大神。而在此之前，Andrew Ng 在斯坦福大学的授课视频“机器学习”也流传甚广，但是这门面向斯坦福大学学生的课程难道相对较高。直到2012年Coursera, Udacity等MOOC平台的建立，把课程视频，作业交互，编程练习有机结合在一起，才产生了更有生命力的MOOC课程。Andrew Ng 在为新课程深度学习写的宣传文章“deeplearning.ai: Announcing new Deep Learning courses on Coursera”里提到，这门机器学习课程自从开办以来，大约有180多万学生学习过，这是一个惊人的数字。
回到这个深度学习系列课：Deep Learning Specialization ，该课程正式开课是8月15号，但是在此之前几天已经开放了，加入后可以免费学习7天，之后开始按月费49美元收取，直到取消这个系列的订阅为止。正式加入的好处是，除了课程视频，还可以在Coursera平台上做题和提交编程作业，得到实时反馈，如果通过的话，还可以拿到相应的课程证书。我在上周六加入了这门以 deeplearning.ai 的名义推出的Deep Learning(深度学习)系列课，并且利用业余时间完成了第一门课“Neural Networks and Deep Learning（神经网络与深度学习）”的相关课程，包括视频观看和交互练习以及编程作业，体验很不错。自从Coursera迁移到新平台后，已经很久没有上过相关的公开课了，这次要不是Andrew Ng 离开百度后重现MOOC江湖，点燃了内心久违的MOOC情节，我大概也不会这么认真的去上公开课了。
If you want to break into AI, this Specialization will help you do so. Deep Learning is one of the most highly sought after skills in tech. We will help you become good at Deep Learning.
In five courses, you will learn the foundations of Deep Learning, understand how to build neural networks, and learn how to lead successful machine learning projects. You will learn about Convolutional networks, RNNs, LSTM, Adam, Dropout, BatchNorm, Xavier/He initialization, and more. You will work on case studies from healthcare, autonomous driving, sign language reading, music generation, and natural language processing. You will master not only the theory, but also see how it is applied in industry. You will practice all these ideas in Python and in TensorFlow, which we will teach.
You will also hear from many top leaders in Deep Learning, who will share with you their personal stories and give you career advice.
AI is transforming multiple industries. After finishing this specialization, you will likely find creative ways to apply it to your work.
We will help you master Deep Learning, understand how to apply it, and build a career in AI.
第二周课程同时也提供了编程作业所需要的基础部分视频课程：Python and Vectorization。这门课程的编程作业使用Python语言，并且提供线上 Jupyter Notebook 编程环境完成作业，无需线下编程验证提交，非常方便。这也和之前机器学习课程的编程作业有了很大区别，之前那门课程使用Octave语言（类似Matlab的GNU Octave)，并且是线下编程测试后提交给服务器验证。这次课程线上完成编程作业的感觉是非常棒的，这个稍后再说。另外就是强调数据处理时的 Vectorization(向量化/矢量化），并且重度使用 Numpy 工具包, 如果没有特别提示，请尽量避免使用 "for loop":
当然，这部分最赞的是编程作业的设计了，首先提供了一个热身可选的编程作业：Python Basics with numpy (optional)，然后是本部分的相关作业：Logistic Regression with a Neural Network mindset。每部分先有一个引导将这部分的目标讲清楚，然后点击“Open Notebook”开始作业，Notebook中很多相关代码老师已经精心设置好，对于学生来说，只需要在相应提示的部分写上几行关键代码（主要还是Vectorization)，运行后有相应的output，如果output和里面提示的期望输出一致的话，就可以点击保存继续下一题了，非常方便，完成作业后就可以提交了，这部分难度不大：
这是我学完Andrew Ng这个深度学习系列课程第一门课程“Neural Networks and Deep Learning（神经网络与深度学习）” 的体验，如果用几个字来总结这个深度学习系列课程，依然是：值、很值、非常值。如果你是完全的人工智能的门外汉或者入门者，那么建议你先修一下Andrew Ng的 Machine Learning(机器学习)公开课 ，用来过渡和理解相关概念，当然这个是可选项；如果你是一个业内的从业者或者深度学习工具的使用者，那么这门课程很适合给你扫清很多迷雾；当然，如果你对机器学习和深度学习了如指掌，完全可以对这门课程一笑了之。 
本条目发布于2017年08月17号。属于深度学习分类，被贴了 AI、AI课程、Andrew Ng、Andrew Ng 机器学习、Andrew Ng 深度学习、Andrew Ng 深度学习课程、Coursera、Coursera深度学习、Coursera深度学习课程、Deep Learning、Deep Learning 专题、Deep Learning 课程、deeplearning.ai、Machine Learning、MOOC、人工智能、人工智能课程、公开课、反向传播算法、吴恩达、吴恩达深度学习、吴恩达深度学习课程、机器学习、机器学习公开课、深度学习、深度学习公开课、深度学习课程、深度学习课程体验、深度学习课程值不值、深度学习课程小结、深度学习课程推荐、深度学习课程测评、深度学习课程评价、课程 标签。作者是52nlp。
斯坦福大学在三月份开设了一门“深度学习与自然语言处理”的课程：CS224d: Deep Learning for Natural Language Processing，授课老师是青年才俊 Richard Socher，他本人是德国人，大学期间涉足自然语言处理，在德国读研时又专攻计算机视觉，之后在斯坦福大学攻读博士学位，拜师NLP领域的巨牛 Chris Manning 和 Deep Learning 领域的巨牛 Andrew Ng，其博士论文是《Recursive Deep Learning for Natural Language Processing and Computer Vision》，也算是多年求学生涯的完美一击。毕业后以联合创始人及CTO的身份创办了MetaMind，作为AI领域的新星创业公司，MetaMind创办之初就拿了800万美元的风投，值得关注。
回到这们课程CS224d,其实可以翻译为“面向自然语言处理的深度学习（Deep Learning for Natural Language Processing）”，这门课程是面向斯坦福学生的校内课程，不过课程的相关材料都放到了网上，包括课程视频，课件，相关知识，预备知识，作业等等，相当齐备。课程大纲相当有章法和深度，从基础讲起，再讲到深度学习在NLP领域的具体应用，包括命名实体识别，机器翻译，句法分析器，情感分析等。Richard Socher此前在ACL 2012和NAACL 2013 做过一个Tutorial，Deep Learning for NLP (without Magic)，感兴趣的同学可以先参考一下: Deep Learning for NLP (without Magic) - ACL 2012 Tutorial - 相关视频及课件 。另外，由于这门课程的视频放在Youtube上，@爱可可-爱生活 老师维护了一个网盘链接：http://pan.baidu.com/s/1pJyrXaF ，同步更新相关资料，可以关注。
本条目发布于2015年01月27号。属于PRML、机器学习分类，被贴了 back propagation、BP网络、Deep Learning、forward propagation、neural networks、Pattern Recognition And Machine Learning、PRML、PRML读书会、卷积网络、机器学习、机器学习书籍、机器学习读书会、梯度下降、正则化、深度学习、神经网络、神经网络学习、神经网络定义、神经网络训练 标签。作者是prml。
['Writing II: Rhetorical Composing', 'Genetics and Society: A Course for Educators', 'General Game Playing', 'Genes and the Human Condition (From Behavior to Biotechnology)', 'A Brief History of Humankind', 'New Models of Business in Society', 'Analyse Num\xc3\xa9rique pour Ing\xc3\xa9nieurs', 'Evolution: A Course for Educators', 'Coding the Matrix: Linear Algebra through Computer Science Applications', 'The Dynamic Earth: A Course for Educators']
'BROWN CORPUS\n\nA Standard Corpus of Present-Day Edited American\nEnglish, for use with Digital Computers.\n\nby W. N. Francis and H. Kucera (1964)\nDepartment of Linguistics, Brown University\nProvidence, Rhode Island, USA\n\nRevised 1971, Revised and Amplified 1979\n\nhttp://www.hit.uib.no/icame/brown/bcm.html\n\nDistributed with the permission of the copyright holder,\nredistribution permitted.\n'
[('The', 'AT'), ('Fulton', 'NP-TL'), ('County', 'NN-TL'), ('Grand', 'JJ-TL'), ('Jury', 'NN-TL'), ('said', 'VBD'), ('Friday', 'NR'), ('an', 'AT'), ('investigation', 'NN'), ('of', 'IN')]
>>> texts_lower = [[word for word in document.lower().split()] for document in courses]
['writing', 'ii:', 'rhetorical', 'composing', 'rhetorical', 'composing', 'engages', 'you', 'in', 'a', 'series', 'of', 'interactive', 'reading,', 'research,', 'and', 'composing', 'activities', 'along', 'with', 'assignments', 'designed', 'to', 'help', 'you', 'become', 'more', 'effective', 'consumers', 'and', 'producers', 'of', 'alphabetic,', 'visual', 'and', 'multimodal', 'texts.', 'join', 'us', 'to', 'become', 'more', 'effective', 'writers...', 'and', 'better', 'citizens.', 'rhetorical', 'composing', 'is', 'a', 'course', 'where', 'writers', 'exchange', 'words,', 'ideas,', 'talents,', 'and', 'support.', 'you', 'will', 'be', 'introduced', 'to', 'a', ...
>>> texts_tokenized = [[word.lower() for word in word_tokenize(document.decode('utf-8'))] for document in courses]
['writing', 'ii', ':', 'rhetorical', 'composing', 'rhetorical', 'composing', 'engages', 'you', 'in', 'a', 'series', 'of', 'interactive', 'reading', ',', 'research', ',', 'and', 'composing', 'activities', 'along', 'with', 'assignments', 'designed', 'to', 'help', 'you', 'become', 'more', 'effective', 'consumers', 'and', 'producers', 'of', 'alphabetic', ',', 'visual', 'and', 'multimodal', 'texts.', 'join', 'us', 'to', 'become', 'more', 'effective', 'writers', '...', 'and', 'better', 'citizens.', 'rhetorical', 'composing', 'is', 'a', 'course', 'where', 'writers', 'exchange', 'words', ',', 'ideas', ',', 'talents', ',', 'and', 'support.', 'you', 'will', 'be', 'introduced', 'to', 'a', ...
['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', 'her', 'hers', 'herself', 'it', 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', 'should', 'now']
>>> texts_filtered_stopwords = [[word for word in document if not word in english_stopwords] for document in texts_tokenized]
['writing', 'ii', ':', 'rhetorical', 'composing', 'rhetorical', 'composing', 'engages', 'series', 'interactive', 'reading', ',', 'research', ',', 'composing', 'activities', 'along', 'assignments', 'designed', 'help', 'become', 'effective', 'consumers', 'producers', 'alphabetic', ',', 'visual', 'multimodal', 'texts.', 'join', 'us', 'become', 'effective', 'writers', '...', 'better', 'citizens.', 'rhetorical', 'composing', 'course', 'writers', 'exchange', 'words', ',', 'ideas', ',', 'talents', ',', 'support.', 'introduced', 'variety', 'rhetorical', 'concepts\xe2\x80\x94that', ',', 'ideas', 'techniques', 'inform', 'persuade', 'audiences\xe2\x80\x94that', 'help', 'become', 'effective', 'consumer', 'producer', 'written', ',', 'visual', ',', 'multimodal', 'texts.', 'class', 'includes', 'short', 'videos', ',', 'demonstrations', ',', 'activities.', 'envision', 'rhetorical', 'composing', 'learning', 'community', 'includes', 'enrolled', 'course', 'instructors.', 'bring', 'expertise', 'writing', ',', 'rhetoric', 'course', 'design', ',', 'designed', 'assignments', 'course', 'infrastructure', 'help', 'share', 'experiences', 'writers', ',', 'students', ',', 'professionals', 'us.', 'collaborations', 'facilitated', 'wex', ',', 'writers', 'exchange', ',', 'place', 'exchange', 'work', 'feedback']
>>> english_punctuations = [',', '.', ':', ';', '?', '(', ')', '[', ']', '&', '!', '*', '@', '#', '$', '%']
>>> texts_filtered = [[word for word in document if not word in english_punctuations] for document in texts_filtered_stopwords]
['writing', 'ii', 'rhetorical', 'composing', 'rhetorical', 'composing', 'engages', 'series', 'interactive', 'reading', 'research', 'composing', 'activities', 'along', 'assignments', 'designed', 'help', 'become', 'effective', 'consumers', 'producers', 'alphabetic', 'visual', 'multimodal', 'texts.', 'join', 'us', 'become', 'effective', 'writers', '...', 'better', 'citizens.', 'rhetorical', 'composing', 'course', 'writers', 'exchange', 'words', 'ideas', 'talents', 'support.', 'introduced', 'variety', 'rhetorical', 'concepts\xe2\x80\x94that', 'ideas', 'techniques', 'inform', 'persuade', 'audiences\xe2\x80\x94that', 'help', 'become', 'effective', 'consumer', 'producer', 'written', 'visual', 'multimodal', 'texts.', 'class', 'includes', 'short', 'videos', 'demonstrations', 'activities.', 'envision', 'rhetorical', 'composing', 'learning', 'community', 'includes', 'enrolled', 'course', 'instructors.', 'bring', 'expertise', 'writing', 'rhetoric', 'course', 'design', 'designed', 'assignments', 'course', 'infrastructure', 'help', 'share', 'experiences', 'writers', 'students', 'professionals', 'us.', 'collaborations', 'facilitated', 'wex', 'writers', 'exchange', 'place', 'exchange', 'work', 'feedback']
>>> texts_stemmed = [[st.stem(word) for word in docment] for docment in texts_filtered]
['writ', 'ii', 'rhet', 'compos', 'rhet', 'compos', 'eng', 'sery', 'interact', 'read', 'research', 'compos', 'act', 'along', 'assign', 'design', 'help', 'becom', 'effect', 'consum', 'produc', 'alphabet', 'vis', 'multimod', 'texts.', 'join', 'us', 'becom', 'effect', 'writ', '...', 'bet', 'citizens.', 'rhet', 'compos', 'cours', 'writ', 'exchang', 'word', 'idea', 'tal', 'support.', 'introduc', 'vary', 'rhet', 'concepts\xe2\x80\x94that', 'idea', 'techn', 'inform', 'persuad', 'audiences\xe2\x80\x94that', 'help', 'becom', 'effect', 'consum', 'produc', 'writ', 'vis', 'multimod', 'texts.', 'class', 'includ', 'short', 'video', 'demonst', 'activities.', 'envid', 'rhet', 'compos', 'learn', 'commun', 'includ', 'enrol', 'cours', 'instructors.', 'bring', 'expert', 'writ', 'rhet', 'cours', 'design', 'design', 'assign', 'cours', 'infrastruct', 'help', 'shar', 'expery', 'writ', 'stud', 'profess', 'us.', 'collab', 'facilit', 'wex', 'writ', 'exchang', 'plac', 'exchang', 'work', 'feedback']
>>> stems_once = set(stem for stem in set(all_stems) if all_stems.count(stem) == 1)
>>> texts = [[stem for stem in text if stem not in stems_once] for text in texts_stemmed]
2013-06-07 21:37:07,120 : INFO : adding document #0 to Dictionary(0 unique tokens)
2013-06-07 21:37:07,263 : INFO : built Dictionary(3341 unique tokens) from 379 documents (total 46417 corpus positions)
2013-06-07 21:58:30,504 : INFO : calculating IDF weights for 379 documents and 3341 features (29166 matrix non-zeros)
2013-06-07 22:04:55,443 : INFO : scanning corpus to determine the number of features
2013-06-07 22:04:55,510 : INFO : creating matrix for 379 documents and 10 features
[(0, 8.3270084238788673), (1, 0.91295652151975082), (2, -0.28296075112669405), (3, 0.0011599008827843801), (4, -4.1820134980024255), (5, -0.37889856481054851), (6, 2.0446999575052125), (7, 2.3297944485200031), (8, -0.32875594265388536), (9, -0.30389668455507612)]
[(210, 1.0), (174, 0.97812241), (238, 0.96428639), (203, 0.96283489), (63, 0.9605484), (189, 0.95390636), (141, 0.94975704), (184, 0.94269753), (111, 0.93654782), (236, 0.93601125)]
