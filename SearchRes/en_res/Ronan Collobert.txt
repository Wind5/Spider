 SENNA is a software distributed under a non-commercial license, which
 SENNA is fast because it uses a simple architecture, self-contained
 because it does not rely on the output of existing NLP system, and
 SENNA is written in ANSI C, with about 2500 lines of code. It requires
 about 150MB of RAM and should run on any IEEE floating point computer.
 D. Palaz, G. Synnaeve and R. Collobert. Jointly Learning to Locate and Classify Words using Convolutional Networks. In Interspeech, 2016.
In this paper, we propose a novel approach for weakly-supervised word
recognition. Most state of the art automatic speech recognition systems are
based on frame-level labels obtained through forced alignments or through a
proposed in vision, that can learn which part of the input is relevant for
classifying a given pattern. Our system is composed of a convolutional
sentence, it is trained using as supervision only some of the words (most
frequent) that are present in a given sentence, without knowing their order
nor quantity. We show that our proposed system is able to jointly classify
and localise words. We also evaluate the system on a keyword spotting task,
and show that it can yield similar performance to strong supervised HMM/GMM
 title = {Jointly Learning to Locate and Classify Words using Convolutional Networks},
 author = {D. Palaz and G. Synnaeve and R. Collobert},
 C. Sun, M. Paluri, R. Collobert, R. Nevatia and L. Bourdev. ProNet: Learning to Propose Object-specific Boxes for Cascaded Neural Networks. In Conference on Computer Vision and Pattern Recognition (CVPR), 2016.
This paper aims to classify and locate objects accurately and efficiently,
without using bounding box annotations. It is challenging as objects in the
wild could appear at arbitrary locations and in different scales. In this
networks to propose image regions that are likely to contain objects, and
applies more powerful but slower networks on the proposed regions. The
scales. We show that such networks can be trained effectively using
image-level annotations, and can be connected into cascades or trees for
state-of-the-art significantly on PASCAL VOC 2012 and MS COCO datasets for
 title = {ProNet: Learning to Propose Object-specific Boxes for Cascaded Neural Networks},
 author = {C. Sun and M. Paluri and R. Collobert and R. Nevatia and L. Bourdev},
 booktitle = {Conference on Computer Vision and Pattern Recognition (CVPR)},
 R. Collobert, L. Van der Maaten and A. Joulin. Torchnet: An Open-Source Platform for (Deep) Learning Research. In ICML Machine Learning Systems Workshop, 2016.
Torch 7 is a scientific computing platform that supports both CPU and GPU
computation, has a lightweight wrapper in a simple scripting language, and
one of the main frameworks for research in (deep) machine learning. Torch
and code re-use, which reduces the chance of bugs, and it makes it
computations. Torchnet is written in pure Lua, which makes it easy to
install on any architecture with a Torch installation. We envision Torchnet
to become a platform to which the community contributes via plugins.
 title = {Torchnet: An Open-Source Platform for (Deep) Learning Research},
 author = {R. Collobert and L. Van der Maaten and A. Joulin},
 J. Legrand and R. Collobert. Phrase Representations for Multiword Expressions. In Workshop on Multiword Expressions (MWE), 2016.
boundaries. This allows these tasks to be expressed as common word tagging
problems. In this paper, we propose to learn fixed-size representations for
arbitrarily sized chunks. We introduce a model that takes advantage of such
classifying phrases. We evaluate our approach on the task of multiword
 J. Legrand and R. Collobert. Deep Neural Networks for Syntactic Parsing of Morphologically Rich Languages. In Association for Computational Linguistics (ACL), 2016.
Morphologically rich languages (MRL) are languages in which much of the
structural information is contained at the wordlevel, leading to high level
tackled using generative models. These models assume input features to be
embeddings, in the context of MRL. We propose to learn morphological
 title = {Deep Neural Networks for Syntactic Parsing of Morphologically Rich Languages},
 J. Legrand, M. Auli and R. Collobert. Neural Network-based Word Alignment through Score Aggregation. In Workshop on Machine Translation (WMT), 2016.
We present a simple neural network for word alignment that builds source
operation that summarizes the alignment scores for a given target word. A
decreasing scores for target words that are not present. Compared to the
popular Fast Align model, our approach improves alignment accuracy by 7 AER
on English-Czech, by 6 AER on Romanian-English and by 1.7 AER on
 author = {J. Legrand and M. Auli and R. Collobert},
 P. H. O. Pinheiro, T. Y. Lin, R. Collobert and P. Dollar. Learning to Refine Object Segments. In European Conference on Computer Vision (ECCV), 2016.
In this work we propose to augment feedforward nets for object segmentation
all layers of the net. Unlike them, our approach does not attempt to output
independent predictions at each layer. Instead, we first output a coarse
‘mask encoding’ in a feedforward pass, then refine this mask encoding in a
 author = {P. H. O. Pinheiro and T. Y. Lin and R. Collobert and P. Dollar},
 P. H. O. Pinheiro, R. Collobert and P. Dollar. Learning to Segment Object Candidates. In Advances in Neural Information Processing Systems (NIPS), 2015.
In this paper, we propose a new way to generate object proposals,
network. Our model is trained jointly with two objectives: given an image
patch, the first part of the system outputs a class-agnostic segmentation
mask, while the second part of the system outputs the likelihood of the
patch being centered on a full object. At test time, the model is
efficiently applied on the whole test image and generates a set of
segmentation masks, each of them being assigned with a corresponding object
 author = {P. H. O. Pinheiro and R. Collobert and P. Dollar},
 P. H. O. Pinheiro and R. Collobert. From Image-level to Pixel-level Labeling with Convolutional Networks. In Conference on Computer Vision and Pattern Recognition (CVPR), 2015.
object class information, and by considering only minimal priors on the
object segmentation task. This problem could be viewed as a kind of weakly
Learning (MIL) framework: every training image is known to have (or not) at
least one pixel corresponding to the image class label, and the
segmentation task can be rewritten as inferring the pixels belonging to the
class of the object (given one image, and its object class). We propose a
training to put more weight on pixels which are important for classifying
the image. We show that at test time, the model has learned to discriminate
the right pixels well enough, such that it performs very well on an
system is trained using a subset of the Imagenet dataset and the
dataset (with no fine-tuning of the model on Pascal VOC). Our model beats
the state of the art results in weakly supervised object segmentation task
by a large margin. We also compare the performance of our model with state
 title = {From Image-level to Pixel-level Labeling with Convolutional Networks},
 booktitle = {Conference on Computer Vision and Pattern Recognition (CVPR)},
 R. Lebret and R. Collobert. N-gram-Based Low-Dimensional Representation for Document Classification. In International Conference on Learning Representations (ICLR), 2015.
The bag-of-words (BOW) model is the common approach for classifying documents,
where words are used as feature for training a classifier. This generally
involves a huge number of features. Some techniques, such as Latent Semantic
Analysis (LSA) or Latent Dirichlet Allocation (LDA), have been designed to summarize
documents in a lower dimension with the least semantic information loss.
Some semantic information is nevertheless always lost, since only words are considered.
Instead, we aim at using information coming from n-grams to overcome
this limitation, while remaining in a low-dimension space. Many approaches, such
as the Skip-gram model, provide good word vector representations very quickly.
We propose to average these representations to obtain representations of n-grams.
All n-grams are thus embedded in a same semantic space. A K-means clustering
can then group them into semantic concepts. The number of features is therefore
dramatically reduced and documents can be represented as bag of semantic
concepts. We show that this model outperforms LSA and LDA on a sentiment
classification task, and yields similar results than a traditional BOW-model with
 R. Lebret and R. Collobert. Phrase-Based Image Captioning. In International Conference on Machine Learning (ICML), 2015.
Generating a novel textual description of an image is an interesting
this paper, we present a simple model that is able to generate descriptive
sentences given a sample image. This model has a strong focus on the syntax
of the descriptions. We train a purely bilinear model that learns a metric
them. The system is then able to infer phrases from a given image sample.
Based on caption syntax statistics, we propose a simple language model that
can produce relevant descriptions for a given test image using the phrases
models, achieves comparable results in two popular datasets for the task:
 D. Palaz, M. Magimai-Doss and R. Collobert. Analysis of CNN-based Speech Recognition System using Raw Speech as Input. In 16th Annual Conference of the International Speech Communication Association (Interspeech), 2015.
between the acoustic speech signal and the phones in two separate steps:
feature extraction and classifier training. In our recent works, we have
shown that, in the framework of convolutional neural networks (CNN), the
relationship between the raw speech signal and the phones can be directly
modeled and ASR systems competitive to standard approach can be built. In
this paper, we first analyze and show that, between the first two
we show that the CNN-based approach yields ASR trends similar to standard
 title = {Analysis of CNN-based Speech Recognition System using Raw Speech as Input},
 author = {D. Palaz and M. Magimai-Doss and R. Collobert},
 booktitle = {16th Annual Conference of the International Speech Communication Association (Interspeech)},
 J. Legrand and R. Collobert. Joint RNN-Based Greedy Parsing and Word Composition. In International Conference on Learning Representations (ICLR), 2015.
This paper introduces a greedy parser based on neural networks, which leverages
a new compositional sub-tree representation. The greedy parser and the compositional
is achieved over continuous (word or tag) representations, and recurrent neural
networks. We reach F1 performance on par with well-known existing parsers,
while having the advantage of speed, thanks to the greedy nature of the parser. We
provide a fully functional implementation of the method described in this paper.
 D. Palaz, M. Magimai-Doss and R. Collobert. Convolutional Neural Networks-based Continuous Speech Recognition using Raw Speech Signal. In 40th International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2015.
neural network (ANN). In our recent work, it was shown that Convolutional
Neural Networks (CNNs) can model phone classes from raw acoustic speech
parameters. We also show that the features learned from raw speech by the
 title = {Convolutional Neural Networks-based Continuous Speech Recognition using Raw Speech Signal},
 author = {D. Palaz and M. Magimai-Doss and R. Collobert},
 booktitle = {40th International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
 R. Lebret and R. Collobert. Rehabilitation of Count-based Models for Word Vector Representations. In Conference on Intelligent Text Processing and Computational Linguistics (CICLing), 2015.
to optimally predict the contexts in which the corresponding words tend to
appear. Such models have succeeded in capturing word similarities as well
interest in a model based on counts. We present a systematic study of the
use of the Hellinger distance to extract semantic representations from the
word co-occurrence statistics of large text corpora. We show that this
distance gives good performance on word similarity and analogy tasks, with
a proper type and size of context, and a dimensionality reduction based on
intuitive, this method also provides an encoding function which can be used
to infer unseen words or phrases. This becomes a clear advantage compared
 title = {Rehabilitation of Count-based Models for Word Vector Representations},
 booktitle = {Conference on Intelligent Text Processing and Computational Linguistics (CICLing)},
 D. Palaz, M. Magimai-Doss and R. Collobert. Joint phoneme segmentation inference and classification using CRFs. In 2nd Global Conference on Signal and Information Processing (GlobalSIP), 2014.
phoneme sequence segmentation and training of ANN. In this paper, we
consists of a local classifier ANN followed by a conditional random field
(CRF) whose parameters are trained jointly, using a cost function that
order to efficiently train such a system we introduce a novel CRF based
segmentation using acyclic graph. We study the viability of the proposed
approach on TIMIT phoneme recognition task. Our studies show that the
hybrid HMM/ANN and ANN/CRF systems where the ANN is trained with manual
 title = {Joint phoneme segmentation inference and classification using CRFs},
 author = {D. Palaz and M. Magimai-Doss and R. Collobert},
 booktitle = {2nd Global Conference on Signal and Information Processing (GlobalSIP)},
 P. H. O. Pinheiro and R. Collobert. Recurrent Convolutional Neural Networks for Scene Labeling. In Proceedings of the 31st International Conference on Machine Learning (ICML), 2014.
The goal of the scene labeling task is to assign a class label to each
pixel in an image. To ensure a good visual coherence and a high class
accuracy, it is essential for a model to capture long range (pixel) label
around each pixel to be labeled. We propose an approach that consists of a
recurrent convolutional neural network which allows us to consider a large
input context while limiting the capacity of the model. Contrary to most
standard approaches, our method does not rely on any segmentation technique
nor any task-specific features. The system is trained in an end-to-end
manner over raw pixels, and models complex spatial dependencies with low
inference cost. As the context size increases with the built-in recurrence,
the system identifies and corrects its own errors. Our approach yields
the SIFT Flow Dataset, while remaining very fast at test time.
 booktitle = {Proceedings of the 31st International Conference on Machine Learning (ICML)},
 J. Legrand and R. Collobert. Recurrent Greedy Parsing with Neural Networks. In Proceedings of the European Conference on Machine Learning, Principles and Practice of Knowledge Discovery in Databases (ECML-PKDD), 2014.
In this paper, we propose a bottom-up greedy and purely discriminative
syntactic parsing approach that relies only on a few simple features. The
core of the architecture is a simple neural network architecture, trained
with an objective function similar to a Conditional Random Field. This
of the input features and the model. Generalization accuracy compares very
(despite the greedy nature of our approach), and prediction speed is very
 booktitle = {Proceedings of the European Conference on Machine Learning and Principles and Practice of Knowledge Discovery in Databases (ECML-PKDD)},
 R. Lebret and R. Collobert. Word Embeddings through Hellinger PCA. In Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics (EACL), pages 482-490, Association for Computational Linguistics, 2014.
Word embeddings resulting from neural language models have been shown to be
a great asset for a large variety of NLP tasks. However, such architecture
might be difficult and time-consuming to train. Instead, we propose to
PCA of the word co-occurence matrix. We compare those new word embeddings
review tasks and show that we can reach similar or even better
good word embeddings, we show that it can provide an easy way to adapt
 booktitle = {Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics (EACL)},
 D. Palaz, R. Collobert and M. Magimai-Doss. Estimating Phoneme Class Conditional Probabilities from Raw Speech Signal using Convolutional Neural Networks. In Interspeech, 2013.
speech signal based on prior knowledge such as, speech perception or/and
specifically in the field of image processing and text processing, have
extraction and modeling steps) may not be necessary. Motivated from these
paper investigates a novel approach, where the input to the ANN is raw
architectures to show the benefit of CNNs and compare the proposed approach
extracted and modeled by a multilayer perceptron. Our studies show that the
 title = {Estimating Phoneme Class Conditional Probabilities from Raw Speech Signal using Convolutional Neural Networks},
 author = {D. Palaz and R. Collobert and M. Magimai-Doss},
 A. Bordes, L. Bottou, R. Collobert, D. Roth, J. Weston and L. Zettlemoyer. Introduction to the Special Issue on Learning Semantics. Machine Learning, 94:127-131, 2013.
 title = {Introduction to the Special Issue on Learning Semantics},
 author = {A. Bordes and L. Bottou and R. Collobert and D. Roth and J. Weston and L. Zettlemoyer},
 M. Yazdani, R. Collobert and A. Popescu-Belis. Learning to Rank on Network Data. In Eleventh Workshop on Mining and Learning with Graphs, ACM, 2013.
This paper proposes a method for learning to rank over network data. The
ranking is performed with respect to a query object which can be part of
the network or out- side it. The ranking method makes use of the features
of the nodes as well as the existing links between them. First, a
to objects' content and therefore, the scoring is consistent in every
network. By formulating link prediction as a ranking problem, the method
uses both the attributes of the nodes and the structure of the links,
one, a random walk method, a relational topic model, and a method based
on the weighted number of common neighbors. In addition, the propagation
algorithm improves results even when the query object is not part of the
 author = {M. Yazdani and R. Collobert and A. Popescu-Belis},
 booktitle = {Eleventh Workshop on Mining and Learning with Graphs},
 R. Collobert, K. Kavukcuoglu and C. Farabet. Implementing Neural Networks Efficiently. In Neural Networks: Tricks of the Trade, G. Montavon, G. Orr and K-R. Muller (Ed), Springer, 2012.
can be set up as quickly as possible with best possible computational
performance. To that end, we provide a new framework called Torch7,
 author = {R. Collobert and K. Kavukcuoglu and C. Farabet},
 editor = {G. Montavon and G. Orr and K-R. Muller},
 J. Weston, F. Ratle, H. Mobahi and R. Collobert. Deep Learning via Semi-Supervised Embedding. In Neural Networks: Tricks of the Trade, G. Montavon, G. Orr and K-R. Muller (Ed), Springer, 2012.
can be easily applied to deep multi-layer architectures, either as a
regularizer at the output layer, or on each layer of the
 author = {J. Weston and F. Ratle and H. Mobahi and R. Collobert},
 editor = {G. Montavon and G. Orr and K-R. Muller},
 R. Collobert, K. Kavukcuoglu and C. Farabet. Torch7: A Matlab-like Environment for Machine Learning. In BigLearn, NIPS Workshop, 2011.
 learning library that extends Lua. Its goal is to provide a
 author = {R. Collobert and K. Kavukcuoglu and C. Farabet},
 R. Collobert, J. Weston, L. Bottou, M. Karlen, K. Kavukcuoglu and P. Kuksa. Natural Language Processing (Almost) from Scratch. Journal of Machine Learning Research, 12:2493-2537, 2011.
 unlabeled training data. This work is then used as a basis for
 author = {R. Collobert and J. Weston and L. Bottou and M. Karlen and K. Kavukcuoglu and P. Kuksa},
 P. Kuksa, Y. Qi, B. Bai, R. Collobert, J.Weston, V. Pavlovic and X. Ning. Semi-Supervised Abstraction-Augmented String Kernel for Multi-Level Bio-Relation Extraction. In ECML PKDD, 2010.
 bio-entities in text at multiple levels, e.g., at the article,
 sentence or relation level. A key limitation of current bRE
 systems is that they are restricted by the availability of
 with mismatches in the string kernel framework. Our string kernel
 provides a unified framework for solving bRE with multiple degrees
 title = {Semi-Supervised Abstraction-Augmented String Kernel for Multi-Level Bio-Relation Extraction},
 author = {P. Kuksa and Y. Qi and B. Bai and R. Collobert and J.Weston and V. Pavlovic and X. Ning},
We present a general framework and learning algorithm for the task
 of concept labeling: each word in a given sentence has to be
 tagged with the unique physical entity (e.g. person, object or
 location) or abstract con- cept it refers to. Our method allows
 both world knowledge and linguistic information to be used during
 learning and prediction. We show experimentally that we can learn
 to use world knowledge to resolve ambiguities in language, such as
 word senses or ref- erence resolution, without the use of
 author = {A. Bordes and N. Usunier and R. Collobert and J. Weston},
 B. Bai, J. Weston, D. Grangier, R. Collobert, C. Cortes and M. Mohri. Half Transductive Ranking. In Artificial Intelligence and Statistics (AISTATS), 2010.
We study the standard retrieval task of ranking a fixed set of
 items given a previously unseen query and pose it as the
 (where the vector representation of each example is learned) allow
 extension. Inductive approaches on the other hand allow for the
 author = {B. Bai and J. Weston and D. Grangier and R. Collobert and C. Cortes and M. Mohri},
 A. Bordes, N. Usunier, J. Weston and R. Collobert. Learning to Disambiguate Natural Language Using World Knowledge. In NIPS workshop on Grammar Induction, Representation of Language and Language Learning, 2009.
We present a general framework and learning algorithm for the task
 of concept labeling: each word in a given sentence has to be
 tagged with the unique physical entity (e.g. person, object or
 location) or abstract concept it refers to. Our method allows both
 learning and prediction. We show experimentally that we can handle
 natural language and learn to use world knowledge to resolve
 title = {Learning to Disambiguate Natural Language Using World Knowledge},
 author = {A. Bordes and N. Usunier and J. Weston and R. Collobert},
 booktitle = {NIPS workshop on Grammar Induction, Representation of Language and Language Learning},
 B. Bai, J. Weston, D. Grangier, R. Collobert, C. Cortes and M. Mohri. Ranking with Half Transductive Models. In NIPS Workshop on Advances in Ranking, 2009.
We study the standard retrieval task of ranking a fixed set of
 documents given a previously unseen query and pose it as the
 as the document set is fixed. Existing transductive approaches are
 natural non-linear methods for this set, but have no direct
 can be applied to the unseen queries, but fail to exploit the
 availability of the document set in its full extent. This work
 author = {B. Bai and J. Weston and D. Grangier and R. Collobert and C. Cortes and M. Mohri},
 B. Bai, J. Weston, D. Grangier, R. Collobert, K. Sadamasa, Y. Qi, C. Cortes and M. Mohri. Polynomial Semantic Indexing. In Advances in Neural Information Processing Systems (NIPS), 2009.
 discriminatively trained to directly map from the word content in
 author = {B. Bai and J. Weston and D. Grangier and R. Collobert and K. Sadamasa and Y. Qi and C. Cortes and M. Mohri},
 shallow systems, these feature layers are learnt for the task of
 interest, and do not require any engineering. We will show how
 in NLP tasks. We will study multi-tasking different tasks, new
 learning. Finally, we will highlight how some of these advances
 can be applied to other fields of research, like computer vision,
 Y. Qi, P. Kuksa, R. Collobert, K. Sadamasa, K. Kavukcuoglu and J. Weston. Semi-Supervised Sequence Labeling with Self-Learned Features. In IEEE International Conference on Data Mining (ICDM), 2009.
 assigning labels to words in a natural language sequence. The
 performance is restricted by the availability of labeled words. To
 improve the sequence labeling procedure in IE through a class of
 classifier can be trained with annotated text sequences and used
 to classify each word in a large set of unannotated sentences. By
 averaging predicted labels over all cases in the unlabeled corpus,
 word (or word attribute) in the dictionary and re-trains the
 features. Basic SLF models how likely a word could be assigned to
 and scalable behaviour and is easy to tune. We applied this
 author = {Y. Qi and P. Kuksa and R. Collobert and K. Sadamasa and K. Kavukcuoglu and J. Weston},
 B. Bai, J. Weston, D. Grangier, R. Collobert, K. Sadamasa, Y. Qi, O. Chapelle and K. Weinberger. Learning to Rank with (a Lot of) Word Features. Journal of Information Retrieval, volume Special Issue on Learning to Rank for Information Retrieval, 2009.
 which defines a class of nonlinear (quadratic) models that are
 discriminatively trained to directly map from the word content in
 polysemy). However, unlike LSI our models are trained from a
 supervised signal directly on the ranking task of interest, which
 we argue is the reason for our superior results. As the query and
 sparsification. We provide an empirical study of all these methods
 on retrieval tasks based on Wikipedia documents as well as an
 title = {Learning to Rank with (a Lot of) Word Features},
 author = {B. Bai and J. Weston and D. Grangier and R. Collobert and K. Sadamasa and Y. Qi and O. Chapelle and K. Weinberger},
 volume = {Special Issue on Learning to Rank for Information Retrieval},
 T. Barnickel, J. Weston, R. Collobert, H-W. Mewes and V. Stmpflen. Large Scale Application of Neural Network Based Semantic Role Labeling for Automated Relation Extraction from Biomedical Texts. PLoS one, 4(7), July 2009.
To reduce the increasing amount of time spent on literature search
 deal with large text corpora like MEDLINE in an acceptable time
 but are not able to extract any specific type of semantic
 trees, on the other hand, are computationally expensive and the
 domain exist focusing specifically on the detection of a limited
 set of relation types. For systems biology, generic approaches for
 the detection of a multitude of relation types which in addition
 are able to process large text corpora are needed but the number
 of systems meeting both requirements is very limited. We introduce
 the use of SENNA (Semantic Extraction using a Neural Network
 Role Labeling (SRL) program, for the large scale extraction of
 processing times of SENNA and other SRL systems or syntactical
 parsers used in the biomedical domain revealed that SENNA is the
 with SENNA on a 100 node cluster within three days. The accuracy
 of the presented relation extraction approach was evaluated on two
 values of 0.71/0.43. We show that the accuracy as well as
 title = {Large Scale Application of Neural Network Based Semantic Role Labeling for Automated Relation Extraction from Biomedical Texts},
 author = {T. Barnickel and J. Weston and R. Collobert and H-W. Mewes and V. St\"umpflen},
 Y. Qi, R. Collobert, P. Kuksa, K. Kavukcuoglu and J. Weston. Combining Labeled and Unlabeled Data with Word-Class Distribution Learning. In The 18th ACM Conference on Information and Knowledge Management (CIKM), 2009.
 it the task of information extraction (IE) by utilizing unlabeled
 iteratively builds class label distributions for each word in the
 dictionary by averaging predicted labels over all cases in the
 no difficult parameters to tune. We applied our method on German
 and English name en- tity recognition (NER) tasks. WCDL shows
 title = {Combining Labeled and Unlabeled Data with Word-Class Distribution Learning},
 author = {Y. Qi and R. Collobert and P. Kuksa and K. Kavukcuoglu and J. Weston},
 booktitle = {The 18th ACM Conference on Information and Knowledge Management ({CIKM})},
 B. Bai, J. Weston, D. Grangier, R. Collobert, O. Chapelle and K. Weinberger. Supervised Semantic Indexing. In The 18th ACM Conference on Information and Knowledge Management (CIKM), 2009.
 algorithm that is trained on (query, document) pairs of text
 documents to predict the quality of their match. Like Latent
 are trained with a supervised signal directly on the ranking task
 of interest, which we argue is the reason for our superior
 results. As the query and target texts are modeled separately, our
 as online advertising placement. Dealing with models on all pairs
 several improvements to our basic model for addressing this issue,
 correlated feature hashing (CFH). We provide an empirical study of
 all these methods on retrieval tasks based on Wikipedia documents
 author = {B. Bai and J. Weston and D. Grangier and R. Collobert and O. Chapelle and K. Weinberger},
 booktitle = {The 18th ACM Conference on Information and Knowledge Management ({CIKM})},
 H. Mobahi, R. Collobert and J. Weston. Deep Learning from Temporal Coherence in Video. In International Conference on Machine Learning, ICML, 2009.
 recordings. That is, two successive frames are likely to contain
 the same object or objects. This coherence is used as a
 supervisory signal over the unlabeled data, and is used to improve
 the performance on a supervised task of interest. We demonstrate
 the effectiveness of this method on some pose invariant object and
 author = {H. Mobahi and R. Collobert and J. Weston},
 Y. Bengio, J. Louradour, R. Collobert and J. Weston. Curriculum Learning. In International Conference on Machine Learning, ICML, 2009.
Humans and animals learn much better when the examples are not
 we formalize such training strategies in the context of machine
 learning, and call them curriculum learning. In the context of
 effect on the speed of convergence of the training process to a
 minimum and, in the case of non-convex criteria, on the quality of
 the local minima obtained: curriculum learning can be seen as a
Our large-scale language model in our unified NLP paper has been trained
 author = {Y. Bengio and J. Louradour and R. Collobert and J. Weston},
 B. Bai, J. Weston, R. Collobert and D. Grangier. Supervised Semantic Indexing. In 31st European Conference on Information Retrieval, 2009.
We present a class of models that are discriminatively trained to
 directly map from the word content in a query-document or
 document- document pair to a ranking score. Like Latent Semantic
 words (synonymy, pol- ysemy). However, unlike LSI our models are
 trained with a supervised signal directly on the task of interest,
 which we argue is the reason for our superior results. We provide
 an empirical study on Wikipedia documents, using the links to
 author = {B. Bai and J. Weston and R. Collobert and D. Grangier},
learning algorithms. It is easy to use and very efficient, thanks to a
implementation. Torch is easily extensible and has been shown to scale to
 howpublished = {NIPS Workshop on Machine Learning Open Source Software},
 M. Karlen, J. Weston, A. Erkan and R. Collobert. Large Scale Manifold Transduction. In International Conference on Machine Learning, ICML, 2008.
We show how the regularizer of Transductive Support Vector Machines (TSVM)
can be trained by stochastic gradient descent for linear models and
online, have vastly superior training and testing speed to existing TSVM
obtain competitive error rates. We then go on to propose a natural
 author = {M. Karlen and J. Weston and A. Erkan and R. Collobert},
 R. Collobert and J. Weston. A Unified Architecture for Natural Language Processing: Deep Neural Networks with Multitask Learning. In International Conference on Machine Learning, ICML, 2008.
semantically similar words and the likelihood that the sentence makes sense
network is trained jointly on all these tasks using weight-sharing, an
instance of multitask learning. All the tasks use labeled data except
the language model which is learnt from unlabeled text and represents a
novel form of semi-supervised learning for the shared tasks. We show how
 title = {A Unified Architecture for Natural Language Processing: Deep Neural Networks with Multitask Learning},
 J. Weston, F. Rattle and R. Collobert. Deep Learning via Semi-Supervised Embedding. In International Conference on Machine Learning, ICML, 2008.
We show how nonlinear embedding algorithms popular for use with shallow
to deep multi-layer architectures, either as a regularizer at the output
layer, or on each layer of the architecture. This provides a simple
 author = {J. Weston and F. Rattle and R. Collobert},
 R. Collobert and J. Weston. Fast Semantic Extraction Using a Novel Neural Network Architecture. In Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 560-567, June 2007.
We describe a novel neural network architecture for the problem of semantic
stages and handbuilt features, and are too slow to be applied as part of
their use of a syntactic parser (Pradhan et al., 2004; Gildea and Jurafsky,
2002). Our method instead learns a direct mapping from source sentence to
semantic tags for a given predicate without the aid of a parser or a
 title = {Fast Semantic Extraction Using a Novel Neural Network Architecture},
 booktitle = {Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics},
 J. Weston, R. Collobert, F. Sinz, L. Bottou and V. Vapnik. Inference with the Universum. In Proceedings of the Twenty-third International Conference on Machine Learning (ICML 2006), pages 1009-1016, ACM Press, 2006.
In this paper we study a new framework introduced by Vapnik (1998; 2006)
that is an alternative capacity concept to the large margin approach. In
the particular case of binary classification, we are given a set of labeled
examples, and a collection of rage the Universum by maximizing the number
 author = {J. Weston and R. Collobert and F. Sinz and L. Bottou and V. Vapnik},
 booktitle = {Proceedings of the Twenty-third International Conference on Machine Learning (ICML 2006)},
 R. Collobert, F. Sinz, J. Weston and L. Bottou. Large Scale Transductive SVMs. Journal of Machine Learning Research, 7:1687-1712, September 2006.
We show how the Concave-Convex Procedure can be applied to Transductive SVMs, which
traditionally require solving a combinatorial search problem. This provides for the rst
time a highly scalable algorithm in the nonlinear case. Detailed experiments verify the
This is a derivative of the original paper Trading Convexity for Scalability.
 author = {R. Collobert and F. Sinz and J. Weston and L. Bottou},
 R. Collobert, F. Sinz, J. Weston and L. Bottou. Trading convexity for scalability. In Proceedings of the Twenty-third International Conference on Machine Learning (ICML 2006), pages 201-208, ACM Press, 2006.
properties and are amenable to theoretical analysis. However, in this work
convexity. We show how concave-convex programming can be applied to produce
(i) faster SVMs where training errors are no longer support vectors, and
This paper received the best paper award at ICML 2006 conference.
 author = {R. Collobert and F. Sinz and J. Weston and L. Bottou},
 booktitle = {Proceedings of the Twenty-third International Conference on Machine Learning (ICML 2006)},
This thesis aims to address machine learning in general, with a particular
focus on large models and large databases. After introducing the learning
problem in a formal way, we first review several important machine learning
Support Vector Machines. We then present a training method for Support
of such a model is still intractable on very large databases. We thus
propose a divide and conquer approach based on a kind of Mixture of Experts
in order to break up the training problem into small pieces, while keeping
good generalization performance. This mixture model can be applied to any
kind of existing machine learning algorithm. Even though it performs well
in practice the major drawback of this algorithm is the number of
hyper-parameters to tune, which makes it difficult to use. We thus prefer
which are easier to tune, and more suitable than Support Vector Machines
for large databases. We finally show that the margin idea introduced with
Support Vector Machines can be applied to a certain class of Multi Layer
This is my PhD thesis. I did my PhD both at IDIAP and
 R. Collobert and S. Bengio. Links Between Perceptrons, MLPs and SVMs. In International Conference on Machine Learning, ICML, 2004.
Machines (SVMs). We first study ways to control the capacity of Perceptrons
idea introduced with SVMs. After showing that under simple conditions a
Perceptron is equivalent to an SVM, we show it can be computationally
expensive in time to train an SVM (and thus a Perceptron) with stochastic
gradient descent, mainly because of the margin maximization term in the
cost function. We then show that if we remove this margin maximization
term, the learning rate or the use of early stopping can still control the
margin. These ideas are extended afterward to the case of MLPs. Moreover,
under some assumptions it also appears that MLPs are a kind of mixture of
SVMs, maximizing the margin in the hidden layer space. Finally, we present
a very simple MLP based on the previous findings, which yields better
Neural networks with the right criterion (like an hinge loss) work well,
Also, each neuron in the hidden layer of a neural network acts
interestingly as a kind of SVM, on a subset of the training set.
 R. Collobert and S. Bengio. A Gentle Hessian for Efficient Gradient Descent. In IEEE International Conference on Acoustic, Speech, and Signal Processing, ICASSP, 2004.
have been proposed over the years, but they usually need to compute the
inverse of the Hessian of the cost function (or an approximation of this
inverse) during training. In most cases, this leads to an O(n^2) cost in
time and space per iteration, where n is the number of parameters, which
is prohibitive for large n. We propose instead a study of the Hessian
before training. Based on a second order analysis, we show that a
Hessian. We also show that the condition of block-diagonality in common
machine learning models can be achieved by simply selecting an appropriate
training criterion. Finally, we propose a version of the SVM criterion
applied to MLPs, which verifies the aspects highlighted in this second
Probably because in the past neural network were studied on very small
correct by: if not well tuned (like a SVM having a Gaussian kernel with a
small variance!) neural networks do overfit. But in fact, in many cases,
We show here that the choice of the architecture itself has an impact on
In particular we show that the margin criterion used in SVMs is well suited
for neural network optimization: with the hinge loss, the Hessian is better
 booktitle = {{IEEE} International Conference on Acoustic, Speech, and Signal Processing, {ICASSP}},
 R. Collobert, Y. Bengio and S. Bengio. Scaling Large Learning Problems with Hard Parallel Mixtures. International Journal on Pattern Recognition and Artificial Intelligence (IJPRAI), 17(3):349-365, 2003.
A challenge for statistical learning is to deal with large data sets,
e.g. in data mining. The training time of ordinary Support Vector
Machines is at least quadratic, which raises a serious research challenge
if we want to deal with data sets of millions of examples. We propose a
training data is iteratively partitioned by a ``gater'' model in such a way
that it becomes easy to learn an ``expert'' model separately in each region
of the partition. A probabilistic extension and the use of a set of
generative models allows representing the gater so that all pieces of the
of the algorithm, the iterative algorithm provably goes down in a cost
The aim was to use a divide-and-conquer method to break up the SVM
do work, they are unfortunately quite difficult to tune, because of the
 title = {Scaling Large Learning Problems with Hard Parallel Mixtures},
 author = {R. Collobert and Y. Bengio and S. Bengio},
 journal = {International Journal on Pattern Recognition and Artificial Intelligence ({IJPRAI})},
 C. Sanderson, S. Bengio, H. Bourlard, J. Marithoz, R. Collobert, M.F. BenZeghiba, F. Cardinaux and S. Marcel. Speech & Face Based Biometric Authentication at IDIAP. In International Conference on Multimedia and Expo, ICME, volume 3, pages 1-4, 2003.
We present an overview of recent research at IDIAP on speech & face based
adaptation techniques, confidence measures (for use in fusion of audio &
visual scores), face verification in difficult image conditions, as well as
other related research issues. We also overview the open source Torch
library, which has aided in the implementation of the above mentioned
 title = {Speech \& Face Based Biometric Authentication at {IDIAP}},
 author = {C. Sanderson and S. Bengio and H. Bourlard and J. Mari\'ethoz and R. Collobert and M.F. BenZeghiba and F. Cardinaux and S. Marcel},
 R. Collobert, S. Bengio and J. Marithoz. Torch: a modular machine learning software library. Technical Report IDIAP-RR 02-46, IDIAP, 2002.
learning algorithms recently, mainly due to the generally good results they
these machine learning algorithms are often complex to implement and to use
properly and efficiently. We thus present in this paper a new machine
already been implemented and are available in a unified framework, in order
for scientists to be able to use them, compare them, and even extend them
available under a BSD license and can be retrieved from the web by
This presented the first version of the Torch machine learning library. Several versions
 author = {R. Collobert and S. Bengio and J. Mari\'ethoz},
 R. Collobert, S. Bengio and Y. Bengio. A Parallel Mixture of SVMs for Very Large Scale Problems. In T.G. Dietterich, S. Becker and Z. Ghahramani, editors, Advances in Neural Information Processing Systems, NIPS 14, pages 633-640, MIT Press, 2002.
for many classification problems but they suffer from the complexity of
their training algorithm which is at least quadratic with respect to the
number of examples. Hence, it is hopeless to try to solve real-life
problems having more than a few hundreds of thousands examples with
SVMs. The present paper proposes a new mixture of SVMs that can be easily
implemented in parallel and where each SVM is trained on a small subset of
the whole dataset. Experiments on a large benchmark dataset (Forest) as
number of examples). In addition, and that is a surprise, a significant
This is our first paper on Mixture of SVMs. The aim was to use a
divide-and-conquer method to break up the SVM complexity and solve large
and probabilistic mixtures has been published in IJPRAI and presented at SVM'2002.
 title = {A Parallel Mixture of {SVMs} for Very Large Scale Problems},
 author = {R. Collobert and S. Bengio and Y. Bengio},
 booktitle = {Advances in Neural Information Processing Systems, {NIPS} 14},
 editor = {Dietterich, T.G. and Becker, S. and Ghahramani, Z.},
 R. Collobert, S. Bengio and Y. Bengio. A Parallel Mixture of SVMs for Very Large Scale Problems. Neural Computation, 14(5):1105-1114, 2002.
for many classification problems but they suffer from the complexity of
their training algorithm which is at least quadratic with respect to the
number of examples. Hence, it is hopeless to try to solve real-life
problems having more than a few hundreds of thousands examples with
SVMs. The present paper proposes a new mixture of SVMs that can be easily
implemented in parallel and where each SVM is trained on a small subset of
to locally grow linearly with the number of examples). In addition, and
The aim was to use a divide-and-conquer method to break up the SVM
do work, they are unfortunately quite difficult to tune, because of the
and probabilistic mixtures has been published in IJPRAI and presented at SVM'2002.
 title = {A Parallel Mixture of {SVMs} for Very Large Scale Problems},
 author = {R. Collobert and S. Bengio and Y. Bengio},
 R. Collobert and S. Bengio. SVMTorch: Support Vector Machines for Large-Scale Regression Problems. Journal of Machine Learning Research, 1:143-160, 2001.
solving a quadratic optimization problem which needs on the order of l
square memory and time resources to solve, where l is the number of
recent paper from Lin (2000), we show that a convergence proof exists for
problem. Though nowadays it may seems obvious, curiously it was not the
technique used to train regression SVMs at the time we proposed this
 title = {{SVMT}orch: Support Vector Machines for Large-Scale Regression Problems},
