http://deeplearning.net/tutorial/gettingstarted.html
baidu
15
These tutorials do not attempt to make up for a graduate or undergraduate course
in machine learning, but we do make a rapid overview of some important concepts
(and notation) to make sure that were on the same page. Youll also need to
download the datasets mentioned in this chapter in order to run the example code of
On each learning algorithm page, you will be able to download the corresponding files. If you want to download all of them at the same time, you can clone the git repository of the tutorial:
On Linux or Mac systems, after cloning, all datasets can be downloaded at once with:
digit images and it is divided in 60,000 examples for the training set and
10,000 examples for testing. In many papers as well as in this tutorial, the
official training set of 60,000 is divided into an actual training set of 50,000
learning rate and size of the model). All digit images have been size-normalized and
centered in a fixed size image of 28 x 28 pixels. In the original dataset
each pixel of the image is represented by a value between 0 and 255, where
0 is black, 255 is white and anything in between is a different shade of grey.
For convenience we pickled the dataset to make it easier to use in python.
The pickled file represents a tuple of 3 lists : the training set, the
validation set and the testing set. Each of the three lists is a pair
formed from a list of images and a list of class labels for each of the
images. An image is represented as numpy 1-dimensional array of 784 (28
x 28) float values between 0 and 1 (0 stands for black, 1 for white).
The labels are numbers between 0 and 9 indicating which digit the image
represents. The code block below shows how to load the dataset.
When using the dataset, we usually divide it in minibatches (see
Stochastic Gradient Descent). We encourage you to store the dataset into shared
variables and access it based on the minibatch index, given a fixed
related to using the GPU. There is a large overhead when copying data
into the GPU memory. If you would copy data on request (each minibatch
individually when needed) as the code will do if you do not use shared
variables, due to this overhead, the GPU code will not be much faster
then the CPU code (maybe even slower). If you have your data in
Theano shared variables though, you give Theano the possibility to copy
the entire data on the GPU in a single call when the shared variables are constructed.
Afterwards the GPU can access any minibatch by taking a slice from this
shared variables, without needing to copy any information from the CPU
Because the datapoints and their labels are usually of different nature
suggest to use different variables for label and data. Also we recommend
testing set to make the code more readable (resulting in 6 different
Since now the data is in one variable, and a minibatch is defined as a
slice of that variable, it comes more natural to define a minibatch by
indicating its index and its size. In our setup the batch size stays constant
throughout the execution of the code, therefore a function will actually
require only the index to identify on which datapoints to work.
The code below shows how to store your data and how to
 The reason we store our dataset in shared variables is to allow
 Theano to copy it into the GPU memory (when code is run on GPU).
 Since copying data into the GPU is slow, copying a minibatch everytime
 is needed (the default behaviour if the data is not in a shared
 # When storing data on the GPU it has to be stored as floats
 # therefore we will store the labels as ``floatX`` as well
 # we need them as ints (we use labels as index, and if they are
 # floats it doesnt make sense) therefore instead of returning
 # ``shared_y`` we will have to cast it to int. This little hack
The data has to be stored as floats on the GPU ( the right
To get around this shortcoming for the labels, we store them as float,
If you are running your code on the GPU and the dataset you are using
is too large to fit in memory the code will crash. In such a case you
should store the data in a shared variable. You can however store a
sufficiently small chunk of your data (several minibatches) in a shared
variable and use that during training. Once you got through the chunk,
update the values it stores. This way you minimize the number of data
the test set is used to evaluate the final generalization error and
The tutorials mostly deal with classification problems, where each data set
use superscripts to distinguish training set examples: is thus the i-th training example of dimensionality . Similarly,
: element at i-th row and j-th column of matrix 
Whats exciting about Deep Learning is largely the use of unsupervised learning
of deep networks. But supervised learning also plays an important role. The
utility of unsupervised pre-training is often evaluated on the basis of what
reviews the basics of supervised learning for classification models, and covers
the minibatch stochastic gradient descent algorithm that is used to fine-tune
many of the models in the Deep Learning Tutorials. Have a look at these
for more basics on the notion of optimizing a training criterion using the gradient.
The models presented in these deep learning tutorials are mostly used
for classification. The objective in training a classifier is to minimize the number
of errors (zero-one loss) on unseen examples. If is the prediction function, then this loss can be written as:
(to avoid biasing the evaluation of validation or test error). is the
# expression of the zero one loss ; to get the actual value this
# symbolic expression has to be compiled into a Theano function (see
Since the zero-one loss is not differentiable, optimizing it for large models
The likelihood of the correct class is not the same as the
number of right predictions, but from the point of view of a randomly
you should see that they are correlated on the validation set but
Since we usually speak in terms of minimizing a loss function, learning will
The NLL of our classifier is a differentiable surrogate for the zero-one loss,
and we use the gradient of this function over our training data as a
# NLL is a symbolic variable ; to get the actual value of NLL, this symbolic
# expression has to be compiled into a Theano function (see the Theano
# note on syntax: T.arange(y.shape[0]) is a vector of integers [0,1,2,...,len(y)].
# Indexing a matrix M by the two vectors [0,1,...,K], [a,b,...,k] returns the
# elements M[0,a], M[1,b], ..., M[K,k] as a vector. Here, we use this
# syntax to retrieve the log-probability of the correct labels, y.
algorithm in which we repeatedly make small steps downward on an error
For the purpose of ordinary gradient descent we consider that the training
data is rolled into the loss function. Then the pseudocode of this
Stochastic gradient descent (SGD) works according to the same principles as
ordinary gradient descent, but proceeds more quickly by estimating the gradient from just
a few examples at a time instead of the entire training set. In its purest
form, we estimate the gradient from just a single example at a time.
 # that may repeat examples (if there is only a finite training set)
The variant that we recommend for deep learning is a further twist on
Minibatch SGD (MSGD) works identically to SGD, except that we use more than
one training example to make each estimate of the gradient. This technique reduces
variance in the estimate of the gradient, and often makes better use of the
There is a tradeoff in the choice of the minibatch size . The
reduction of variance and use of SIMD instructions helps most when increasing
 from 1 to 2, but the marginal improvement fades rapidly to nothing.
With large , time is wasted in reducing the variance of the gradient
estimator, that time would be better spent on additional gradient steps.
anywhere from 1 to maybe several hundreds. In the tutorial we set it to 20,
If you are training for a fixed number of epochs, the minibatch size becomes important
because it controls the number of updates done to your parameters. Training the same model
for 10 epochs using a batch size of 1 yields completely different results compared
to training for the same 10 epochs but with a batchsize of 20. Keep this in mind when
switching between batch sizes and be prepared to tweak all the other parameters according
All code-blocks above show pseudocode of how the algorithm looks like. Implementing such
# assume loss is a symbolic description of the loss function given
 # here x_batch and y_batch are elements of train_batches and
 # therefore numpy arrays; function MSGD also updates the params
train our model from data we are trying to prepare it to do well on new
examples, not the ones it has already seen. The training loop above for MSGD
does not take this into account, and may overfit the training examples.
There are several techniques for regularization; the ones we will explain
L1 and L2 regularization involve adding an extra term to the loss function,
which penalizes certain parameter configurations. Formally, if our loss function is:
controls the relative importance of the regularization parameter. Commonly used values for p
are 1 and 2, hence the L1/L2 nomenclature. If p=2, then the regularizer is
In principle, adding a regularization term to the loss will encourage smooth
network mappings in a neural network (by penalizing large values of the
correspond to modelling the data well (NLL) and having simple or smooth
theory, correspond to finding the right trade-off between the fit to the
training data and the generality of the solution that is found. To follow
solution (as measured by our simplicity criterion) that fits the training
Note that the fact that a solution is simple does not mean that it will
The code block below shows how to compute the loss in python when it
loss = NLL + lambda_1 * L1 + lambda_2 * L2
validation set. A validation set is a set of examples that we never use for
gradient descent, but which is also not a part of the test set. The
validation examples are considered to be representative of future test examples.
We can use them during training because they are not part of the test set.
judgement call and a few heuristics exist, but these tutorials will make use
of a strategy based on a geometrically increasing amount of patience.
patience_increase = 2 # wait this much longer when a new best is
improvement_threshold = 0.995 # a relative improvement of this much is
 # Report "1" for first epoch, "n_epochs" for last epoch
 # iteration number. We want it to start at 0.
 # note that if we do `iter % validation_frequency` it will be
 # true for iter = 0 which we do not want. We want it true for
 this_validation_loss = ... # compute zero-one loss on validation set
# best_params refers to the best out-of-sample parameters observed during the optimization
If we run out of batches of training data before running out of patience, then
we just go back to the beginning of the training set and repeat.
patience. The code should check at least two times how it
performs before running out of patience. This is the reason we used
This algorithm could possibly be improved by using a test of statistical significance
rather than the simple comparison, when deciding whether to increase the
After the loop exits, the best_params variable refers to the best-performing
model on the validation set. If we repeat this procedure for another model
class, or even another random initialization, we should use the same
models. If we have to choose what the best model class or the best
we have finally chosen the model we think is the best (on validation data), we
report that models test set performance. That is the performance we expect on
The technique of early-stopping requires us to partition the set of examples into three sets
The training set is used for minibatch stochastic gradient descent on the
As we perform this gradient descent, we periodically consult the validation set
to see how our model is doing on the real objective function (or at least our
When we see a good model on the validation set, we save it.
When it has been a long time since seeing a good model, we abandon our search
and return the best parameters found, for evaluation on the test set.
When youre doing experiments, it can take hours (sometimes days!) for
gradient-descent to find the best parameters. You will want to save those
weights once you find them. You may also want to save your current-best
The best way to save/archive your models parameters is to use pickle or
deepcopy the ndarray objects. So for example, if your parameters are in
shared variables w, v, u, then your save command should look something
>>> save_file = open(path, wb) # this will overwrite current contents
>>> cPickle.dump(v.get_value(borrow=True), save_file, -1) # .. and it triggers much more efficient
This technique is a bit verbose, but it is tried and true. You will be able
to load your data and render it in matplotlib without trouble, years after
Do not pickle your training or test functions for long-term storage
but you should not necessarily pickle a Theano function. If you update your
Theano folder and one of the internal changes, then you may not be able to
un-pickle your model. Theano is still in active development, and the internal
APIs are subject to change. So to be on the safe side  do not pickle your
mechanism is aimed at for short-term storage, such as a temp file, or a copy to
Visualizations can be very powerful tools for understanding what your model or
training algorithm is doing. You might be tempted to insert matplotlib
script. However, later you will observe something interesting in one of those
pre-rendered images and want to investigate something that isnt clear from
If you have enough disk space, your training script should save intermediate models and a visualization
You already have a model-saving function right? Just use it again to save
Libraries youll want to know about: Python Image Library (PIL), matplotlib.
