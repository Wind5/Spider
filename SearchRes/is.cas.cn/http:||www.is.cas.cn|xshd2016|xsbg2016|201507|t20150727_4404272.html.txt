http://www.is.cas.cn/xshd2016/xsbg2016/201507/t20150727_4404272.html
 文章来源：  |  发布时间：2015-07-27  |  【打印】 【关闭】
 Title: Improving Topic Models with Latent Feature Word RepresentationsSpeaker: Mark Johnson (Macquarie University, Australia)          web.science.mq.edu.au/~mjohnsonTime: 30th July 2015, 15:30      (Re-scheduled, originally announced to be 31st July 15:00. Apologies.)Venue: Seminar Room (334), Level 3, Building 5,     Institute of Software, Chinese Academy of Sciences (CAS),     4 Zhongguancun South Fourth Street, Haidian District, Beijing 100190 Abstract:Probabilistic topic models are widely used to discover latent topicsin document collections, while latent feature vector representationsof words learnt from very large external corpora have been used toimprove the performance of many NLP tasks. In this talk I explain howwe extended two existing Dirichlet multinomial topic models byincorporating latent feature vector representations of words trainedon very large corpora, and show that this improves the word-topicmapping learnt on much smaller target corpora.Experimental resultsshow that by using latent feature information from large externalcorpora, our new models produce significantimprovements on topiccoherence, document clustering and document classification tasks. Theimprovement is greatest on datasets with few or short documents,including social media such as Twitter. Joint work with Dat Quoc Nguyen, Richard Billingsley and Lan Du.
